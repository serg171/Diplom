{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "yX74fF7DtbFx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: OpenNMT-py in ./anaconda3/lib/python3.7/site-packages (1.0.0)\n",
      "Requirement already satisfied: flask in ./anaconda3/lib/python3.7/site-packages (from OpenNMT-py) (1.1.1)\n",
      "Requirement already satisfied: configargparse in ./anaconda3/lib/python3.7/site-packages (from OpenNMT-py) (1.0)\n",
      "Requirement already satisfied: pyonmttok==1.*; platform_system == \"Linux\" in ./anaconda3/lib/python3.7/site-packages (from OpenNMT-py) (1.18.1)\n",
      "Requirement already satisfied: six in ./anaconda3/lib/python3.7/site-packages (from OpenNMT-py) (1.12.0)\n",
      "Requirement already satisfied: tensorboard>=1.14 in ./anaconda3/lib/python3.7/site-packages (from OpenNMT-py) (2.1.0)\n",
      "Requirement already satisfied: tqdm~=4.30.0 in ./anaconda3/lib/python3.7/site-packages (from OpenNMT-py) (4.30.0)\n",
      "Requirement already satisfied: torchtext==0.4.0 in ./anaconda3/lib/python3.7/site-packages (from OpenNMT-py) (0.4.0)\n",
      "Requirement already satisfied: future in ./anaconda3/lib/python3.7/site-packages (from OpenNMT-py) (0.17.1)\n",
      "Requirement already satisfied: torch>=1.2 in ./anaconda3/lib/python3.7/site-packages (from OpenNMT-py) (1.4.0)\n",
      "Requirement already satisfied: click>=5.1 in ./anaconda3/lib/python3.7/site-packages (from flask->OpenNMT-py) (7.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in ./anaconda3/lib/python3.7/site-packages (from flask->OpenNMT-py) (2.10.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in ./anaconda3/lib/python3.7/site-packages (from flask->OpenNMT-py) (1.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in ./anaconda3/lib/python3.7/site-packages (from flask->OpenNMT-py) (0.16.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in ./anaconda3/lib/python3.7/site-packages (from tensorboard>=1.14->OpenNMT-py) (0.9.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./anaconda3/lib/python3.7/site-packages (from tensorboard>=1.14->OpenNMT-py) (2.22.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./anaconda3/lib/python3.7/site-packages (from tensorboard>=1.14->OpenNMT-py) (3.1.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in ./anaconda3/lib/python3.7/site-packages (from tensorboard>=1.14->OpenNMT-py) (1.10.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in ./anaconda3/lib/python3.7/site-packages (from tensorboard>=1.14->OpenNMT-py) (1.26.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./anaconda3/lib/python3.7/site-packages (from tensorboard>=1.14->OpenNMT-py) (41.4.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in ./anaconda3/lib/python3.7/site-packages (from tensorboard>=1.14->OpenNMT-py) (1.17.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in ./anaconda3/lib/python3.7/site-packages (from tensorboard>=1.14->OpenNMT-py) (0.33.6)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in ./anaconda3/lib/python3.7/site-packages (from tensorboard>=1.14->OpenNMT-py) (3.11.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./anaconda3/lib/python3.7/site-packages (from tensorboard>=1.14->OpenNMT-py) (0.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in ./anaconda3/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask->OpenNMT-py) (1.1.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in ./anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in ./anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py) (3.0.4)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in ./anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py) (4.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./anaconda3/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "# ! pip install OpenNMT-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "o4rkgF8oakzS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading https://files.pythonhosted.org/packages/63/ac/b93411318529980ab7f41e59ed64ec3ffed08ead32389e29eb78585dd55d/rouge-0.3.2-py3-none-any.whl\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-0.3.2\n"
     ]
    }
   ],
   "source": [
    "# ! pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade https://storage.googleapis.com/jax-releases/cuda102/jaxlib-0.1.44-cp37-none-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aWHXIGkGuirc"
   },
   "outputs": [],
   "source": [
    "import onmt\n",
    "import pyonmttok\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import rouge\n",
    "import gin\n",
    "import trax\n",
    "import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26UBuSCvdxOw"
   },
   "source": [
    "### 0. Загрузка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ggnp8RmHqUum"
   },
   "outputs": [],
   "source": [
    "#! wget https://github.com/RossiyaSegodnya/ria_news_dataset/raw/master/ria.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-VmbB8SVqm9O"
   },
   "outputs": [],
   "source": [
    "#! gunzip ria.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iGJi355ZmWjW"
   },
   "outputs": [],
   "source": [
    "#TAG_RE = re.compile(r'<[^>]+>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A6fT2ZxY5f95"
   },
   "outputs": [],
   "source": [
    "def remove_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    droped_html_tags = re.sub(clean, ' ', text)\n",
    "    return re.sub(r'\\n|\\r', ' ', droped_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1cnnjs9O0lgv",
    "outputId": "e220014e-c0d8-4843-c6a8-6b648841e135"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1003869it [02:55, 5733.59it/s]\n"
     ]
    }
   ],
   "source": [
    "test_val_indexes = np.random.choice(1003869, 40000, replace=False) # 40000 индексов для теста и валидации\n",
    "\n",
    "with open('ria.json') as f:\n",
    "    for i, line in tqdm(enumerate(f)):\n",
    "        l = json.loads(line)\n",
    "        text = remove_tags(l['text'])\n",
    "        title = remove_tags(l['title'])\n",
    "        if i in test_val_indexes:\n",
    "            with open('texts_val.txt', 'a') as text_file:\n",
    "                text_file.write(text)\n",
    "                text_file.write('\\n')\n",
    "            with open('labels_val.txt', 'a') as label_file:\n",
    "                label_file.write(title)\n",
    "                label_file.write('\\n')\n",
    "        else:\n",
    "            with open('texts.txt', 'a') as text_file:\n",
    "                text_file.write(text)\n",
    "                text_file.write('\\n')\n",
    "            with open('labels.txt', 'a') as label_file:\n",
    "                label_file.write(title)\n",
    "                label_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X1y1JbVxQzFZ"
   },
   "outputs": [],
   "source": [
    "with open(\"texts_val.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    with open(\"val_texts.txt\", \"w\") as f1:\n",
    "        f1.writelines(lines[:20000])\n",
    "    with open(\"test_texts.txt\", \"w\") as f2:\n",
    "        f2.writelines(lines[20000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vM7gmTwFRtCM"
   },
   "outputs": [],
   "source": [
    "with open(\"labels_val.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    with open(\"val_labels.txt\", \"w\") as f1:\n",
    "        f1.writelines(lines[:20000])\n",
    "    with open(\"test_labels.txt\", \"w\") as f2:\n",
    "        f2.writelines(lines[20000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K4Mg7n9Ldq8y"
   },
   "source": [
    "### 1. BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TF86zazGT4ps"
   },
   "outputs": [],
   "source": [
    "# ~10 минут\n",
    "tokenizer = pyonmttok.Tokenizer(\"conservative\", joiner_annotate=True)\n",
    "learner = pyonmttok.BPELearner(tokenizer=tokenizer, symbols=50000)\n",
    "learner.ingest_file(\"texts.txt\")\n",
    "BPE = learner.learn(\"model-50k_with_joiner\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('lstm_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ekToGL7VCQU"
   },
   "outputs": [],
   "source": [
    "# ~ 20 минут\n",
    "BPE.tokenize_file(input_path='texts.txt', output_path='lstm_v2/texts_tokenized_train')\n",
    "BPE.tokenize_file(input_path='labels.txt', output_path='lstm_v2/labels_tokenized_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3IQkGB2xaTqq"
   },
   "outputs": [],
   "source": [
    "BPE.tokenize_file(input_path='val_texts.txt', output_path='lstm_v2/texts_tokenized_val')\n",
    "BPE.tokenize_file(input_path='val_labels.txt', output_path='lstm_v2/labels_tokenized_val')\n",
    "BPE.tokenize_file(input_path='test_texts.txt', output_path='lstm_v2/texts_tokenized_test')\n",
    "BPE.tokenize_file(input_path='test_labels.txt', output_path='lstm_v2/labels_tokenized_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cahl6LKJd9lJ"
   },
   "outputs": [],
   "source": [
    "os.remove('texts.txt')\n",
    "os.remove('labels.txt')\n",
    "os.remove('val_texts.txt')\n",
    "os.remove('val_labels.txt')\n",
    "os.remove('test_texts.txt')\n",
    "os.remove('test_labels.txt')\n",
    "os.remove('texts_val.txt')\n",
    "os.remove('labels_val.txt')\n",
    "os.remove('lstm_v2/texts_tokenized_train')\n",
    "os.remove('lstm_v2/labels_tokenized_train')\n",
    "os.remove('lstm_v2/texts_tokenized_val')\n",
    "os.remove('lstm_v2/labels_tokenized_val')\n",
    "os.remove('lstm_v2/texts_tokenized_test')\n",
    "os.remove('lstm_v2/labels_tokenized_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### очистка от пустых строк после BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lstm_v2/labels_tokenized_test') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = []\n",
    "for i, line in enumerate(lines):\n",
    "    if line == '\\n':\n",
    "        nulls.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:00, 61913.21it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, line in tqdm(enumerate(lines)):\n",
    "    if i not in nulls:\n",
    "        with open('lstm_v2/clear_labels_tokenized_test', 'a') as f:\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ICDIzVBdk77"
   },
   "source": [
    "### 2. Препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MqtJ1L2ScvRg"
   },
   "outputs": [],
   "source": [
    "#os.mkdir('lstm_v2/preprocessed_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clear_texts_tokenized_val',\n",
       " 'clear_labels_tokenized_train',\n",
       " 'clear_labels_tokenized_val',\n",
       " 'clear_texts_tokenized_test',\n",
       " 'clear_texts_tokenized_train',\n",
       " 'preprocessed_data',\n",
       " 'clear_labels_tokenized_test']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('lstm_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "qP64vnRvZpHR",
    "outputId": "c1542917-c41f-470f-bdca-0e13317f3995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-01-31 07:38:54,886 INFO] Extracting features...\n",
      "[2020-01-31 07:38:54,886 INFO]  * number of source features: 0.\n",
      "[2020-01-31 07:38:54,886 INFO]  * number of target features: 0.\n",
      "[2020-01-31 07:38:54,886 INFO] Building `Fields` object...\n",
      "[2020-01-31 07:38:54,887 INFO] Building & saving training data...\n",
      "[2020-01-31 07:38:55,845 INFO] Building shard 0.\n",
      "[2020-01-31 07:38:56,600 INFO] Building shard 1.\n",
      "[2020-01-31 07:38:57,385 INFO] Building shard 2.\n",
      "[2020-01-31 07:38:58,131 INFO] Building shard 3.\n",
      "[2020-01-31 07:38:58,868 INFO] Building shard 4.\n",
      "[2020-01-31 07:38:59,650 INFO] Building shard 5.\n",
      "[2020-01-31 07:39:05,643 INFO]  * saving 0th train data shard to lstm_v2/preprocessed_data/preprocessed_data.train.0.pt.\n",
      "[2020-01-31 07:39:07,230 INFO]  * saving 2th train data shard to lstm_v2/preprocessed_data/preprocessed_data.train.2.pt.\n",
      "[2020-01-31 07:39:10,654 INFO]  * saving 1th train data shard to lstm_v2/preprocessed_data/preprocessed_data.train.1.pt.\n",
      "[2020-01-31 07:39:12,474 INFO]  * saving 3th train data shard to lstm_v2/preprocessed_data/preprocessed_data.train.3.pt.\n",
      "[2020-01-31 07:39:13,554 INFO]  * saving 4th train data shard to lstm_v2/preprocessed_data/preprocessed_data.train.4.pt.\n",
      "[2020-01-31 07:39:13,946 INFO]  * saving 5th train data shard to lstm_v2/preprocessed_data/preprocessed_data.train.5.pt.\n",
      "[2020-01-31 07:39:24,652 INFO] Building shard 6.\n",
      "[2020-01-31 07:39:26,672 INFO] Building shard 7.\n",
      "[2020-01-31 07:39:31,420 INFO] Building shard 8.\n",
      "[2020-01-31 07:39:35,782 INFO] Building shard 9.\n",
      "[2020-01-31 07:39:36,890 INFO] Building shard 10.\n",
      "[2020-01-31 07:39:38,646 INFO]  * saving 6th train data shard to lstm_v2/preprocessed_data/preprocessed_data.train.6.pt.\n",
      "[2020-01-31 07:39:40,015 INFO] Building shard 11.\n",
      "[2020-01-31 07:39:41,501 INFO]  * saving 7th train data shard to lstm_v2/preprocessed_data/preprocessed_data.train.7.pt.\n",
      "[2020-01-31 07:39:44,103 INFO]  * saving 8th train data shard to lstm_v2/preprocessed_data/preprocessed_data.train.8.pt.\n",
      "[2020-01-31 07:39:47,213 INFO]  * saving 10th train data shard to lstm_v2/preprocessed_data/preprocessed_data.train.10.pt.\n",
      "[2020-01-31 07:39:50,337 INFO]  * saving 9th train data shard to lstm_v2/preprocessed_data/preprocessed_data.train.9.pt.\n",
      "[2020-01-31 07:39:56,640 INFO]  * saving 11th train data shard to lstm_v2/preprocessed_data/preprocessed_data.train.11.pt.\n",
      "[2020-01-31 07:40:03,530 INFO] Building shard 12.\n",
      "[2020-01-31 07:40:05,626 INFO] Building shard 13.\n",
      "[2020-01-31 07:40:06,857 INFO] Building shard 14.\n",
      "[2020-01-31 07:40:10,322 INFO] Building shard 15.\n",
      "[2020-01-31 07:40:17,468 INFO]  * saving 13th train data shard to lstm_v2/preprocessed_data/preprocessed_data.train.13.pt.\n",
      "[2020-01-31 07:40:19,756 INFO]  * saving 14th train data shard to lstm_v2/preprocessed_data/preprocessed_data.train.14.pt.\n",
      "[2020-01-31 07:40:20,140 INFO]  * saving 12th train data shard to lstm_v2/preprocessed_data/preprocessed_data.train.12.pt.\n",
      "[2020-01-31 07:40:20,760 INFO] Building shard 16.\n",
      "[2020-01-31 07:40:27,784 INFO]  * saving 15th train data shard to lstm_v2/preprocessed_data/preprocessed_data.train.15.pt.\n",
      "[2020-01-31 07:40:28,087 INFO] Building shard 17.\n",
      "[2020-01-31 07:40:38,089 INFO]  * saving 16th train data shard to lstm_v2/preprocessed_data/preprocessed_data.train.16.pt.\n",
      "[2020-01-31 07:40:45,029 INFO] Building shard 18.\n",
      "[2020-01-31 07:40:46,077 INFO]  * saving 17th train data shard to lstm_v2/preprocessed_data/preprocessed_data.train.17.pt.\n",
      "[2020-01-31 07:40:47,676 INFO] Building shard 19.\n",
      "[2020-01-31 07:40:51,472 INFO]  * saving 19th train data shard to lstm_v2/preprocessed_data/preprocessed_data.train.19.pt.\n",
      "[2020-01-31 07:40:56,932 INFO]  * saving 18th train data shard to lstm_v2/preprocessed_data/preprocessed_data.train.18.pt.\n",
      "[2020-01-31 07:41:17,253 INFO]  * tgt vocab size: 48950.\n",
      "[2020-01-31 07:41:17,343 INFO]  * src vocab size: 50002.\n",
      "[2020-01-31 07:41:17,625 INFO] Building & saving validation data...\n",
      "[2020-01-31 07:41:18,316 INFO] Building shard 0.\n",
      "[2020-01-31 07:41:20,266 INFO]  * saving 0th valid data shard to lstm_v2/preprocessed_data/preprocessed_data.valid.0.pt.\n"
     ]
    }
   ],
   "source": [
    "! onmt_preprocess -train_src lstm_v2/clear_texts_tokenized_train \\\n",
    "                  -train_tgt lstm_v2/clear_labels_tokenized_train \\\n",
    "                  -valid_src lstm_v2/clear_texts_tokenized_val \\\n",
    "                  -valid_tgt lstm_v2/clear_labels_tokenized_val \\\n",
    "                  -src_seq_length 3000 \\\n",
    "                  -tgt_seq_length 1000 \\\n",
    "                  -filter_valid \\\n",
    "                  -lower \\\n",
    "                  -shard_size 50000 \\\n",
    "                  -num_threads 6 \\\n",
    "                  -save_data lstm_v2/preprocessed_data/preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-01-31 07:44:03,360 INFO]  * src vocab size = 50002\n",
      "[2020-01-31 07:44:03,360 INFO]  * tgt vocab size = 48950\n",
      "[2020-01-31 07:44:03,360 INFO] Building model...\n",
      "[2020-01-31 07:44:09,210 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(50002, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(500, 500, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(48950, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(1000, 500)\n",
      "        (1): LSTMCell(500, 500)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=48950, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "[2020-01-31 07:44:09,211 INFO] encoder: 29009000\n",
      "[2020-01-31 07:44:09,211 INFO] decoder: 54756950\n",
      "[2020-01-31 07:44:09,211 INFO] * number of parameters: 83765950\n",
      "[2020-01-31 07:44:09,675 INFO] Starting training on GPU: [0]\n",
      "[2020-01-31 07:44:09,675 INFO] Start training loop and validate every 10000 steps...\n",
      "[2020-01-31 07:44:09,675 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.0.pt\n",
      "[2020-01-31 07:44:15,511 INFO] number of examples: 49871\n",
      "[2020-01-31 07:49:18,807 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.1.pt\n",
      "[2020-01-31 07:49:25,274 INFO] number of examples: 49840\n",
      "[2020-01-31 07:54:43,158 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.10.pt\n",
      "[2020-01-31 07:54:49,981 INFO] number of examples: 49779\n",
      "[2020-01-31 07:58:00,170 INFO] Step 2000/60000; acc:  21.22; ppl: 573.89; xent: 6.35; lr: 0.00100; 52415/1780 tok/s;    830 sec\n",
      "[2020-01-31 08:00:24,988 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.11.pt\n",
      "[2020-01-31 08:00:31,791 INFO] number of examples: 49662\n",
      "[2020-01-31 08:06:49,982 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.12.pt\n",
      "[2020-01-31 08:06:56,434 INFO] number of examples: 49699\n",
      "[2020-01-31 08:12:58,491 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.13.pt\n",
      "[2020-01-31 08:13:06,326 INFO] number of examples: 49681\n",
      "[2020-01-31 08:13:58,321 INFO] Step 4000/60000; acc:  34.57; ppl: 104.03; xent: 4.64; lr: 0.00100; 55008/1595 tok/s;   1789 sec\n",
      "[2020-01-31 08:19:15,207 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.14.pt\n",
      "[2020-01-31 08:19:22,073 INFO] number of examples: 49621\n",
      "[2020-01-31 08:22:12,129 INFO] Saving checkpoint lstm_v2/lstm_model_v2_step_5000.pt\n",
      "[2020-01-31 08:25:49,980 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.15.pt\n",
      "[2020-01-31 08:25:57,695 INFO] number of examples: 49584\n",
      "[2020-01-31 08:30:28,227 INFO] Step 6000/60000; acc:  42.76; ppl: 47.58; xent: 3.86; lr: 0.00100; 56206/1538 tok/s;   2779 sec\n",
      "[2020-01-31 08:32:23,427 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.16.pt\n",
      "[2020-01-31 08:32:30,356 INFO] number of examples: 49659\n",
      "[2020-01-31 08:38:53,470 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.17.pt\n",
      "[2020-01-31 08:39:01,439 INFO] number of examples: 49639\n",
      "[2020-01-31 08:45:31,886 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.18.pt\n",
      "[2020-01-31 08:45:38,691 INFO] number of examples: 49708\n",
      "[2020-01-31 08:47:35,150 INFO] Step 8000/60000; acc:  47.02; ppl: 31.19; xent: 3.44; lr: 0.00100; 56346/1495 tok/s;   3805 sec\n",
      "[2020-01-31 08:51:43,868 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.19.pt\n",
      "[2020-01-31 08:51:45,828 INFO] number of examples: 13706\n",
      "[2020-01-31 08:53:28,671 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.2.pt\n",
      "[2020-01-31 08:53:34,823 INFO] number of examples: 49852\n",
      "[2020-01-31 08:58:57,304 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.3.pt\n",
      "[2020-01-31 08:59:03,710 INFO] number of examples: 49871\n",
      "[2020-01-31 09:02:05,145 INFO] Step 10000/60000; acc:  50.13; ppl: 23.88; xent: 3.17; lr: 0.00100; 53001/1784 tok/s;   4675 sec\n",
      "[2020-01-31 09:02:05,147 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.valid.0.pt\n",
      "[2020-01-31 09:02:08,220 INFO] number of examples: 19921\n",
      "[2020-01-31 09:03:45,428 INFO] Validation perplexity: 20.8833\n",
      "[2020-01-31 09:03:45,429 INFO] Validation accuracy: 51.2595\n",
      "[2020-01-31 09:03:45,744 INFO] Saving checkpoint lstm_v2/lstm_model_v2_step_10000.pt\n",
      "[2020-01-31 09:06:06,315 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.4.pt\n",
      "[2020-01-31 09:06:13,284 INFO] number of examples: 49828\n",
      "[2020-01-31 09:11:39,930 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.5.pt\n",
      "[2020-01-31 09:11:46,444 INFO] number of examples: 49814\n",
      "[2020-01-31 09:17:06,337 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.6.pt\n",
      "[2020-01-31 09:17:11,409 INFO] number of examples: 49816\n",
      "[2020-01-31 09:17:58,325 INFO] Step 12000/60000; acc:  51.41; ppl: 20.29; xent: 3.01; lr: 0.00100; 46244/1630 tok/s;   5629 sec\n",
      "[2020-01-31 09:22:30,928 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.7.pt\n",
      "[2020-01-31 09:22:38,339 INFO] number of examples: 49823\n",
      "[2020-01-31 09:28:05,666 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.8.pt\n",
      "[2020-01-31 09:28:11,164 INFO] number of examples: 49818\n",
      "[2020-01-31 09:32:10,514 INFO] Step 14000/60000; acc:  52.99; ppl: 17.10; xent: 2.84; lr: 0.00100; 52791/1828 tok/s;   6481 sec\n",
      "[2020-01-31 09:33:44,734 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.9.pt\n",
      "[2020-01-31 09:33:52,560 INFO] number of examples: 49788\n",
      "[2020-01-31 09:39:31,386 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.0.pt\n",
      "[2020-01-31 09:39:36,336 INFO] number of examples: 49871\n",
      "[2020-01-31 09:39:39,293 INFO] Saving checkpoint lstm_v2/lstm_model_v2_step_15000.pt\n",
      "[2020-01-31 09:44:49,557 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.1.pt\n",
      "[2020-01-31 09:44:56,859 INFO] number of examples: 49840\n",
      "[2020-01-31 09:46:29,538 INFO] Step 16000/60000; acc:  53.09; ppl: 16.76; xent: 2.82; lr: 0.00100; 52236/1732 tok/s;   7340 sec\n",
      "[2020-01-31 09:50:15,235 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.10.pt\n",
      "[2020-01-31 09:50:22,052 INFO] number of examples: 49779\n",
      "[2020-01-31 09:55:57,424 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.11.pt\n",
      "[2020-01-31 09:56:03,759 INFO] number of examples: 49662\n",
      "[2020-01-31 10:01:27,628 INFO] Step 18000/60000; acc:  53.73; ppl: 15.32; xent: 2.73; lr: 0.00100; 54184/1691 tok/s;   8238 sec\n",
      "[2020-01-31 10:02:22,474 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.12.pt\n",
      "[2020-01-31 10:02:29,486 INFO] number of examples: 49699\n",
      "[2020-01-31 10:08:31,757 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.13.pt\n",
      "[2020-01-31 10:08:38,731 INFO] number of examples: 49681\n",
      "[2020-01-31 10:14:47,787 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.14.pt\n",
      "[2020-01-31 10:14:55,010 INFO] number of examples: 49621\n",
      "[2020-01-31 10:17:47,769 INFO] Step 20000/60000; acc:  53.90; ppl: 14.82; xent: 2.70; lr: 0.00100; 55333/1556 tok/s;   9218 sec\n",
      "[2020-01-31 10:17:47,771 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.valid.0.pt\n",
      "[2020-01-31 10:17:50,603 INFO] number of examples: 19921\n",
      "[2020-01-31 10:19:29,863 INFO] Validation perplexity: 13.337\n",
      "[2020-01-31 10:19:29,864 INFO] Validation accuracy: 55.5253\n",
      "[2020-01-31 10:19:30,188 INFO] Saving checkpoint lstm_v2/lstm_model_v2_step_20000.pt\n",
      "[2020-01-31 10:23:05,955 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.15.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-01-31 10:23:14,281 INFO] number of examples: 49584\n",
      "[2020-01-31 10:29:40,865 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.16.pt\n",
      "[2020-01-31 10:29:47,226 INFO] number of examples: 49659\n",
      "[2020-01-31 10:36:10,741 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.17.pt\n",
      "[2020-01-31 10:36:18,332 INFO] number of examples: 49639\n",
      "[2020-01-31 10:36:25,245 INFO] Step 22000/60000; acc:  55.39; ppl: 13.08; xent: 2.57; lr: 0.00100; 50897/1364 tok/s;  10336 sec\n",
      "[2020-01-31 10:42:48,703 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.18.pt\n",
      "[2020-01-31 10:42:55,776 INFO] number of examples: 49708\n",
      "[2020-01-31 10:49:00,849 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.19.pt\n",
      "[2020-01-31 10:49:03,243 INFO] number of examples: 13706\n",
      "[2020-01-31 10:50:46,190 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.2.pt\n",
      "[2020-01-31 10:50:52,351 INFO] number of examples: 49852\n",
      "[2020-01-31 10:52:28,103 INFO] Step 24000/60000; acc:  56.49; ppl: 12.05; xent: 2.49; lr: 0.00100; 55300/1621 tok/s;  11298 sec\n",
      "[2020-01-31 10:56:14,245 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.3.pt\n",
      "[2020-01-31 10:56:20,507 INFO] number of examples: 49871\n",
      "[2020-01-31 10:59:24,598 INFO] Saving checkpoint lstm_v2/lstm_model_v2_step_25000.pt\n",
      "[2020-01-31 11:01:42,140 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.4.pt\n",
      "[2020-01-31 11:01:48,288 INFO] number of examples: 49828\n",
      "[2020-01-31 11:06:34,959 INFO] Step 26000/60000; acc:  57.06; ppl: 11.54; xent: 2.45; lr: 0.00100; 52357/1815 tok/s;  12145 sec\n",
      "[2020-01-31 11:07:14,328 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.5.pt\n",
      "[2020-01-31 11:07:20,350 INFO] number of examples: 49814\n",
      "[2020-01-31 11:12:39,329 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.6.pt\n",
      "[2020-01-31 11:12:45,340 INFO] number of examples: 49816\n",
      "[2020-01-31 11:18:04,263 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.7.pt\n",
      "[2020-01-31 11:18:10,477 INFO] number of examples: 49823\n",
      "[2020-01-31 11:20:28,155 INFO] Step 28000/60000; acc:  57.89; ppl: 10.54; xent: 2.35; lr: 0.00100; 51937/1875 tok/s;  12978 sec\n",
      "[2020-01-31 11:23:37,732 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.8.pt\n",
      "[2020-01-31 11:23:44,249 INFO] number of examples: 49818\n",
      "[2020-01-31 11:29:17,776 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.9.pt\n",
      "[2020-01-31 11:29:24,358 INFO] number of examples: 49788\n",
      "[2020-01-31 11:35:02,905 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.0.pt\n",
      "[2020-01-31 11:35:08,913 INFO] number of examples: 49871\n",
      "[2020-01-31 11:35:14,376 INFO] Step 30000/60000; acc:  57.68; ppl: 10.49; xent: 2.35; lr: 0.00100; 53154/1736 tok/s;  13865 sec\n",
      "[2020-01-31 11:35:14,379 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.valid.0.pt\n",
      "[2020-01-31 11:35:17,113 INFO] number of examples: 19921\n",
      "[2020-01-31 11:36:54,165 INFO] Validation perplexity: 10.1422\n",
      "[2020-01-31 11:36:54,165 INFO] Validation accuracy: 58.6188\n",
      "[2020-01-31 11:36:54,469 INFO] Saving checkpoint lstm_v2/lstm_model_v2_step_30000.pt\n",
      "[2020-01-31 11:42:01,399 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.1.pt\n",
      "[2020-01-31 11:42:09,391 INFO] number of examples: 49840\n",
      "[2020-01-31 11:47:27,982 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.10.pt\n",
      "[2020-01-31 11:47:33,546 INFO] number of examples: 49779\n",
      "[2020-01-31 11:50:52,389 INFO] Step 32000/60000; acc:  57.86; ppl: 10.61; xent: 2.36; lr: 0.00100; 46712/1577 tok/s;  14803 sec\n",
      "[2020-01-31 11:53:08,835 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.11.pt\n",
      "[2020-01-31 11:53:16,065 INFO] number of examples: 49662\n",
      "[2020-01-31 11:59:35,045 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.12.pt\n",
      "[2020-01-31 11:59:42,180 INFO] number of examples: 49699\n",
      "[2020-01-31 12:05:46,749 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.13.pt\n",
      "[2020-01-31 12:05:52,844 INFO] number of examples: 49681\n",
      "[2020-01-31 12:06:52,909 INFO] Step 34000/60000; acc:  57.42; ppl: 10.44; xent: 2.35; lr: 0.00100; 54814/1591 tok/s;  15763 sec\n",
      "[2020-01-31 12:12:02,404 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.14.pt\n",
      "[2020-01-31 12:12:10,096 INFO] number of examples: 49621\n",
      "[2020-01-31 12:15:05,406 INFO] Saving checkpoint lstm_v2/lstm_model_v2_step_35000.pt\n",
      "[2020-01-31 12:18:38,740 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.15.pt\n",
      "[2020-01-31 12:18:46,070 INFO] number of examples: 49584\n",
      "[2020-01-31 12:23:29,082 INFO] Step 36000/60000; acc:  57.84; ppl: 10.22; xent: 2.32; lr: 0.00100; 55974/1529 tok/s;  16759 sec\n",
      "[2020-01-31 12:25:14,324 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.16.pt\n",
      "[2020-01-31 12:25:20,779 INFO] number of examples: 49659\n",
      "[2020-01-31 12:31:44,315 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.17.pt\n",
      "[2020-01-31 12:31:51,787 INFO] number of examples: 49639\n",
      "[2020-01-31 12:38:21,554 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.18.pt\n",
      "[2020-01-31 12:38:29,985 INFO] number of examples: 49708\n",
      "[2020-01-31 12:40:32,224 INFO] Step 38000/60000; acc:  58.52; ppl:  9.64; xent: 2.27; lr: 0.00100; 56326/1500 tok/s;  17783 sec\n",
      "[2020-01-31 12:44:34,544 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.19.pt\n",
      "[2020-01-31 12:44:35,756 INFO] number of examples: 13706\n",
      "[2020-01-31 12:46:18,887 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.2.pt\n",
      "[2020-01-31 12:46:25,076 INFO] number of examples: 49852\n",
      "[2020-01-31 12:51:46,482 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.3.pt\n",
      "[2020-01-31 12:51:52,680 INFO] number of examples: 49871\n",
      "[2020-01-31 12:54:59,845 INFO] Step 40000/60000; acc:  60.03; ppl:  8.82; xent: 2.18; lr: 0.00100; 53141/1789 tok/s;  18650 sec\n",
      "[2020-01-31 12:54:59,847 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.valid.0.pt\n",
      "[2020-01-31 12:55:01,412 INFO] number of examples: 19921\n",
      "[2020-01-31 12:56:38,027 INFO] Validation perplexity: 9.38483\n",
      "[2020-01-31 12:56:38,027 INFO] Validation accuracy: 59.5273\n",
      "[2020-01-31 12:56:38,320 INFO] Saving checkpoint lstm_v2/lstm_model_v2_step_40000.pt\n",
      "[2020-01-31 12:58:52,578 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.4.pt\n",
      "[2020-01-31 12:58:59,954 INFO] number of examples: 49828\n",
      "[2020-01-31 13:04:26,101 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.5.pt\n",
      "[2020-01-31 13:04:31,058 INFO] number of examples: 49814\n",
      "[2020-01-31 13:09:50,478 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.6.pt\n",
      "[2020-01-31 13:09:56,418 INFO] number of examples: 49816\n",
      "[2020-01-31 13:10:47,615 INFO] Step 42000/60000; acc:  59.92; ppl:  8.73; xent: 2.17; lr: 0.00100; 46381/1640 tok/s;  19598 sec\n",
      "[2020-01-31 13:15:14,987 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.7.pt\n",
      "[2020-01-31 13:15:21,307 INFO] number of examples: 49823\n",
      "[2020-01-31 13:20:48,693 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.8.pt\n",
      "[2020-01-31 13:20:56,264 INFO] number of examples: 49818\n",
      "[2020-01-31 13:24:59,773 INFO] Step 44000/60000; acc:  60.43; ppl:  8.29; xent: 2.11; lr: 0.00100; 52799/1827 tok/s;  20450 sec\n",
      "[2020-01-31 13:26:29,674 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.9.pt\n",
      "[2020-01-31 13:26:35,111 INFO] number of examples: 49788\n",
      "[2020-01-31 13:32:13,781 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.0.pt\n",
      "[2020-01-31 13:32:21,072 INFO] number of examples: 49871\n",
      "[2020-01-31 13:32:28,734 INFO] Saving checkpoint lstm_v2/lstm_model_v2_step_45000.pt\n",
      "[2020-01-31 13:37:33,889 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.1.pt\n",
      "[2020-01-31 13:37:39,086 INFO] number of examples: 49840\n",
      "[2020-01-31 13:39:15,324 INFO] Step 46000/60000; acc:  59.80; ppl:  8.70; xent: 2.16; lr: 0.00100; 52400/1738 tok/s;  21306 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-01-31 13:42:57,311 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.10.pt\n",
      "[2020-01-31 13:43:05,171 INFO] number of examples: 49779\n",
      "[2020-01-31 13:48:40,503 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.11.pt\n",
      "[2020-01-31 13:48:46,503 INFO] number of examples: 49662\n",
      "[2020-01-31 13:54:16,062 INFO] Step 48000/60000; acc:  59.84; ppl:  8.56; xent: 2.15; lr: 0.00100; 54198/1687 tok/s;  22206 sec\n",
      "[2020-01-31 13:55:04,761 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.12.pt\n",
      "[2020-01-31 13:55:13,031 INFO] number of examples: 49699\n",
      "[2020-01-31 14:01:15,758 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.13.pt\n",
      "[2020-01-31 14:01:21,821 INFO] number of examples: 49681\n",
      "[2020-01-31 14:07:32,813 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.14.pt\n",
      "[2020-01-31 14:07:39,465 INFO] number of examples: 49621\n",
      "[2020-01-31 14:10:38,336 INFO] Step 50000/60000; acc:  59.34; ppl:  8.80; xent: 2.17; lr: 0.00050; 55220/1552 tok/s;  23189 sec\n",
      "[2020-01-31 14:10:38,339 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.valid.0.pt\n",
      "[2020-01-31 14:10:41,359 INFO] number of examples: 19921\n",
      "[2020-01-31 14:12:18,587 INFO] Validation perplexity: 9.15901\n",
      "[2020-01-31 14:12:18,587 INFO] Validation accuracy: 59.61\n",
      "[2020-01-31 14:12:18,895 INFO] Saving checkpoint lstm_v2/lstm_model_v2_step_50000.pt\n",
      "[2020-01-31 14:15:48,863 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.15.pt\n",
      "[2020-01-31 14:15:56,594 INFO] number of examples: 49584\n",
      "[2020-01-31 14:22:23,597 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.16.pt\n",
      "[2020-01-31 14:22:31,544 INFO] number of examples: 49659\n",
      "[2020-01-31 14:28:56,010 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.17.pt\n",
      "[2020-01-31 14:29:03,612 INFO] number of examples: 49639\n",
      "[2020-01-31 14:29:19,817 INFO] Step 52000/60000; acc:  60.99; ppl:  7.87; xent: 2.06; lr: 0.00050; 50802/1360 tok/s;  24310 sec\n",
      "[2020-01-31 14:35:36,164 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.18.pt\n",
      "[2020-01-31 14:35:42,387 INFO] number of examples: 49708\n",
      "[2020-01-31 14:41:47,488 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.19.pt\n",
      "[2020-01-31 14:41:49,814 INFO] number of examples: 13706\n",
      "[2020-01-31 14:43:33,239 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.2.pt\n",
      "[2020-01-31 14:43:39,839 INFO] number of examples: 49852\n",
      "[2020-01-31 14:45:20,124 INFO] Step 54000/60000; acc:  62.05; ppl:  7.31; xent: 1.99; lr: 0.00050; 55200/1626 tok/s;  25270 sec\n",
      "[2020-01-31 14:49:02,476 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.3.pt\n",
      "[2020-01-31 14:49:08,914 INFO] number of examples: 49871\n",
      "[2020-01-31 14:52:18,842 INFO] Saving checkpoint lstm_v2/lstm_model_v2_step_55000.pt\n",
      "[2020-01-31 14:54:31,306 INFO] Loading dataset from lstm_v2/preprocessed_data/preprocessed_data.train.4.pt\n",
      "[2020-01-31 14:54:38,036 INFO] number of examples: 49828\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sergey_kechin/anaconda3/bin/onmt_train\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/sergey_kechin/anaconda3/lib/python3.7/site-packages/onmt/bin/train.py\", line 204, in main\n",
      "    train(opt)\n",
      "  File \"/home/sergey_kechin/anaconda3/lib/python3.7/site-packages/onmt/bin/train.py\", line 88, in train\n",
      "    single_main(opt, 0)\n",
      "  File \"/home/sergey_kechin/anaconda3/lib/python3.7/site-packages/onmt/train_single.py\", line 143, in main\n",
      "    valid_steps=opt.valid_steps)\n",
      "  File \"/home/sergey_kechin/anaconda3/lib/python3.7/site-packages/onmt/trainer.py\", line 244, in train\n",
      "    report_stats)\n",
      "  File \"/home/sergey_kechin/anaconda3/lib/python3.7/site-packages/onmt/trainer.py\", line 365, in _gradient_accumulation\n",
      "    with_align=self.with_align)\n",
      "  File \"/home/sergey_kechin/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/sergey_kechin/anaconda3/lib/python3.7/site-packages/onmt/models/model.py\", line 45, in forward\n",
      "    enc_state, memory_bank, lengths = self.encoder(src, lengths)\n",
      "  File \"/home/sergey_kechin/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/sergey_kechin/anaconda3/lib/python3.7/site-packages/onmt/encoders/rnn_encoder.py\", line 76, in forward\n",
      "    memory_bank, encoder_final = self.rnn(packed_emb)\n",
      "  File \"/home/sergey_kechin/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/sergey_kechin/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\", line 562, in forward\n",
      "    self.num_layers, self.dropout, self.training, self.bidirectional)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!onmt_train -data lstm_v2/preprocessed_data/preprocessed_data  \\\n",
    "            -save_model lstm_v2/lstm_model_v2 \\\n",
    "            -train_steps 60000 \\\n",
    "            -valid_steps 10000 \\\n",
    "            -optim adam \\\n",
    "            -learning_rate 0.001 \\\n",
    "            -report_every 2000 \\\n",
    "            -gpu_ranks 0 \\\n",
    "            -tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-01-31 14:55:57,690 INFO] Translating shard 0.\n",
      "PRED AVG SCORE: -0.2222, PRED PPL: 1.2488\n",
      "Total translation time (s): 381.589313\n",
      "Average translation time (s): 0.038159\n",
      "Tokens per second: 262.900445\n",
      "[2020-01-31 15:02:20,377 INFO] Translating shard 1.\n",
      "PRED AVG SCORE: -0.2205, PRED PPL: 1.2467\n",
      "Total translation time (s): 379.849602\n",
      "Average translation time (s): 0.037985\n",
      "Tokens per second: 267.390039\n"
     ]
    }
   ],
   "source": [
    "! onmt_translate -gpu 0 \\\n",
    "               -batch_size 8 \\\n",
    "               -beam_size 10 \\\n",
    "               -model lstm_v2/lstm_model_v2_step_55000.pt \\\n",
    "               -src lstm_v2/clear_texts_tokenized_test \\\n",
    "               -output lstm_v2/pred_test.txt \\\n",
    "               -report_time \\\n",
    "               -min_length 1 \\\n",
    "               -stepwise_penalty \\\n",
    "               -coverage_penalty summary \\\n",
    "               -beta 5 \\\n",
    "               -length_penalty wu \\\n",
    "               -alpha 0.9 \\\n",
    "               -block_ngram_repeat 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPE.detokenize_file(input_path='lstm_v2/pred_test.txt', output_path='lstm_v2/pred_test_detokenized.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPE.detokenize_file(input_path='lstm_v2/clear_labels_tokenized_test', output_path='lstm_v2/true_test_detokenized.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Очистка от пустых настоящих заголовков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lstm_v2/pred_test_detokenized.txt') as f1:\n",
    "    predictions = f1.readlines()\n",
    "with open('lstm_v2/true_test_detokenized.txt') as f2:\n",
    "    true = f2.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2612 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, element in enumerate(true):\n",
    "    if len(element) < 5:\n",
    "        print(i, element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, element in enumerate(true):\n",
    "    if i != 2612:        \n",
    "        with open('lstm_v2/true.txt', 'a') as f1:\n",
    "            f1.write(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, element in enumerate(predictions):\n",
    "    if i != 2612:        \n",
    "        with open('lstm_v2/predictions.txt', 'a') as f2:\n",
    "            f2.write(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.40704861192204495,\n",
       "  'p': 0.4254433360751212,\n",
       "  'r': 0.39838380926281153},\n",
       " 'rouge-2': {'f': 0.22824682595138046,\n",
       "  'p': 0.23940092178879546,\n",
       "  'r': 0.22377283102694753},\n",
       " 'rouge-l': {'f': 0.3785419590169131,\n",
       "  'p': 0.40359357600470086,\n",
       "  'r': 0.3778888807812343}}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge import FilesRouge\n",
    "files_rouge_detoc = FilesRouge('lstm_v2/predictions.txt', 'lstm_v2/true.txt')\n",
    "scores_detoc = files_rouge_detoc.get_scores(avg=True)\n",
    "scores_detoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравнение предсказанных заголовков и настоящих"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['греция попросила помощи ес в борьбе с лесными пожарами\\n',\n",
       " 'российские инспекторы совершат наблюдательные полеты над сша и канадой\\n',\n",
       " 'глушаков выйдет в стартовом составе сборной рф на матч против греции\\n',\n",
       " 'чуров ради ордена александра невского отказался от другой награды\\n',\n",
       " 'митрохин снова стал председателем \"яблока\"\\n',\n",
       " 'второй за день взрыв прогремел в багдаде, более 10 человек погибли\\n',\n",
       " 'заработал взломанный накануне сайт поддержки егэ в петербурге\\n',\n",
       " 'лавров обсудил с аннаном перспективы проведения конференции по сирии\\n',\n",
       " 'беспорядки вспыхнули в колонии на ставрополье\\n',\n",
       " 'власти испании позитивно восприняли доклад мвф\\n',\n",
       " 'биография игоря зубова\\n',\n",
       " 'замглавы мвд алешин снят с должности\\n',\n",
       " '\"новая голландия\" 16 июня вновь открывается для посетителей\\n',\n",
       " 'выборы губернатора в новгородской области обойдутся в 59 млн руб\\n',\n",
       " 'дело заведено на возможного виновника дтп, где погиб полицейский\\n',\n",
       " 'константин зырянов пропустил тренировку сборной россии из-за простуды\\n',\n",
       " 'биржи европы закрылись ростом в ожидании мер по поддержке экономики\\n',\n",
       " 'состояние жкх лужского района шокировало главу ленобласти\\n',\n",
       " 'глава мид рф и замгенсека оон обсудили гуманитарную помощь сирии\\n',\n",
       " 'музей \"огни москвы\", или как столица офонарела\\n',\n",
       " 'поклонный крест святой евдокии московской установят на рождественском бульваре в москве\\n',\n",
       " 'солнце выжигает поля подсолнухов в калмыкии\\n',\n",
       " '\"северная верфь\" отказалась от иска к \"коммерсанту\" о защите репутации\\n',\n",
       " 'маркин: ряд свидетелей по делу о болотной могут стать подозреваемыми\\n',\n",
       " 'питание соответствует нормам в 10% проверенных школ и детсадов москвы\\n',\n",
       " 'режиссер питер гринуэй сравнил российское кино со старым вином\\n',\n",
       " 'стресс заставляет кузнечиков есть много углеводов, выяснили ученые\\n',\n",
       " 'сасовское летное училище получило новую впп\\n',\n",
       " 'в подмосковном красногорске прошел фестиваль народного искусства\\n',\n",
       " 'население новокузнецка сократилось за восемь лет на 17 тысяч человек\\n']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['греция попросила ес помочь в борьбе с лесными пожарами\\n',\n",
       " 'российские военные инспекторы совершат наблюдательные полеты над сша\\n',\n",
       " 'футболист глушаков выйдет в стартовом составе сборной россии по футболу\\n',\n",
       " 'чуров попросил наградить александра невского орденом александра невского\\n',\n",
       " '\"яблоко\" переизбрал митрохина на пост председателя партии\\n',\n",
       " 'число погибших при взрыве в багдаде возросло до 11 человек\\n',\n",
       " 'сайт поддержки егэ, взломанный накануне, заработал в петербурге\\n',\n",
       " 'лавров и аннан обсудили перспективы конференции по сирии\\n',\n",
       " 'беспорядки произошли на ставрополье, данных о пострадавших нет\\n',\n",
       " 'испания позитивно восприняла доклад мвф\\n',\n",
       " 'биография зубов зубов\\n',\n",
       " 'замглавы мвд рф, курировавший криминалом, снят с должности\\n',\n",
       " 'памятник архитектуры \"новая голландия\" открывается в петербурге\\n',\n",
       " 'новгородские власти направят 59 млн рублей на выборы губернатора\\n',\n",
       " 'житель тамбовской области насмерть сбил сотрудника дпс\\n',\n",
       " 'футболист зырянов пропустил тренировку перед матчем с греками из-за простуды\\n',\n",
       " 'биржи европы закрылись ростом на внутреннем оптимизме\\n',\n",
       " 'губернатор ленобласти раскритиковал власти за реформу жкх\\n',\n",
       " 'лавров и замгенсека оон обсудили ситуацию в сирии\\n',\n",
       " '\"светить всегда, светить везде\"\\n',\n",
       " 'поклонный крест установят на рождественском бульваре в москве\\n',\n",
       " 'в калмыкии из-за жары введен режим чрезвычайной ситуации\\n',\n",
       " '\"северная верфь\" отказалась от иска к \"коммерсанту\"\\n',\n",
       " 'ск может стать подозреваемыми по делу о беспорядках на болотной\\n',\n",
       " 'лишь каждая десятый школа в москве соответствует санитарным требованиям\\n',\n",
       " 'режиссер питер гринуэй считает, что в россии можно создавать кино\\n',\n",
       " 'кузнечики в состоянии стресса, считают ученые\\n',\n",
       " 'сасовское училище получило новую взлетную полосу\\n',\n",
       " 'фестиваль \"хранители наследия россии\" прошел в красногорске\\n',\n",
       " 'в новокузнецке в 2010 году живут 548 тыс человек\\n']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Transformer trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"PAD\": PAD_token, \"SOS\": SOS_token, \"EOS\": EOS_token}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count PAD, SOS, EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "\n",
    "vocab = Voc('News')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 963781/963781 [01:51<00:00, 8633.57it/s] \n",
      "100%|██████████| 963781/963781 [00:03<00:00, 260758.44it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('lstm_v2/clear_texts_tokenized_train') as train_texts_file:\n",
    "    lines = train_texts_file.readlines()\n",
    "\n",
    "for i in tqdm(range(len(lines))):\n",
    "    vocab.addSentence(lines[i])\n",
    "\n",
    "with open('lstm_v2/clear_labels_tokenized_train') as train_labels_file:\n",
    "    lines = train_labels_file.readlines()\n",
    "\n",
    "for i in tqdm(range(len(lines))):\n",
    "    vocab.addSentence(lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir('trax_transformer/samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "963781it [38:00, 422.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# записываем каждый сэмпл в отдельный файлик + обрезка и паддинг\n",
    "TEXT_MAX_LENGTH = 3000\n",
    "LABEL_MAX_LENGTH = 90\n",
    "\n",
    "with open('lstm_v2/clear_texts_tokenized_train') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open('lstm_v2/clear_labels_tokenized_train') as f:\n",
    "    label_lines = f.readlines()\n",
    "\n",
    "name = 1\n",
    "for text, label in tqdm(zip(lines, label_lines)):\n",
    "    text = text.split(' ')\n",
    "    t_len = len(text)\n",
    "    if t_len > TEXT_MAX_LENGTH:\n",
    "        text = text[:TEXT_MAX_LENGTH]\n",
    "    elif t_len < TEXT_MAX_LENGTH:\n",
    "        text.extend([\"PAD\"] * (TEXT_MAX_LENGTH - t_len))\n",
    "    \n",
    "    label = label.split(' ')\n",
    "    l_len = len(label)\n",
    "    mask = [1] * l_len\n",
    "    if l_len > LABEL_MAX_LENGTH:\n",
    "        label = label[:LABEL_MAX_LENGTH]\n",
    "        mask = mask[:LABEL_MAX_LENGT]\n",
    "    elif l_len < LABEL_MAX_LENGTH:\n",
    "        label.extend([\"PAD\"] * (LABEL_MAX_LENGTH - l_len))\n",
    "        mask.extend([0] * (LABEL_MAX_LENGTH - l_len))\n",
    "    \n",
    "    tokenized_text = []\n",
    "    for token in text:\n",
    "        tokenized_text.append(vocab.word2index[token])\n",
    "\n",
    "    tokenized_label = []\n",
    "    for token in label:\n",
    "        tokenized_label.append(vocab.word2index[token])\n",
    "\n",
    "    output = {'text': tokenized_text, 'mask': mask, 'label': tokenized_label}\n",
    "    \n",
    "    with open('trax_transformer/samples/{}'.format(name), 'w') as text_labels_tokens_file:\n",
    "        json.dump(output, text_labels_tokens_file)\n",
    "    \n",
    "    name +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# даталоадер\n",
    "def news_inputs(vocab, batch_size, files_dir):\n",
    "    files_names = os.listdir(files_dir)\n",
    "    \n",
    "    while True:\n",
    "        batch_indexes = np.random.choice(range(1, len(files_names)+1), size=batch_size, replace=False)\n",
    "        \n",
    "        texts = []\n",
    "        labels = []\n",
    "        mask = []\n",
    "        for index in batch_indexes:\n",
    "            with open(os.path.join(files_dir, str(index))) as file:\n",
    "                sample = json.loads(file.readline())\n",
    "                texts.append(sample['text'])\n",
    "                labels.append(sample['label'])\n",
    "                mask.append(sample['mask'])\n",
    "\n",
    "        texts = np.stack(texts)\n",
    "        labels = np.stack(labels)\n",
    "        mask = np.stack(mask)\n",
    "\n",
    "        yield texts, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_inputs = trax.supervised.Inputs(lambda _: news_inputs(vocab, 8, 'trax_transformer/samples'))\n",
    "data_stream = copy_inputs.train_stream(1)\n",
    "inputs, target, mask = next(data_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 3000)\n",
      "(8, 90)\n",
      "(8, 90)\n",
      "74009\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(target.shape)\n",
    "print(mask.shape)\n",
    "print(len(vocab.word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure hyperparameters.\n",
    "gin.parse_config(\"\"\"\n",
    "import trax.models\n",
    "import trax.optimizers\n",
    "import trax.supervised.inputs\n",
    "import trax.supervised.trainer_lib\n",
    "\n",
    "# Parameters for MultifactorSchedule:\n",
    "# ==============================================================================\n",
    "MultifactorSchedule.constant = 1.0\n",
    "MultifactorSchedule.factors = 'constant * linear_warmup * rsqrt_decay'\n",
    "MultifactorSchedule.warmup_steps = 20000\n",
    "\n",
    "# Parameters for Adafactor:\n",
    "# ==============================================================================\n",
    "Adafactor.beta1 = 0.0\n",
    "Adafactor.decay_rate = 0.8\n",
    "Adafactor.clipping_threshold = 1.0\n",
    "Adafactor.epsilon1 = 1e-30\n",
    "Adafactor.epsilon2 = 0.001\n",
    "Adafactor.factored = True\n",
    "Adafactor.multiply_by_parameter_scale = True\n",
    "\n",
    "\n",
    "# Parameters for train:\n",
    "# ==============================================================================\n",
    "train.optimizer = @trax.optimizers.Adafactor\n",
    "train.id_to_mask = 0\n",
    "\n",
    "# Parameters for Transformer:\n",
    "# ==============================================================================\n",
    "Transformer.d_model = 512\n",
    "Transformer.d_ff = 1024\n",
    "Transformer.dropout = 0.1\n",
    "Transformer.max_len = 3000\n",
    "Transformer.mode = 'train'\n",
    "Transformer.n_heads = 4\n",
    "Transformer.n_encoder_layers = 2\n",
    "Transformer.n_decoder_layers = 2\n",
    "Transformer.input_vocab_size = 74009\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir('trax_transformer/model_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shutil.rmtree('trax_transformer/model_1/eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from trax_transformer/model_1/model.pkl at step 159000\n"
     ]
    }
   ],
   "source": [
    "output_dir = ('trax_transformer/model_1')\n",
    "trainer = trax.supervised.Trainer(\n",
    "    model=trax.models.Transformer,\n",
    "    loss_fn=trax.layers.CrossEntropyLoss,\n",
    "    optimizer=trax.optimizers.Adafactor,\n",
    "    inputs=copy_inputs,\n",
    "    lr_schedule=trax.lr.MultifactorSchedule,\n",
    "    output_dir=output_dir,\n",
    "    has_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step  160000: Ran 1000 train steps in 888.60 secs\n",
      "Step  160000: Evaluation\n",
      "Step  160000: train                   accuracy |  0.61363637\n",
      "Step  160000: train                       loss |  2.13586664\n",
      "Step  160000: train         neg_log_perplexity |  2.13586664\n",
      "Step  160000: train weights_per_batch_per_core |  88.00000000\n",
      "Step  160000: eval                    accuracy |  0.48571429\n",
      "Step  160000: eval                        loss |  2.51269341\n",
      "Step  160000: eval          neg_log_perplexity |  2.51269341\n",
      "Step  160000: eval  weights_per_batch_per_core |  70.00000000\n",
      "Step  160000: Finished evaluation\n",
      "\n",
      "Step  161000: Ran 1000 train steps in 795.50 secs\n",
      "Step  161000: Evaluation\n",
      "Step  161000: train                   accuracy |  0.70652175\n",
      "Step  161000: train                       loss |  1.43594956\n",
      "Step  161000: train         neg_log_perplexity |  1.43594956\n",
      "Step  161000: train weights_per_batch_per_core |  92.00000000\n",
      "Step  161000: eval                    accuracy |  0.50000000\n",
      "Step  161000: eval                        loss |  2.81419897\n",
      "Step  161000: eval          neg_log_perplexity |  2.81419897\n",
      "Step  161000: eval  weights_per_batch_per_core |  86.00000000\n",
      "Step  161000: Finished evaluation\n",
      "\n",
      "Step  162000: Ran 1000 train steps in 796.03 secs\n",
      "Step  162000: Evaluation\n",
      "Step  162000: train                   accuracy |  0.67326730\n",
      "Step  162000: train                       loss |  1.37369299\n",
      "Step  162000: train         neg_log_perplexity |  1.37369299\n",
      "Step  162000: train weights_per_batch_per_core |  101.00000000\n",
      "Step  162000: eval                    accuracy |  0.57142860\n",
      "Step  162000: eval                        loss |  2.43268967\n",
      "Step  162000: eval          neg_log_perplexity |  2.43268967\n",
      "Step  162000: eval  weights_per_batch_per_core |  91.00000000\n",
      "Step  162000: Finished evaluation\n",
      "\n",
      "Step  163000: Ran 1000 train steps in 795.43 secs\n",
      "Step  163000: Evaluation\n",
      "Step  163000: train                   accuracy |  0.50574714\n",
      "Step  163000: train                       loss |  2.21669793\n",
      "Step  163000: train         neg_log_perplexity |  2.21669793\n",
      "Step  163000: train weights_per_batch_per_core |  87.00000000\n",
      "Step  163000: eval                    accuracy |  0.60824746\n",
      "Step  163000: eval                        loss |  1.68598258\n",
      "Step  163000: eval          neg_log_perplexity |  1.68598258\n",
      "Step  163000: eval  weights_per_batch_per_core |  97.00000000\n",
      "Step  163000: Finished evaluation\n",
      "\n",
      "Step  164000: Ran 1000 train steps in 795.67 secs\n",
      "Step  164000: Evaluation\n",
      "Step  164000: train                   accuracy |  0.56521738\n",
      "Step  164000: train                       loss |  2.05917025\n",
      "Step  164000: train         neg_log_perplexity |  2.05917025\n",
      "Step  164000: train weights_per_batch_per_core |  92.00000000\n",
      "Step  164000: eval                    accuracy |  0.70652175\n",
      "Step  164000: eval                        loss |  1.47698116\n",
      "Step  164000: eval          neg_log_perplexity |  1.47698116\n",
      "Step  164000: eval  weights_per_batch_per_core |  92.00000000\n",
      "Step  164000: Finished evaluation\n",
      "\n",
      "Step  165000: Ran 1000 train steps in 795.62 secs\n",
      "Step  165000: Evaluation\n",
      "Step  165000: train                   accuracy |  0.65000004\n",
      "Step  165000: train                       loss |  2.01189470\n",
      "Step  165000: train         neg_log_perplexity |  2.01189470\n",
      "Step  165000: train weights_per_batch_per_core |  80.00000000\n",
      "Step  165000: eval                    accuracy |  0.53012049\n",
      "Step  165000: eval                        loss |  2.96968794\n",
      "Step  165000: eval          neg_log_perplexity |  2.96968794\n",
      "Step  165000: eval  weights_per_batch_per_core |  83.00000000\n",
      "Step  165000: Finished evaluation\n",
      "\n",
      "Step  166000: Ran 1000 train steps in 795.88 secs\n",
      "Step  166000: Evaluation\n",
      "Step  166000: train                   accuracy |  0.51162791\n",
      "Step  166000: train                       loss |  2.72051597\n",
      "Step  166000: train         neg_log_perplexity |  2.72051597\n",
      "Step  166000: train weights_per_batch_per_core |  86.00000000\n",
      "Step  166000: eval                    accuracy |  0.48780486\n",
      "Step  166000: eval                        loss |  2.61871600\n",
      "Step  166000: eval          neg_log_perplexity |  2.61871600\n",
      "Step  166000: eval  weights_per_batch_per_core |  82.00000000\n",
      "Step  166000: Finished evaluation\n",
      "\n",
      "Step  167000: Ran 1000 train steps in 795.66 secs\n",
      "Step  167000: Evaluation\n",
      "Step  167000: train                   accuracy |  0.62650603\n",
      "Step  167000: train                       loss |  2.05436087\n",
      "Step  167000: train         neg_log_perplexity |  2.05436087\n",
      "Step  167000: train weights_per_batch_per_core |  83.00000000\n",
      "Step  167000: eval                    accuracy |  0.49425286\n",
      "Step  167000: eval                        loss |  2.59633160\n",
      "Step  167000: eval          neg_log_perplexity |  2.59633160\n",
      "Step  167000: eval  weights_per_batch_per_core |  87.00000000\n",
      "Step  167000: Finished evaluation\n",
      "\n",
      "Step  168000: Ran 1000 train steps in 795.95 secs\n",
      "Step  168000: Evaluation\n",
      "Step  168000: train                   accuracy |  0.56818181\n",
      "Step  168000: train                       loss |  2.26797080\n",
      "Step  168000: train         neg_log_perplexity |  2.26797080\n",
      "Step  168000: train weights_per_batch_per_core |  88.00000000\n",
      "Step  168000: eval                    accuracy |  0.57999998\n",
      "Step  168000: eval                        loss |  2.19914174\n",
      "Step  168000: eval          neg_log_perplexity |  2.19914174\n",
      "Step  168000: eval  weights_per_batch_per_core |  100.00000000\n",
      "Step  168000: Finished evaluation\n",
      "\n",
      "Step  169000: Ran 1000 train steps in 795.61 secs\n",
      "Step  169000: Evaluation\n",
      "Step  169000: train                   accuracy |  0.67032969\n",
      "Step  169000: train                       loss |  2.07134914\n",
      "Step  169000: train         neg_log_perplexity |  2.07134914\n",
      "Step  169000: train weights_per_batch_per_core |  91.00000000\n",
      "Step  169000: eval                    accuracy |  0.51282054\n",
      "Step  169000: eval                        loss |  2.71154237\n",
      "Step  169000: eval          neg_log_perplexity |  2.71154237\n",
      "Step  169000: eval  weights_per_batch_per_core |  78.00000000\n",
      "Step  169000: Finished evaluation\n",
      "\n",
      "Step  170000: Ran 1000 train steps in 795.64 secs\n",
      "Step  170000: Evaluation\n",
      "Step  170000: train                   accuracy |  0.48684210\n",
      "Step  170000: train                       loss |  2.96068025\n",
      "Step  170000: train         neg_log_perplexity |  2.96068025\n",
      "Step  170000: train weights_per_batch_per_core |  76.00000000\n",
      "Step  170000: eval                    accuracy |  0.55421686\n",
      "Step  170000: eval                        loss |  2.19569635\n",
      "Step  170000: eval          neg_log_perplexity |  2.19569635\n",
      "Step  170000: eval  weights_per_batch_per_core |  83.00000000\n",
      "Step  170000: Finished evaluation\n",
      "\n",
      "Step  171000: Ran 1000 train steps in 795.27 secs\n",
      "Step  171000: Evaluation\n",
      "Step  171000: train                   accuracy |  0.57142860\n",
      "Step  171000: train                       loss |  2.17882776\n",
      "Step  171000: train         neg_log_perplexity |  2.17882776\n",
      "Step  171000: train weights_per_batch_per_core |  91.00000000\n",
      "Step  171000: eval                    accuracy |  0.72631580\n",
      "Step  171000: eval                        loss |  1.06942475\n",
      "Step  171000: eval          neg_log_perplexity |  1.06942475\n",
      "Step  171000: eval  weights_per_batch_per_core |  95.00000000\n",
      "Step  171000: Finished evaluation\n",
      "\n",
      "Step  172000: Ran 1000 train steps in 796.35 secs\n",
      "Step  172000: Evaluation\n",
      "Step  172000: train                   accuracy |  0.57142860\n",
      "Step  172000: train                       loss |  1.98286593\n",
      "Step  172000: train         neg_log_perplexity |  1.98286593\n",
      "Step  172000: train weights_per_batch_per_core |  84.00000000\n",
      "Step  172000: eval                    accuracy |  0.67391306\n",
      "Step  172000: eval                        loss |  1.63042390\n",
      "Step  172000: eval          neg_log_perplexity |  1.63042390\n",
      "Step  172000: eval  weights_per_batch_per_core |  92.00000000\n",
      "Step  172000: Finished evaluation\n",
      "\n",
      "Step  173000: Ran 1000 train steps in 796.40 secs\n",
      "Step  173000: Evaluation\n",
      "Step  173000: train                   accuracy |  0.61363637\n",
      "Step  173000: train                       loss |  1.92400813\n",
      "Step  173000: train         neg_log_perplexity |  1.92400813\n",
      "Step  173000: train weights_per_batch_per_core |  88.00000000\n",
      "Step  173000: eval                    accuracy |  0.56944448\n",
      "Step  173000: eval                        loss |  1.77133203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  173000: eval          neg_log_perplexity |  1.77133203\n",
      "Step  173000: eval  weights_per_batch_per_core |  72.00000000\n",
      "Step  173000: Finished evaluation\n",
      "\n",
      "Step  174000: Ran 1000 train steps in 796.52 secs\n",
      "Step  174000: Evaluation\n",
      "Step  174000: train                   accuracy |  0.73404253\n",
      "Step  174000: train                       loss |  1.23178387\n",
      "Step  174000: train         neg_log_perplexity |  1.23178387\n",
      "Step  174000: train weights_per_batch_per_core |  94.00000000\n",
      "Step  174000: eval                    accuracy |  0.48837209\n",
      "Step  174000: eval                        loss |  2.49915838\n",
      "Step  174000: eval          neg_log_perplexity |  2.49915838\n",
      "Step  174000: eval  weights_per_batch_per_core |  86.00000000\n",
      "Step  174000: Finished evaluation\n",
      "\n",
      "Step  175000: Ran 1000 train steps in 796.55 secs\n",
      "Step  175000: Evaluation\n",
      "Step  175000: train                   accuracy |  0.65979385\n",
      "Step  175000: train                       loss |  1.73388553\n",
      "Step  175000: train         neg_log_perplexity |  1.73388553\n",
      "Step  175000: train weights_per_batch_per_core |  97.00000000\n",
      "Step  175000: eval                    accuracy |  0.59139782\n",
      "Step  175000: eval                        loss |  1.78689229\n",
      "Step  175000: eval          neg_log_perplexity |  1.78689229\n",
      "Step  175000: eval  weights_per_batch_per_core |  93.00000000\n",
      "Step  175000: Finished evaluation\n",
      "\n",
      "Step  176000: Ran 1000 train steps in 795.87 secs\n",
      "Step  176000: Evaluation\n",
      "Step  176000: train                   accuracy |  0.73493981\n",
      "Step  176000: train                       loss |  1.14218616\n",
      "Step  176000: train         neg_log_perplexity |  1.14218616\n",
      "Step  176000: train weights_per_batch_per_core |  83.00000000\n",
      "Step  176000: eval                    accuracy |  0.64285713\n",
      "Step  176000: eval                        loss |  1.60656965\n",
      "Step  176000: eval          neg_log_perplexity |  1.60656965\n",
      "Step  176000: eval  weights_per_batch_per_core |  84.00000000\n",
      "Step  176000: Finished evaluation\n",
      "\n",
      "Step  177000: Ran 1000 train steps in 796.00 secs\n",
      "Step  177000: Evaluation\n",
      "Step  177000: train                   accuracy |  0.65306121\n",
      "Step  177000: train                       loss |  1.40929782\n",
      "Step  177000: train         neg_log_perplexity |  1.40929782\n",
      "Step  177000: train weights_per_batch_per_core |  98.00000000\n",
      "Step  177000: eval                    accuracy |  0.72619051\n",
      "Step  177000: eval                        loss |  1.29016328\n",
      "Step  177000: eval          neg_log_perplexity |  1.29016328\n",
      "Step  177000: eval  weights_per_batch_per_core |  84.00000000\n",
      "Step  177000: Finished evaluation\n",
      "\n",
      "Step  178000: Ran 1000 train steps in 796.14 secs\n",
      "Step  178000: Evaluation\n",
      "Step  178000: train                   accuracy |  0.59574467\n",
      "Step  178000: train                       loss |  1.78459489\n",
      "Step  178000: train         neg_log_perplexity |  1.78459489\n",
      "Step  178000: train weights_per_batch_per_core |  94.00000000\n",
      "Step  178000: eval                    accuracy |  0.50000000\n",
      "Step  178000: eval                        loss |  2.58975744\n",
      "Step  178000: eval          neg_log_perplexity |  2.58975744\n",
      "Step  178000: eval  weights_per_batch_per_core |  96.00000000\n",
      "Step  178000: Finished evaluation\n",
      "\n",
      "Step  179000: Ran 1000 train steps in 796.23 secs\n",
      "Step  179000: Evaluation\n",
      "Step  179000: train                   accuracy |  0.54347825\n",
      "Step  179000: train                       loss |  2.29735208\n",
      "Step  179000: train         neg_log_perplexity |  2.29735208\n",
      "Step  179000: train weights_per_batch_per_core |  92.00000000\n",
      "Step  179000: eval                    accuracy |  0.51315790\n",
      "Step  179000: eval                        loss |  2.77229071\n",
      "Step  179000: eval          neg_log_perplexity |  2.77229071\n",
      "Step  179000: eval  weights_per_batch_per_core |  76.00000000\n",
      "Step  179000: Finished evaluation\n",
      "\n",
      "Step  180000: Ran 1000 train steps in 796.39 secs\n",
      "Step  180000: Evaluation\n",
      "Step  180000: train                   accuracy |  0.48809525\n",
      "Step  180000: train                       loss |  2.48364472\n",
      "Step  180000: train         neg_log_perplexity |  2.48364472\n",
      "Step  180000: train weights_per_batch_per_core |  84.00000000\n",
      "Step  180000: eval                    accuracy |  0.55555558\n",
      "Step  180000: eval                        loss |  2.53603387\n",
      "Step  180000: eval          neg_log_perplexity |  2.53603387\n",
      "Step  180000: eval  weights_per_batch_per_core |  90.00000000\n",
      "Step  180000: Finished evaluation\n",
      "\n",
      "Step  181000: Ran 1000 train steps in 796.33 secs\n",
      "Step  181000: Evaluation\n",
      "Step  181000: train                   accuracy |  0.64516127\n",
      "Step  181000: train                       loss |  2.70612168\n",
      "Step  181000: train         neg_log_perplexity |  2.70612168\n",
      "Step  181000: train weights_per_batch_per_core |  93.00000000\n",
      "Step  181000: eval                    accuracy |  0.54430383\n",
      "Step  181000: eval                        loss |  2.35064244\n",
      "Step  181000: eval          neg_log_perplexity |  2.35064244\n",
      "Step  181000: eval  weights_per_batch_per_core |  79.00000000\n",
      "Step  181000: Finished evaluation\n",
      "\n",
      "Step  182000: Ran 1000 train steps in 796.84 secs\n",
      "Step  182000: Evaluation\n",
      "Step  182000: train                   accuracy |  0.54945058\n",
      "Step  182000: train                       loss |  2.71797776\n",
      "Step  182000: train         neg_log_perplexity |  2.71797776\n",
      "Step  182000: train weights_per_batch_per_core |  91.00000000\n",
      "Step  182000: eval                    accuracy |  0.55421686\n",
      "Step  182000: eval                        loss |  2.27290821\n",
      "Step  182000: eval          neg_log_perplexity |  2.27290821\n",
      "Step  182000: eval  weights_per_batch_per_core |  83.00000000\n",
      "Step  182000: Finished evaluation\n",
      "\n",
      "Step  183000: Ran 1000 train steps in 796.92 secs\n",
      "Step  183000: Evaluation\n",
      "Step  183000: train                   accuracy |  0.63218391\n",
      "Step  183000: train                       loss |  2.25177360\n",
      "Step  183000: train         neg_log_perplexity |  2.25177360\n",
      "Step  183000: train weights_per_batch_per_core |  87.00000000\n",
      "Step  183000: eval                    accuracy |  0.67777777\n",
      "Step  183000: eval                        loss |  1.61078238\n",
      "Step  183000: eval          neg_log_perplexity |  1.61078238\n",
      "Step  183000: eval  weights_per_batch_per_core |  90.00000000\n",
      "Step  183000: Finished evaluation\n",
      "\n",
      "Step  184000: Ran 1000 train steps in 796.17 secs\n",
      "Step  184000: Evaluation\n",
      "Step  184000: train                   accuracy |  0.55681819\n",
      "Step  184000: train                       loss |  2.02380943\n",
      "Step  184000: train         neg_log_perplexity |  2.02380943\n",
      "Step  184000: train weights_per_batch_per_core |  88.00000000\n",
      "Step  184000: eval                    accuracy |  0.60240966\n",
      "Step  184000: eval                        loss |  1.98313856\n",
      "Step  184000: eval          neg_log_perplexity |  1.98313856\n",
      "Step  184000: eval  weights_per_batch_per_core |  83.00000000\n",
      "Step  184000: Finished evaluation\n",
      "\n",
      "Step  185000: Ran 1000 train steps in 796.53 secs\n",
      "Step  185000: Evaluation\n",
      "Step  185000: train                   accuracy |  0.60465115\n",
      "Step  185000: train                       loss |  1.85800350\n",
      "Step  185000: train         neg_log_perplexity |  1.85800350\n",
      "Step  185000: train weights_per_batch_per_core |  86.00000000\n",
      "Step  185000: eval                    accuracy |  0.60714287\n",
      "Step  185000: eval                        loss |  1.79082370\n",
      "Step  185000: eval          neg_log_perplexity |  1.79082370\n",
      "Step  185000: eval  weights_per_batch_per_core |  84.00000000\n",
      "Step  185000: Finished evaluation\n",
      "\n",
      "Step  186000: Ran 1000 train steps in 796.55 secs\n",
      "Step  186000: Evaluation\n",
      "Step  186000: train                   accuracy |  0.64044946\n",
      "Step  186000: train                       loss |  1.73354089\n",
      "Step  186000: train         neg_log_perplexity |  1.73354089\n",
      "Step  186000: train weights_per_batch_per_core |  89.00000000\n",
      "Step  186000: eval                    accuracy |  0.51724136\n",
      "Step  186000: eval                        loss |  2.86760187\n",
      "Step  186000: eval          neg_log_perplexity |  2.86760187\n",
      "Step  186000: eval  weights_per_batch_per_core |  87.00000000\n",
      "Step  186000: Finished evaluation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(30):\n",
    "    trainer.train_epoch(n_steps=1000, n_eval_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir('trax_transformer/samples_test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "44it [00:00, 438.30it/s]\u001b[A\n",
      "89it [00:00, 439.14it/s]\u001b[A\n",
      "127it [00:00, 419.12it/s]\u001b[A\n",
      "167it [00:00, 411.89it/s]\u001b[A\n",
      "211it [00:00, 417.59it/s]\u001b[A\n",
      "256it [00:00, 425.53it/s]\u001b[A\n",
      "301it [00:00, 431.04it/s]\u001b[A\n",
      "345it [00:00, 432.89it/s]\u001b[A\n",
      "390it [00:00, 435.32it/s]\u001b[A\n",
      "435it [00:01, 438.87it/s]\u001b[A\n",
      "481it [00:01, 442.26it/s]\u001b[A\n",
      "525it [00:01, 428.14it/s]\u001b[A\n",
      "569it [00:01, 429.07it/s]\u001b[A\n",
      "614it [00:01, 433.36it/s]\u001b[A\n",
      "658it [00:01, 417.68it/s]\u001b[A\n",
      "700it [00:01, 407.08it/s]\u001b[A\n",
      "746it [00:01, 419.07it/s]\u001b[A\n",
      "790it [00:01, 422.95it/s]\u001b[A\n",
      "833it [00:01, 424.57it/s]\u001b[A\n",
      "878it [00:02, 430.79it/s]\u001b[A\n",
      "922it [00:02, 432.08it/s]\u001b[A\n",
      "966it [00:02, 433.51it/s]\u001b[A\n",
      "1010it [00:02, 432.73it/s]\u001b[A\n",
      "1054it [00:02, 431.79it/s]\u001b[A\n",
      "1098it [00:02, 432.16it/s]\u001b[A\n",
      "1142it [00:02, 431.50it/s]\u001b[A\n",
      "1186it [00:02, 428.96it/s]\u001b[A\n",
      "1229it [00:02, 419.13it/s]\u001b[A\n",
      "1271it [00:02, 417.14it/s]\u001b[A\n",
      "1315it [00:03, 423.12it/s]\u001b[A\n",
      "1358it [00:03, 420.06it/s]\u001b[A\n",
      "1401it [00:03, 420.53it/s]\u001b[A\n",
      "1445it [00:03, 425.01it/s]\u001b[A\n",
      "1490it [00:03, 431.25it/s]\u001b[A\n",
      "1535it [00:03, 435.20it/s]\u001b[A\n",
      "1579it [00:03, 421.32it/s]\u001b[A\n",
      "1624it [00:03, 426.93it/s]\u001b[A\n",
      "1669it [00:03, 431.93it/s]\u001b[A\n",
      "1713it [00:04, 428.10it/s]\u001b[A\n",
      "1757it [00:04, 430.40it/s]\u001b[A\n",
      "1801it [00:04, 426.89it/s]\u001b[A\n",
      "1844it [00:04, 423.80it/s]\u001b[A\n",
      "1890it [00:04, 432.52it/s]\u001b[A\n",
      "1937it [00:04, 440.53it/s]\u001b[A\n",
      "1983it [00:04, 446.03it/s]\u001b[A\n",
      "2028it [00:04, 446.44it/s]\u001b[A\n",
      "2073it [00:04, 446.19it/s]\u001b[A\n",
      "2118it [00:04, 443.20it/s]\u001b[A\n",
      "2164it [00:05, 445.86it/s]\u001b[A\n",
      "2209it [00:05, 441.19it/s]\u001b[A\n",
      "2255it [00:05, 446.12it/s]\u001b[A\n",
      "2301it [00:05, 449.19it/s]\u001b[A\n",
      "2348it [00:05, 452.52it/s]\u001b[A\n",
      "2395it [00:05, 455.72it/s]\u001b[A\n",
      "2441it [00:05, 430.90it/s]\u001b[A\n",
      "2485it [00:05, 407.77it/s]\u001b[A\n",
      "2527it [00:05, 392.56it/s]\u001b[A\n",
      "2567it [00:06, 377.03it/s]\u001b[A\n",
      "2606it [00:06, 372.80it/s]\u001b[A\n",
      "2644it [00:06, 369.55it/s]\u001b[A\n",
      "2684it [00:06, 377.17it/s]\u001b[A\n",
      "2729it [00:06, 396.02it/s]\u001b[A\n",
      "2775it [00:06, 411.65it/s]\u001b[A\n",
      "2821it [00:06, 422.73it/s]\u001b[A\n",
      "2868it [00:06, 434.08it/s]\u001b[A\n",
      "2915it [00:06, 443.52it/s]\u001b[A\n",
      "2961it [00:06, 447.95it/s]\u001b[A\n",
      "3007it [00:07, 451.47it/s]\u001b[A\n",
      "3053it [00:07, 449.79it/s]\u001b[A\n",
      "3099it [00:07, 447.18it/s]\u001b[A\n",
      "3144it [00:07, 446.92it/s]\u001b[A\n",
      "3191it [00:07, 451.25it/s]\u001b[A\n",
      "3237it [00:07, 452.13it/s]\u001b[A\n",
      "3284it [00:07, 456.27it/s]\u001b[A\n",
      "3331it [00:07, 459.13it/s]\u001b[A\n",
      "3377it [00:07, 456.58it/s]\u001b[A\n",
      "3423it [00:07, 456.84it/s]\u001b[A\n",
      "3470it [00:08, 458.17it/s]\u001b[A\n",
      "3517it [00:08, 458.68it/s]\u001b[A\n",
      "3563it [00:08, 450.22it/s]\u001b[A\n",
      "3609it [00:08, 451.98it/s]\u001b[A\n",
      "3655it [00:08, 452.35it/s]\u001b[A\n",
      "3702it [00:08, 456.10it/s]\u001b[A\n",
      "3749it [00:08, 457.99it/s]\u001b[A\n",
      "3796it [00:08, 458.89it/s]\u001b[A\n",
      "3842it [00:08, 458.74it/s]\u001b[A\n",
      "3889it [00:08, 460.06it/s]\u001b[A\n",
      "3936it [00:09, 460.68it/s]\u001b[A\n",
      "3983it [00:09, 461.10it/s]\u001b[A\n",
      "4030it [00:09, 454.59it/s]\u001b[A\n",
      "4076it [00:09, 450.54it/s]\u001b[A\n",
      "4123it [00:09, 453.73it/s]\u001b[A\n",
      "4169it [00:09, 454.95it/s]\u001b[A\n",
      "4215it [00:09, 451.19it/s]\u001b[A\n",
      "4261it [00:09, 452.94it/s]\u001b[A\n",
      "4307it [00:09, 450.84it/s]\u001b[A\n",
      "4353it [00:09, 450.93it/s]\u001b[A\n",
      "4399it [00:10, 452.62it/s]\u001b[A\n",
      "4446it [00:10, 455.39it/s]\u001b[A\n",
      "4492it [00:10, 451.70it/s]\u001b[A\n",
      "4538it [00:10, 447.50it/s]\u001b[A\n",
      "4584it [00:10, 449.54it/s]\u001b[A\n",
      "4631it [00:10, 453.52it/s]\u001b[A\n",
      "4678it [00:10, 455.57it/s]\u001b[A\n",
      "4726it [00:10, 459.80it/s]\u001b[A\n",
      "4772it [00:10, 458.97it/s]\u001b[A\n",
      "4818it [00:11, 457.74it/s]\u001b[A\n",
      "4864it [00:11, 457.57it/s]\u001b[A\n",
      "4910it [00:11, 457.57it/s]\u001b[A\n",
      "4957it [00:11, 458.13it/s]\u001b[A\n",
      "5003it [00:11, 451.13it/s]\u001b[A\n",
      "5049it [00:11, 453.04it/s]\u001b[A\n",
      "5095it [00:11, 452.67it/s]\u001b[A\n",
      "5141it [00:11, 444.78it/s]\u001b[A\n",
      "5186it [00:11, 443.89it/s]\u001b[A\n",
      "5231it [00:11, 444.14it/s]\u001b[A\n",
      "5278it [00:12, 449.88it/s]\u001b[A\n",
      "5324it [00:12, 450.50it/s]\u001b[A\n",
      "5371it [00:12, 453.47it/s]\u001b[A\n",
      "5417it [00:12, 452.95it/s]\u001b[A\n",
      "5463it [00:12, 447.95it/s]\u001b[A\n",
      "5508it [00:12, 446.05it/s]\u001b[A\n",
      "5553it [00:12, 444.98it/s]\u001b[A\n",
      "5599it [00:12, 449.19it/s]\u001b[A\n",
      "5645it [00:12, 449.44it/s]\u001b[A\n",
      "5690it [00:12, 448.34it/s]\u001b[A\n",
      "5737it [00:13, 452.31it/s]\u001b[A\n",
      "5784it [00:13, 454.92it/s]\u001b[A\n",
      "5830it [00:13, 452.09it/s]\u001b[A\n",
      "5876it [00:13, 451.69it/s]\u001b[A\n",
      "5922it [00:13, 449.08it/s]\u001b[A\n",
      "5967it [00:13, 445.29it/s]\u001b[A\n",
      "6013it [00:13, 447.35it/s]\u001b[A\n",
      "6058it [00:13, 444.58it/s]\u001b[A\n",
      "6105it [00:13, 448.90it/s]\u001b[A\n",
      "6151it [00:13, 450.14it/s]\u001b[A\n",
      "6198it [00:14, 453.28it/s]\u001b[A\n",
      "6244it [00:14, 454.38it/s]\u001b[A\n",
      "6291it [00:14, 458.16it/s]\u001b[A\n",
      "6337it [00:14, 457.03it/s]\u001b[A\n",
      "6383it [00:14, 451.24it/s]\u001b[A\n",
      "6429it [00:14, 443.23it/s]\u001b[A\n",
      "6474it [00:14, 444.12it/s]\u001b[A\n",
      "6520it [00:14, 448.57it/s]\u001b[A\n",
      "6567it [00:14, 452.11it/s]\u001b[A\n",
      "6613it [00:15, 452.67it/s]\u001b[A\n",
      "6659it [00:15, 451.28it/s]\u001b[A\n",
      "6706it [00:15, 456.38it/s]\u001b[A\n",
      "6752it [00:15, 454.96it/s]\u001b[A\n",
      "6798it [00:15, 452.48it/s]\u001b[A\n",
      "6845it [00:15, 455.08it/s]\u001b[A\n",
      "6891it [00:15, 447.29it/s]\u001b[A\n",
      "6937it [00:15, 449.35it/s]\u001b[A\n",
      "6984it [00:15, 453.93it/s]\u001b[A\n",
      "7031it [00:15, 455.27it/s]\u001b[A\n",
      "7077it [00:16, 455.86it/s]\u001b[A\n",
      "7124it [00:16, 457.11it/s]\u001b[A\n",
      "7170it [00:16, 450.68it/s]\u001b[A\n",
      "7216it [00:16, 451.70it/s]\u001b[A\n",
      "7263it [00:16, 455.03it/s]\u001b[A\n",
      "7310it [00:16, 457.13it/s]\u001b[A\n",
      "7356it [00:16, 451.67it/s]\u001b[A\n",
      "7402it [00:16, 446.50it/s]\u001b[A\n",
      "7447it [00:16, 445.95it/s]\u001b[A\n",
      "7492it [00:16, 440.50it/s]\u001b[A\n",
      "7539it [00:17, 448.09it/s]\u001b[A\n",
      "7586it [00:17, 451.95it/s]\u001b[A\n",
      "7633it [00:17, 456.88it/s]\u001b[A\n",
      "7679it [00:17, 454.09it/s]\u001b[A\n",
      "7726it [00:17, 456.92it/s]\u001b[A\n",
      "7772it [00:17, 454.26it/s]\u001b[A\n",
      "7819it [00:17, 456.96it/s]\u001b[A\n",
      "7865it [00:17, 450.14it/s]\u001b[A\n",
      "7911it [00:17, 451.11it/s]\u001b[A\n",
      "7957it [00:17, 452.89it/s]\u001b[A\n",
      "8004it [00:18, 455.60it/s]\u001b[A\n",
      "8051it [00:18, 457.10it/s]\u001b[A\n",
      "8098it [00:18, 458.21it/s]\u001b[A\n",
      "8144it [00:18, 454.93it/s]\u001b[A\n",
      "8191it [00:18, 456.65it/s]\u001b[A\n",
      "8237it [00:18, 455.81it/s]\u001b[A\n",
      "8283it [00:18, 456.94it/s]\u001b[A\n",
      "8329it [00:18, 444.23it/s]\u001b[A\n",
      "8375it [00:18, 446.80it/s]\u001b[A\n",
      "8421it [00:18, 449.55it/s]\u001b[A\n",
      "8467it [00:19, 450.36it/s]\u001b[A\n",
      "8513it [00:19, 451.37it/s]\u001b[A\n",
      "8559it [00:19, 448.65it/s]\u001b[A\n",
      "8604it [00:19, 435.12it/s]\u001b[A\n",
      "8648it [00:19, 436.08it/s]\u001b[A\n",
      "8694it [00:19, 442.30it/s]\u001b[A\n",
      "8739it [00:19, 429.63it/s]\u001b[A\n",
      "8784it [00:19, 433.80it/s]\u001b[A\n",
      "8829it [00:19, 436.67it/s]\u001b[A\n",
      "8875it [00:20, 442.08it/s]\u001b[A\n",
      "8923it [00:20, 451.64it/s]\u001b[A\n",
      "8969it [00:20, 448.48it/s]\u001b[A\n",
      "9014it [00:20, 448.24it/s]\u001b[A\n",
      "9060it [00:20, 449.28it/s]\u001b[A\n",
      "9106it [00:20, 451.85it/s]\u001b[A\n",
      "9153it [00:20, 456.32it/s]\u001b[A\n",
      "9199it [00:20, 456.76it/s]\u001b[A\n",
      "9245it [00:20, 453.50it/s]\u001b[A\n",
      "9291it [00:20, 446.56it/s]\u001b[A\n",
      "9337it [00:21, 449.43it/s]\u001b[A\n",
      "9384it [00:21, 453.77it/s]\u001b[A\n",
      "9430it [00:21, 454.62it/s]\u001b[A\n",
      "9476it [00:21, 453.40it/s]\u001b[A\n",
      "9523it [00:21, 456.65it/s]\u001b[A\n",
      "9569it [00:21, 456.55it/s]\u001b[A\n",
      "9615it [00:21, 456.24it/s]\u001b[A\n",
      "9661it [00:21, 455.89it/s]\u001b[A\n",
      "9707it [00:21, 453.55it/s]\u001b[A\n",
      "9753it [00:21, 442.82it/s]\u001b[A\n",
      "9799it [00:22, 446.62it/s]\u001b[A\n",
      "9845it [00:22, 447.41it/s]\u001b[A\n",
      "9891it [00:22, 449.33it/s]\u001b[A\n",
      "9937it [00:22, 450.95it/s]\u001b[A\n",
      "9983it [00:22, 451.48it/s]\u001b[A\n",
      "10029it [00:22, 452.06it/s]\u001b[A\n",
      "10075it [00:22, 450.96it/s]\u001b[A\n",
      "10121it [00:22, 447.67it/s]\u001b[A\n",
      "10167it [00:22, 449.64it/s]\u001b[A\n",
      "10212it [00:22, 448.56it/s]\u001b[A\n",
      "10257it [00:23, 445.43it/s]\u001b[A\n",
      "10303it [00:23, 446.99it/s]\u001b[A\n",
      "10349it [00:23, 449.37it/s]\u001b[A\n",
      "10395it [00:23, 450.16it/s]\u001b[A\n",
      "10441it [00:23, 446.10it/s]\u001b[A\n",
      "10486it [00:23, 443.92it/s]\u001b[A\n",
      "10531it [00:23, 442.21it/s]\u001b[A\n",
      "10576it [00:23, 439.31it/s]\u001b[A\n",
      "10620it [00:23, 414.32it/s]\u001b[A\n",
      "10662it [00:24, 395.18it/s]\u001b[A\n",
      "10702it [00:24, 382.17it/s]\u001b[A\n",
      "10741it [00:24, 374.52it/s]\u001b[A\n",
      "10779it [00:24, 375.05it/s]\u001b[A\n",
      "10817it [00:24, 367.08it/s]\u001b[A\n",
      "10862it [00:24, 387.06it/s]\u001b[A\n",
      "10908it [00:24, 405.48it/s]\u001b[A\n",
      "10955it [00:24, 420.41it/s]\u001b[A\n",
      "11000it [00:24, 428.84it/s]\u001b[A\n",
      "11044it [00:24, 431.02it/s]\u001b[A\n",
      "11088it [00:25, 431.94it/s]\u001b[A\n",
      "11132it [00:25, 428.83it/s]\u001b[A\n",
      "11177it [00:25, 434.05it/s]\u001b[A\n",
      "11221it [00:25, 433.55it/s]\u001b[A\n",
      "11268it [00:25, 441.35it/s]\u001b[A\n",
      "11314it [00:25, 444.51it/s]\u001b[A\n",
      "11360it [00:25, 447.87it/s]\u001b[A\n",
      "11406it [00:25, 449.52it/s]\u001b[A\n",
      "11451it [00:25, 448.44it/s]\u001b[A\n",
      "11497it [00:26, 450.59it/s]\u001b[A\n",
      "11543it [00:26, 452.21it/s]\u001b[A\n",
      "11589it [00:26, 444.81it/s]\u001b[A\n",
      "11635it [00:26, 446.63it/s]\u001b[A\n",
      "11681it [00:26, 448.54it/s]\u001b[A\n",
      "11727it [00:26, 450.21it/s]\u001b[A\n",
      "11773it [00:26, 448.63it/s]\u001b[A\n",
      "11819it [00:26, 449.23it/s]\u001b[A\n",
      "11864it [00:26, 448.29it/s]\u001b[A\n",
      "11909it [00:26, 443.61it/s]\u001b[A\n",
      "11956it [00:27, 449.35it/s]\u001b[A\n",
      "12002it [00:27, 451.10it/s]\u001b[A\n",
      "12048it [00:27, 433.59it/s]\u001b[A\n",
      "12092it [00:27, 411.81it/s]\u001b[A\n",
      "12134it [00:27, 395.60it/s]\u001b[A\n",
      "12174it [00:27, 384.10it/s]\u001b[A\n",
      "12213it [00:27, 378.87it/s]\u001b[A\n",
      "12252it [00:27, 373.34it/s]\u001b[A\n",
      "12291it [00:27, 375.97it/s]\u001b[A\n",
      "12329it [00:28, 375.88it/s]\u001b[A\n",
      "12374it [00:28, 393.13it/s]\u001b[A\n",
      "12417it [00:28, 402.97it/s]\u001b[A\n",
      "12460it [00:28, 410.16it/s]\u001b[A\n",
      "12505it [00:28, 420.76it/s]\u001b[A\n",
      "12550it [00:28, 427.26it/s]\u001b[A\n",
      "12596it [00:28, 434.03it/s]\u001b[A\n",
      "12641it [00:28, 437.65it/s]\u001b[A\n",
      "12688it [00:28, 445.09it/s]\u001b[A\n",
      "12734it [00:28, 447.01it/s]\u001b[A\n",
      "12780it [00:29, 449.24it/s]\u001b[A\n",
      "12827it [00:29, 452.61it/s]\u001b[A\n",
      "12873it [00:29, 451.33it/s]\u001b[A\n",
      "12919it [00:29, 447.27it/s]\u001b[A\n",
      "12964it [00:29, 446.92it/s]\u001b[A\n",
      "13010it [00:29, 449.45it/s]\u001b[A\n",
      "13057it [00:29, 453.95it/s]\u001b[A\n",
      "13103it [00:29, 452.87it/s]\u001b[A\n",
      "13149it [00:29, 438.52it/s]\u001b[A\n",
      "13193it [00:29, 433.46it/s]\u001b[A\n",
      "13238it [00:30, 436.03it/s]\u001b[A\n",
      "13283it [00:30, 437.53it/s]\u001b[A\n",
      "13327it [00:30, 427.68it/s]\u001b[A\n",
      "13370it [00:30, 416.72it/s]\u001b[A\n",
      "13414it [00:30, 420.91it/s]\u001b[A\n",
      "13459it [00:30, 427.51it/s]\u001b[A\n",
      "13505it [00:30, 435.27it/s]\u001b[A\n",
      "13552it [00:30, 442.66it/s]\u001b[A\n",
      "13598it [00:30, 445.72it/s]\u001b[A\n",
      "13644it [00:30, 449.22it/s]\u001b[A\n",
      "13691it [00:31, 453.91it/s]\u001b[A\n",
      "13738it [00:31, 458.00it/s]\u001b[A\n",
      "13784it [00:31, 457.43it/s]\u001b[A\n",
      "13830it [00:31, 457.10it/s]\u001b[A\n",
      "13876it [00:31, 442.36it/s]\u001b[A\n",
      "13921it [00:31, 418.55it/s]\u001b[A\n",
      "13967it [00:31, 427.74it/s]\u001b[A\n",
      "14013it [00:31, 435.32it/s]\u001b[A\n",
      "14057it [00:31, 436.21it/s]\u001b[A\n",
      "14102it [00:32, 439.25it/s]\u001b[A\n",
      "14147it [00:32, 442.23it/s]\u001b[A\n",
      "14193it [00:32, 444.13it/s]\u001b[A\n",
      "14239it [00:32, 446.78it/s]\u001b[A\n",
      "14285it [00:32, 447.91it/s]\u001b[A\n",
      "14330it [00:32, 439.52it/s]\u001b[A\n",
      "14375it [00:32, 439.50it/s]\u001b[A\n",
      "14419it [00:32, 438.58it/s]\u001b[A\n",
      "14463it [00:32, 419.90it/s]\u001b[A\n",
      "14509it [00:32, 428.87it/s]\u001b[A\n",
      "14555it [00:33, 437.17it/s]\u001b[A\n",
      "14599it [00:33, 435.74it/s]\u001b[A\n",
      "14643it [00:33, 436.74it/s]\u001b[A\n",
      "14688it [00:33, 439.02it/s]\u001b[A\n",
      "14734it [00:33, 443.83it/s]\u001b[A\n",
      "14779it [00:33, 440.56it/s]\u001b[A\n",
      "14824it [00:33, 429.82it/s]\u001b[A\n",
      "14868it [00:33, 430.34it/s]\u001b[A\n",
      "14913it [00:33, 435.63it/s]\u001b[A\n",
      "14960it [00:33, 442.72it/s]\u001b[A\n",
      "15006it [00:34, 447.44it/s]\u001b[A\n",
      "15052it [00:34, 449.79it/s]\u001b[A\n",
      "15098it [00:34, 433.70it/s]\u001b[A\n",
      "15143it [00:34, 436.68it/s]\u001b[A\n",
      "15188it [00:34, 439.04it/s]\u001b[A\n",
      "15233it [00:34, 441.51it/s]\u001b[A\n",
      "15278it [00:34, 438.22it/s]\u001b[A\n",
      "15324it [00:34, 444.13it/s]\u001b[A\n",
      "15369it [00:34, 439.76it/s]\u001b[A\n",
      "15414it [00:35, 438.31it/s]\u001b[A\n",
      "15460it [00:35, 444.30it/s]\u001b[A\n",
      "15507it [00:35, 449.05it/s]\u001b[A\n",
      "15553it [00:35, 449.50it/s]\u001b[A\n",
      "15598it [00:35, 442.70it/s]\u001b[A\n",
      "15643it [00:35, 443.89it/s]\u001b[A\n",
      "15688it [00:35, 445.46it/s]\u001b[A\n",
      "15733it [00:35, 439.57it/s]\u001b[A\n",
      "15778it [00:35, 440.12it/s]\u001b[A\n",
      "15823it [00:35, 441.57it/s]\u001b[A\n",
      "15868it [00:36, 443.89it/s]\u001b[A\n",
      "15914it [00:36, 448.43it/s]\u001b[A\n",
      "15960it [00:36, 450.85it/s]\u001b[A\n",
      "16006it [00:36, 448.82it/s]\u001b[A\n",
      "16051it [00:36, 445.99it/s]\u001b[A\n",
      "16097it [00:36, 448.04it/s]\u001b[A\n",
      "16142it [00:36, 447.16it/s]\u001b[A\n",
      "16187it [00:36, 440.61it/s]\u001b[A\n",
      "16232it [00:36, 442.49it/s]\u001b[A\n",
      "16277it [00:36, 440.52it/s]\u001b[A\n",
      "16322it [00:37, 440.05it/s]\u001b[A\n",
      "16368it [00:37, 444.26it/s]\u001b[A\n",
      "16414it [00:37, 446.27it/s]\u001b[A\n",
      "16459it [00:37, 443.37it/s]\u001b[A\n",
      "16505it [00:37, 447.17it/s]\u001b[A\n",
      "16551it [00:37, 450.76it/s]\u001b[A\n",
      "16597it [00:37, 442.04it/s]\u001b[A\n",
      "16642it [00:37, 428.06it/s]\u001b[A\n",
      "16686it [00:37, 429.90it/s]\u001b[A\n",
      "16730it [00:37, 426.13it/s]\u001b[A\n",
      "16776it [00:38, 434.50it/s]\u001b[A\n",
      "16821it [00:38, 438.86it/s]\u001b[A\n",
      "16867it [00:38, 444.25it/s]\u001b[A\n",
      "16912it [00:38, 444.04it/s]\u001b[A\n",
      "16957it [00:38, 440.69it/s]\u001b[A\n",
      "17003it [00:38, 445.13it/s]\u001b[A\n",
      "17048it [00:38, 443.89it/s]\u001b[A\n",
      "17093it [00:38, 443.04it/s]\u001b[A\n",
      "17138it [00:38, 433.88it/s]\u001b[A\n",
      "17182it [00:39, 432.40it/s]\u001b[A\n",
      "17227it [00:39, 437.07it/s]\u001b[A\n",
      "17273it [00:39, 443.36it/s]\u001b[A\n",
      "17318it [00:39, 444.83it/s]\u001b[A\n",
      "17363it [00:39, 444.35it/s]\u001b[A\n",
      "17408it [00:39, 444.42it/s]\u001b[A\n",
      "17453it [00:39, 443.37it/s]\u001b[A\n",
      "17498it [00:39, 444.49it/s]\u001b[A\n",
      "17543it [00:39, 445.22it/s]\u001b[A\n",
      "17588it [00:39, 437.11it/s]\u001b[A\n",
      "17632it [00:40, 437.33it/s]\u001b[A\n",
      "17677it [00:40, 440.65it/s]\u001b[A\n",
      "17723it [00:40, 444.85it/s]\u001b[A\n",
      "17769it [00:40, 447.21it/s]\u001b[A\n",
      "17815it [00:40, 448.05it/s]\u001b[A\n",
      "17860it [00:40, 445.34it/s]\u001b[A\n",
      "17905it [00:40, 443.57it/s]\u001b[A\n",
      "17950it [00:40, 443.63it/s]\u001b[A\n",
      "17995it [00:40, 443.11it/s]\u001b[A\n",
      "18040it [00:40, 437.37it/s]\u001b[A\n",
      "18084it [00:41, 435.19it/s]\u001b[A\n",
      "18130it [00:41, 439.64it/s]\u001b[A\n",
      "18176it [00:41, 443.04it/s]\u001b[A\n",
      "18221it [00:41, 440.07it/s]\u001b[A\n",
      "18266it [00:41, 441.93it/s]\u001b[A\n",
      "18311it [00:41, 440.36it/s]\u001b[A\n",
      "18356it [00:41, 443.00it/s]\u001b[A\n",
      "18402it [00:41, 447.03it/s]\u001b[A\n",
      "18447it [00:41, 446.07it/s]\u001b[A\n",
      "18492it [00:41, 423.76it/s]\u001b[A\n",
      "18535it [00:42, 423.68it/s]\u001b[A\n",
      "18581it [00:42, 431.76it/s]\u001b[A\n",
      "18626it [00:42, 436.93it/s]\u001b[A\n",
      "18670it [00:42, 436.18it/s]\u001b[A\n",
      "18716it [00:42, 441.14it/s]\u001b[A\n",
      "18761it [00:42, 443.71it/s]\u001b[A\n",
      "18807it [00:42, 446.12it/s]\u001b[A\n",
      "18853it [00:42, 446.67it/s]\u001b[A\n",
      "18898it [00:42, 447.61it/s]\u001b[A\n",
      "18943it [00:42, 438.29it/s]\u001b[A\n",
      "18987it [00:43, 428.09it/s]\u001b[A\n",
      "19031it [00:43, 431.02it/s]\u001b[A\n",
      "19075it [00:43, 427.51it/s]\u001b[A\n",
      "19120it [00:43, 431.34it/s]\u001b[A\n",
      "19164it [00:43, 431.68it/s]\u001b[A\n",
      "19210it [00:43, 437.41it/s]\u001b[A\n",
      "19254it [00:43, 435.94it/s]\u001b[A\n",
      "19298it [00:43, 437.14it/s]\u001b[A\n",
      "19343it [00:43, 438.68it/s]\u001b[A\n",
      "19387it [00:44, 437.83it/s]\u001b[A\n",
      "19431it [00:44, 436.62it/s]\u001b[A\n",
      "19476it [00:44, 438.36it/s]\u001b[A\n",
      "19521it [00:44, 438.99it/s]\u001b[A\n",
      "19566it [00:44, 441.33it/s]\u001b[A\n",
      "19611it [00:44, 429.64it/s]\u001b[A\n",
      "19655it [00:44, 429.94it/s]\u001b[A\n",
      "19699it [00:44, 429.99it/s]\u001b[A\n",
      "19743it [00:44, 428.94it/s]\u001b[A\n",
      "19786it [00:44, 429.05it/s]\u001b[A\n",
      "19830it [00:45, 432.24it/s]\u001b[A\n",
      "19874it [00:45, 433.94it/s]\u001b[A\n",
      "19918it [00:45, 432.21it/s]\u001b[A\n",
      "19963it [00:45, 436.57it/s]\u001b[A\n",
      "20000it [00:45, 440.18it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "# записываем каждый сэмпл в отдельный файлик + обрезка и паддинг\n",
    "TEXT_MAX_LENGTH = 3000\n",
    "LABEL_MAX_LENGTH = 90\n",
    "cancels = 0\n",
    "\n",
    "with open('lstm_v2/clear_texts_tokenized_test') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open('lstm_v2/clear_labels_tokenized_test') as f:\n",
    "    label_lines = f.readlines()\n",
    "\n",
    "name = 1\n",
    "for text, label in tqdm(zip(lines, label_lines)):\n",
    "    try:\n",
    "        text = text.split(' ')\n",
    "        t_len = len(text)\n",
    "        if t_len > TEXT_MAX_LENGTH:\n",
    "            text = text[:TEXT_MAX_LENGTH]\n",
    "        elif t_len < TEXT_MAX_LENGTH:\n",
    "            text.extend([\"PAD\"] * (TEXT_MAX_LENGTH - t_len))\n",
    "\n",
    "        label = label.split(' ')\n",
    "        l_len = len(label)\n",
    "        mask = [1] * l_len\n",
    "        if l_len > LABEL_MAX_LENGTH:\n",
    "            label = label[:LABEL_MAX_LENGTH]\n",
    "            mask = mask[:LABEL_MAX_LENGT]\n",
    "        elif l_len < LABEL_MAX_LENGTH:\n",
    "            label.extend([\"PAD\"] * (LABEL_MAX_LENGTH - l_len))\n",
    "            mask.extend([0] * (LABEL_MAX_LENGTH - l_len))\n",
    "\n",
    "        tokenized_text = []\n",
    "        for token in text:\n",
    "            tokenized_text.append(vocab.word2index[token])\n",
    "\n",
    "        tokenized_label = []\n",
    "        for token in label:\n",
    "            tokenized_label.append(vocab.word2index[token])\n",
    "\n",
    "        output = {'text': tokenized_text, 'mask': mask, 'label': tokenized_label}\n",
    "\n",
    "        with open('trax_transformer/samples_test/{}'.format(name), 'w') as text_labels_tokens_file:\n",
    "            json.dump(output, text_labels_tokens_file)\n",
    "\n",
    "        name +=1\n",
    "    \n",
    "    except:\n",
    "        cancels +=1\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция для получения предсказаний модели\n",
    "def index_to_token(x):\n",
    "    return vocab.index2word[x]\n",
    "\n",
    "def validate_inputs(vocab, batch_size, files_dir, model, predict_dir, true_dir):\n",
    "    files_names = os.listdir(files_dir)\n",
    "    files_names = np.arange(1, len(files_names) + 1)\n",
    "    back_to_tokens = np.vectorize(index_to_token)\n",
    "    \n",
    "    batch_indexes = files_names[::batch_size]\n",
    "    for i in tqdm(range(len(batch_indexes) - 1)):\n",
    "        batch_range = range(batch_indexes[i], batch_indexes[i+1])\n",
    "        texts = []\n",
    "        labels = []\n",
    "        \n",
    "        for index in batch_range:\n",
    "            with open(os.path.join(files_dir, str(index))) as file:\n",
    "                sample = json.loads(file.readline())\n",
    "                texts.append(sample['text'])\n",
    "                labels.append(sample['label'])\n",
    "     \n",
    "        texts = np.stack(texts)\n",
    "        labels = np.stack(labels)\n",
    "\n",
    "        predict = model((texts, labels))[0].argmax(axis=-1)\n",
    "        mask = predict[:, 1:] - predict[:, :-1]\n",
    "\n",
    "        for j, sentence in enumerate(predict):\n",
    "            stop_index = np.argmax(mask[j] == 0) + 1\n",
    "            to_write = ' '.join(back_to_tokens(sentence[:stop_index]))\n",
    "            to_write = re.sub(r'\\n|\\r', '', to_write) + '\\n'\n",
    "            \n",
    "            stop_index_labels = np.argmax(labels[j] == 0)\n",
    "            true = ' '.join(back_to_tokens(labels[j][:stop_index_labels]))\n",
    "\n",
    "            with open(predict_dir, 'a') as file_prediction:\n",
    "                file_prediction.write(to_write)\n",
    "            \n",
    "            with open(true_dir, 'a') as file_true:\n",
    "                file_true.write(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 тестирование модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trax.models.Transformer(mode='eval')\n",
    "model.init_from_file('trax_transformer/model_1/model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2492/2492 [26:26<00:00,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "validate_inputs(vocab, 8, 'trax_transformer/samples_test', model,\n",
    "                'trax_transformer/model_1/prediction',\n",
    "                'trax_transformer/model_1/true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPE = pyonmttok.Tokenizer(\"conservative\", bpe_model_path=\"model-50k_with_joiner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.remove('trax_transformer/model_1/true_detokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPE.detokenize_file(input_path='trax_transformer/model_1/prediction',\n",
    "                    output_path='trax_transformer/model_1/prediction_detokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPE.detokenize_file(input_path='trax_transformer/model_1/true',\n",
    "                    output_path='trax_transformer/model_1/true_detokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Очистка от пустых предсказанных заголовков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trax_transformer/model_1/true_detokenized') as f:\n",
    "    true = f.readlines()\n",
    "with open('trax_transformer/model_1/prediction_detokenized') as f:\n",
    "    prediction = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancels = []\n",
    "for i, sentence in enumerate(prediction):\n",
    "    if len(sentence) < 5:\n",
    "        cancels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cancels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.remove('trax_transformer/model_1/true_for_metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, line in enumerate(prediction):\n",
    "    if i not in cancels:\n",
    "        with open('trax_transformer/model_1/prediction_for_metrics', 'a') as f1:\n",
    "            f1.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, line in enumerate(true):\n",
    "    if i not in cancels:\n",
    "        with open('trax_transformer/model_1/true_for_metrics', 'a') as f1:\n",
    "            f1.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import FilesRouge\n",
    "files_rouge_detoc = FilesRouge('trax_transformer/model_1/prediction_for_metrics',\n",
    "                              'trax_transformer/model_1/true_for_metrics')\n",
    "scores_detoc = files_rouge_detoc.get_scores(avg=True, ignore_empty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.4290884118256301,\n",
       "  'p': 0.4632726660197333,\n",
       "  'r': 0.4321201598256151},\n",
       " 'rouge-2': {'f': 0.2135105618761511,\n",
       "  'p': 0.20826609730102505,\n",
       "  'r': 0.2292366333617198},\n",
       " 'rouge-l': {'f': 0.4029890042208497,\n",
       "  'p': 0.45219391395157066,\n",
       "  'r': 0.42164615234501285}}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_detoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trax_transformer/model_1/prediction_for_metrics') as f:\n",
    "    prediction_samples = f.readlines()\n",
    "    \n",
    "with open('trax_transformer/model_1/true_for_metrics') as f:\n",
    "    true_samples = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'два здания обрушились в италии из-за взрыва газа, 10 человек ранены\\n'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_samples[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'два здания обрушились в горах, взрыва газа, есть человек пострадали ранены\\n'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_samples[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Reformer (reversible only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "#SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 1  # End-of-sentence token\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"PAD\": PAD_token, \"EOS\": EOS_token}\n",
    "        self.index2word = {PAD_token: \"PAD\", EOS_token:\"EOS\"}\n",
    "        self.num_words = 2  # Count PAD, EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "\n",
    "vocab = Voc('News')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 963781/963781 [01:41<00:00, 9507.58it/s] \n",
      "100%|██████████| 963781/963781 [00:03<00:00, 286434.17it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('lstm_v2/clear_texts_tokenized_train') as train_texts_file:\n",
    "    lines = train_texts_file.readlines()\n",
    "\n",
    "for i in tqdm(range(len(lines))):\n",
    "    vocab.addSentence(lines[i])\n",
    "\n",
    "with open('lstm_v2/clear_labels_tokenized_train') as train_labels_file:\n",
    "    lines = train_labels_file.readlines()\n",
    "\n",
    "for i in tqdm(range(len(lines))):\n",
    "    vocab.addSentence(lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir('Reformer_reversible/samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "963781it [38:26, 417.90it/s]\n"
     ]
    }
   ],
   "source": [
    "TEXT_MAX_LENGTH = 3000\n",
    "LABEL_MAX_LENGTH = 90\n",
    "\n",
    "with open('lstm_v2/clear_texts_tokenized_train') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open('lstm_v2/clear_labels_tokenized_train') as f:\n",
    "    label_lines = f.readlines()\n",
    "\n",
    "name = 1\n",
    "for text, label in tqdm(zip(lines, label_lines)):\n",
    "    text = text.split(' ')\n",
    "    t_len = len(text)\n",
    "    if t_len > TEXT_MAX_LENGTH:\n",
    "        text = text[:TEXT_MAX_LENGTH]\n",
    "    elif t_len < TEXT_MAX_LENGTH:\n",
    "        text.extend([\"PAD\"] * (TEXT_MAX_LENGTH - t_len))\n",
    "    \n",
    "    label = label.split(' ')\n",
    "    l_len = len(label)\n",
    "    mask = [1] * l_len\n",
    "    if l_len > LABEL_MAX_LENGTH:\n",
    "        label = label[:LABEL_MAX_LENGTH]\n",
    "        label[-1] = \"EOS\"\n",
    "        mask = mask[:LABEL_MAX_LENGTH]\n",
    "    elif l_len < LABEL_MAX_LENGTH:\n",
    "        label.append(\"EOS\")\n",
    "        mask.append(1)\n",
    "        label.extend([\"PAD\"] * (LABEL_MAX_LENGTH - l_len - 1))\n",
    "        mask.extend([0] * (LABEL_MAX_LENGTH - l_len - 1))\n",
    "    else:\n",
    "        label[-1] = \"EOS\"\n",
    "    \n",
    "    tokenized_text = []\n",
    "    for token in text:\n",
    "        tokenized_text.append(vocab.word2index[token])\n",
    "\n",
    "    tokenized_label = []\n",
    "    for token in label:\n",
    "        tokenized_label.append(vocab.word2index[token])\n",
    "\n",
    "    output = {'text': tokenized_text, 'mask': mask, 'label': tokenized_label}\n",
    "    \n",
    "    with open('Reformer_reversible/samples/{}'.format(name), 'w') as text_labels_tokens_file:\n",
    "        json.dump(output, text_labels_tokens_file)\n",
    "    \n",
    "    name +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_inputs(vocab, batch_size, files_dir):\n",
    "    files_names = os.listdir(files_dir)\n",
    "    \n",
    "    while True:\n",
    "        batch_indexes = np.random.choice(range(1, len(files_names)+1), size=batch_size, replace=False)\n",
    "        \n",
    "        texts = []\n",
    "        labels = []\n",
    "        mask = []\n",
    "        for index in batch_indexes:\n",
    "            with open(os.path.join(files_dir, str(index))) as file:\n",
    "                sample = json.loads(file.readline())\n",
    "                texts.append(sample['text'])\n",
    "                labels.append(sample['label'])\n",
    "                mask.append(sample['mask'])\n",
    "\n",
    "        texts = np.stack(texts)\n",
    "        labels = np.stack(labels)\n",
    "        mask = np.stack(mask)\n",
    "\n",
    "        yield texts, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_inputs = trax.supervised.Inputs(lambda _: news_inputs(vocab, 8, 'Reformer_reversible/samples'))\n",
    "data_stream = copy_inputs.train_stream(1)\n",
    "inputs, target, mask = next(data_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 3000)\n",
      "(8, 90)\n",
      "(8, 90)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(target.shape)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74008"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config(\"\"\"\n",
    "import trax.models\n",
    "import trax.optimizers\n",
    "import trax.supervised.inputs\n",
    "import trax.supervised.trainer_lib\n",
    "# Parameters for _is_jit_init:\n",
    "# ==============================================================================\n",
    "_is_jit_init.value = True\n",
    "# Parameters for _jit_predict_fn:\n",
    "# ==============================================================================\n",
    "_jit_predict_fn.jit = True\n",
    "# Parameters for _jit_update_fn\n",
    "# ==============================================================================\n",
    "_jit_update_fn.jit = True\n",
    "# Parameters for Adafactor:\n",
    "# ==============================================================================\n",
    "Adafactor.beta1 = 0.0\n",
    "Adafactor.decay_rate = 0.8\n",
    "Adafactor.clipping_threshold = 1.0\n",
    "Adafactor.epsilon1 = 1e-30\n",
    "Adafactor.epsilon2 = 0.001\n",
    "Adafactor.factored = True\n",
    "Adafactor.multiply_by_parameter_scale = True\n",
    "# Parameters for backend:\n",
    "# ==============================================================================\n",
    "backend.name = 'jax'\n",
    "# Parameters for EncDecAttention:\n",
    "# ==============================================================================\n",
    "EncDecAttention.masked = True\n",
    "EncDecAttention.n_parallel_heads = None\n",
    "EncDecAttention.use_python_loop = False\n",
    "EncDecAttention.use_reference_code = False\n",
    "\n",
    "# Parameters for MultifactorSchedule:\n",
    "# ==============================================================================\n",
    "MultifactorSchedule.constant = 1.0\n",
    "MultifactorSchedule.factors = 'constant * linear_warmup * rsqrt_decay'\n",
    "MultifactorSchedule.warmup_steps = 3000\n",
    "# Parameters for Reformer:\n",
    "# ==============================================================================\n",
    "Reformer.d_ff = 1024\n",
    "Reformer.d_model = 512\n",
    "Reformer.dropout = 0.1\n",
    "Reformer.ff_activation = @trax.layers.Relu\n",
    "Reformer.ff_dropout = 0.1\n",
    "Reformer.input_vocab_size = 74008\n",
    "Reformer.max_len = 3000\n",
    "Reformer.n_decoder_layers = 6\n",
    "Reformer.n_encoder_layers = 6\n",
    "Reformer.n_heads = 4\n",
    "#Reformer.output_vocab_size\n",
    "\n",
    "# Parameters for SelfAttention\n",
    "# ==============================================================================\n",
    "# SelfAttention.causal = False\n",
    "# SelfAttention.chunk_len = None\n",
    "# SelfAttention.masked = False\n",
    "# SelfAttention.n_chunks_after = 0\n",
    "# SelfAttention.n_chunks_before = 0\n",
    "# SelfAttention.n_parallel_heads = None\n",
    "# SelfAttention.predict_drop_len = 64\n",
    "# SelfAttention.predict_mem_len = 192\n",
    "# SelfAttention.share_qk = False\n",
    "# SelfAttention.use_python_loop = False\n",
    "# SelfAttention.use_reference_code = False\n",
    "\n",
    "# Parameters for train:\n",
    "# ==============================================================================\n",
    "#train.checkpoint_highest = None\n",
    "#train.checkpoint_lowest = None\n",
    "#train.checkpoints_at = None\n",
    "#train.eval_frequency = 1000\n",
    "#train.eval_steps = 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from Reformer_reversible/model_2/model.pkl at step 274738\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'Reformer_reversible/model_2'\n",
    "\n",
    "trainer = trax.supervised.Trainer(\n",
    "    model=trax.models.Reformer,\n",
    "    loss_fn=trax.layers.CrossEntropyLoss,\n",
    "    optimizer=trax.optimizers.Adafactor,\n",
    "    inputs=copy_inputs,\n",
    "    lr_schedule=trax.lr.MultifactorSchedule,\n",
    "    output_dir=output_dir,\n",
    "    id_to_mask=0,\n",
    "    has_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step  111000: Ran 1000 train steps in 3105.33 secs\n",
      "Step  111000: Evaluation\n",
      "Step  111000: train                   accuracy |  0.40217391\n",
      "Step  111000: train                       loss |  3.81247330\n",
      "Step  111000: train         neg_log_perplexity |  3.81247330\n",
      "Step  111000: train          sequence_accuracy |  0.00000000\n",
      "Step  111000: train weights_per_batch_per_core |  92.00000000\n",
      "Step  111000: eval                    accuracy |  0.40000001\n",
      "Step  111000: eval                        loss |  3.64806557\n",
      "Step  111000: eval          neg_log_perplexity |  3.64806557\n",
      "Step  111000: eval           sequence_accuracy |  0.00000000\n",
      "Step  111000: eval  weights_per_batch_per_core |  90.00000000\n",
      "Step  111000: Finished evaluation\n",
      "\n",
      "Step  112000: Ran 1000 train steps in 2883.80 secs\n",
      "Step  112000: Evaluation\n",
      "Step  112000: train                   accuracy |  0.46296296\n",
      "Step  112000: train                       loss |  3.51094055\n",
      "Step  112000: train         neg_log_perplexity |  3.51094055\n",
      "Step  112000: train          sequence_accuracy |  0.00000000\n",
      "Step  112000: train weights_per_batch_per_core |  108.00000000\n",
      "Step  112000: eval                    accuracy |  0.49999997\n",
      "Step  112000: eval                        loss |  2.49042082\n",
      "Step  112000: eval          neg_log_perplexity |  2.49042082\n",
      "Step  112000: eval           sequence_accuracy |  0.00000000\n",
      "Step  112000: eval  weights_per_batch_per_core |  82.00000000\n",
      "Step  112000: Finished evaluation\n",
      "\n",
      "Step  113000: Ran 1000 train steps in 2885.57 secs\n",
      "Step  113000: Evaluation\n",
      "Step  113000: train                   accuracy |  0.40860215\n",
      "Step  113000: train                       loss |  3.23141122\n",
      "Step  113000: train         neg_log_perplexity |  3.23141122\n",
      "Step  113000: train          sequence_accuracy |  0.00000000\n",
      "Step  113000: train weights_per_batch_per_core |  93.00000000\n",
      "Step  113000: eval                    accuracy |  0.38202247\n",
      "Step  113000: eval                        loss |  3.21137071\n",
      "Step  113000: eval          neg_log_perplexity |  3.21137071\n",
      "Step  113000: eval           sequence_accuracy |  0.00000000\n",
      "Step  113000: eval  weights_per_batch_per_core |  89.00000000\n",
      "Step  113000: Finished evaluation\n",
      "\n",
      "Step  114000: Ran 1000 train steps in 2866.05 secs\n",
      "Step  114000: Evaluation\n",
      "Step  114000: train                   accuracy |  0.47826087\n",
      "Step  114000: train                       loss |  2.52207899\n",
      "Step  114000: train         neg_log_perplexity |  2.52207899\n",
      "Step  114000: train          sequence_accuracy |  0.00000000\n",
      "Step  114000: train weights_per_batch_per_core |  92.00000000\n",
      "Step  114000: eval                    accuracy |  0.50505048\n",
      "Step  114000: eval                        loss |  2.75413775\n",
      "Step  114000: eval          neg_log_perplexity |  2.75413775\n",
      "Step  114000: eval           sequence_accuracy |  0.00000000\n",
      "Step  114000: eval  weights_per_batch_per_core |  99.00000000\n",
      "Step  114000: Finished evaluation\n",
      "\n",
      "Step  115000: Ran 1000 train steps in 2824.06 secs\n",
      "Step  115000: Evaluation\n",
      "Step  115000: train                   accuracy |  0.44554454\n",
      "Step  115000: train                       loss |  3.31455517\n",
      "Step  115000: train         neg_log_perplexity |  3.31455517\n",
      "Step  115000: train          sequence_accuracy |  0.00000000\n",
      "Step  115000: train weights_per_batch_per_core |  101.00000000\n",
      "Step  115000: eval                    accuracy |  0.49038464\n",
      "Step  115000: eval                        loss |  2.68826270\n",
      "Step  115000: eval          neg_log_perplexity |  2.68826270\n",
      "Step  115000: eval           sequence_accuracy |  0.00000000\n",
      "Step  115000: eval  weights_per_batch_per_core |  104.00000000\n",
      "Step  115000: Finished evaluation\n",
      "\n",
      "Step  116000: Ran 1000 train steps in 2816.83 secs\n",
      "Step  116000: Evaluation\n",
      "Step  116000: train                   accuracy |  0.44999999\n",
      "Step  116000: train                       loss |  3.11629629\n",
      "Step  116000: train         neg_log_perplexity |  3.11629629\n",
      "Step  116000: train          sequence_accuracy |  0.00000000\n",
      "Step  116000: train weights_per_batch_per_core |  100.00000000\n",
      "Step  116000: eval                    accuracy |  0.54117650\n",
      "Step  116000: eval                        loss |  2.94160175\n",
      "Step  116000: eval          neg_log_perplexity |  2.94160175\n",
      "Step  116000: eval           sequence_accuracy |  0.00000000\n",
      "Step  116000: eval  weights_per_batch_per_core |  85.00000000\n",
      "Step  116000: Finished evaluation\n",
      "\n",
      "Step  117000: Ran 1000 train steps in 2848.87 secs\n",
      "Step  117000: Evaluation\n",
      "Step  117000: train                   accuracy |  0.44791669\n",
      "Step  117000: train                       loss |  3.08398247\n",
      "Step  117000: train         neg_log_perplexity |  3.08398247\n",
      "Step  117000: train          sequence_accuracy |  0.00000000\n",
      "Step  117000: train weights_per_batch_per_core |  96.00000000\n",
      "Step  117000: eval                    accuracy |  0.49019611\n",
      "Step  117000: eval                        loss |  2.61189437\n",
      "Step  117000: eval          neg_log_perplexity |  2.61189437\n",
      "Step  117000: eval           sequence_accuracy |  0.00000000\n",
      "Step  117000: eval  weights_per_batch_per_core |  102.00000000\n",
      "Step  117000: Finished evaluation\n",
      "\n",
      "Step  118000: Ran 1000 train steps in 2821.54 secs\n",
      "Step  118000: Evaluation\n",
      "Step  118000: train                   accuracy |  0.53535354\n",
      "Step  118000: train                       loss |  2.50759482\n",
      "Step  118000: train         neg_log_perplexity |  2.50759482\n",
      "Step  118000: train          sequence_accuracy |  0.00000000\n",
      "Step  118000: train weights_per_batch_per_core |  99.00000000\n",
      "Step  118000: eval                    accuracy |  0.51401865\n",
      "Step  118000: eval                        loss |  2.55365729\n",
      "Step  118000: eval          neg_log_perplexity |  2.55365729\n",
      "Step  118000: eval           sequence_accuracy |  0.00000000\n",
      "Step  118000: eval  weights_per_batch_per_core |  107.00000000\n",
      "Step  118000: Finished evaluation\n",
      "\n",
      "Step  119000: Ran 1000 train steps in 2817.34 secs\n",
      "Step  119000: Evaluation\n",
      "Step  119000: train                   accuracy |  0.47311828\n",
      "Step  119000: train                       loss |  2.97789407\n",
      "Step  119000: train         neg_log_perplexity |  2.97789407\n",
      "Step  119000: train          sequence_accuracy |  0.00000000\n",
      "Step  119000: train weights_per_batch_per_core |  93.00000000\n",
      "Step  119000: eval                    accuracy |  0.35051548\n",
      "Step  119000: eval                        loss |  3.59041405\n",
      "Step  119000: eval          neg_log_perplexity |  3.59041405\n",
      "Step  119000: eval           sequence_accuracy |  0.00000000\n",
      "Step  119000: eval  weights_per_batch_per_core |  97.00000000\n",
      "Step  119000: Finished evaluation\n",
      "\n",
      "Step  120000: Ran 1000 train steps in 2821.57 secs\n",
      "Step  120000: Evaluation\n",
      "Step  120000: train                   accuracy |  0.44680849\n",
      "Step  120000: train                       loss |  3.12868905\n",
      "Step  120000: train         neg_log_perplexity |  3.12868905\n",
      "Step  120000: train          sequence_accuracy |  0.00000000\n",
      "Step  120000: train weights_per_batch_per_core |  94.00000000\n",
      "Step  120000: eval                    accuracy |  0.45714289\n",
      "Step  120000: eval                        loss |  2.74544144\n",
      "Step  120000: eval          neg_log_perplexity |  2.74544144\n",
      "Step  120000: eval           sequence_accuracy |  0.00000000\n",
      "Step  120000: eval  weights_per_batch_per_core |  105.00000000\n",
      "Step  120000: Finished evaluation\n",
      "\n",
      "Step  121000: Ran 1000 train steps in 2876.35 secs\n",
      "Step  121000: Evaluation\n",
      "Step  121000: train                   accuracy |  0.47747749\n",
      "Step  121000: train                       loss |  3.02447987\n",
      "Step  121000: train         neg_log_perplexity |  3.02447987\n",
      "Step  121000: train          sequence_accuracy |  0.00000000\n",
      "Step  121000: train weights_per_batch_per_core |  111.00000000\n",
      "Step  121000: eval                    accuracy |  0.41000000\n",
      "Step  121000: eval                        loss |  3.26112485\n",
      "Step  121000: eval          neg_log_perplexity |  3.26112485\n",
      "Step  121000: eval           sequence_accuracy |  0.00000000\n",
      "Step  121000: eval  weights_per_batch_per_core |  100.00000000\n",
      "Step  121000: Finished evaluation\n",
      "\n",
      "Step  122000: Ran 1000 train steps in 2884.71 secs\n",
      "Step  122000: Evaluation\n",
      "Step  122000: train                   accuracy |  0.41964287\n",
      "Step  122000: train                       loss |  3.05906844\n",
      "Step  122000: train         neg_log_perplexity |  3.05906844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  122000: train          sequence_accuracy |  0.00000000\n",
      "Step  122000: train weights_per_batch_per_core |  112.00000000\n",
      "Step  122000: eval                    accuracy |  0.45833334\n",
      "Step  122000: eval                        loss |  2.80004311\n",
      "Step  122000: eval          neg_log_perplexity |  2.80004311\n",
      "Step  122000: eval           sequence_accuracy |  0.00000000\n",
      "Step  122000: eval  weights_per_batch_per_core |  96.00000000\n",
      "Step  122000: Finished evaluation\n",
      "\n",
      "Step  123000: Ran 1000 train steps in 2885.01 secs\n",
      "Step  123000: Evaluation\n",
      "Step  123000: train                   accuracy |  0.53846157\n",
      "Step  123000: train                       loss |  2.62778187\n",
      "Step  123000: train         neg_log_perplexity |  2.62778187\n",
      "Step  123000: train          sequence_accuracy |  0.00000000\n",
      "Step  123000: train weights_per_batch_per_core |  104.00000000\n",
      "Step  123000: eval                    accuracy |  0.49367091\n",
      "Step  123000: eval                        loss |  3.09498739\n",
      "Step  123000: eval          neg_log_perplexity |  3.09498739\n",
      "Step  123000: eval           sequence_accuracy |  0.00000000\n",
      "Step  123000: eval  weights_per_batch_per_core |  79.00000000\n",
      "Step  123000: Finished evaluation\n",
      "\n",
      "Step  124000: Ran 1000 train steps in 2887.57 secs\n",
      "Step  124000: Evaluation\n",
      "Step  124000: train                   accuracy |  0.36538464\n",
      "Step  124000: train                       loss |  3.45366788\n",
      "Step  124000: train         neg_log_perplexity |  3.45366788\n",
      "Step  124000: train          sequence_accuracy |  0.00000000\n",
      "Step  124000: train weights_per_batch_per_core |  104.00000000\n",
      "Step  124000: eval                    accuracy |  0.46078432\n",
      "Step  124000: eval                        loss |  2.96603155\n",
      "Step  124000: eval          neg_log_perplexity |  2.96603155\n",
      "Step  124000: eval           sequence_accuracy |  0.00000000\n",
      "Step  124000: eval  weights_per_batch_per_core |  102.00000000\n",
      "Step  124000: Finished evaluation\n",
      "\n",
      "Step  125000: Ran 1000 train steps in 2884.48 secs\n",
      "Step  125000: Evaluation\n",
      "Step  125000: train                   accuracy |  0.42201832\n",
      "Step  125000: train                       loss |  3.33760023\n",
      "Step  125000: train         neg_log_perplexity |  3.33760023\n",
      "Step  125000: train          sequence_accuracy |  0.00000000\n",
      "Step  125000: train weights_per_batch_per_core |  109.00000000\n",
      "Step  125000: eval                    accuracy |  0.43478262\n",
      "Step  125000: eval                        loss |  3.16479826\n",
      "Step  125000: eval          neg_log_perplexity |  3.16479826\n",
      "Step  125000: eval           sequence_accuracy |  0.00000000\n",
      "Step  125000: eval  weights_per_batch_per_core |  92.00000000\n",
      "Step  125000: Finished evaluation\n"
     ]
    }
   ],
   "source": [
    "for _ in range(15):\n",
    "    trainer.train_epoch(n_steps=1000, n_eval_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step  228738: Ran 1000 train steps in 2939.15 secs\n",
      "Step  228738: Evaluation\n",
      "Step  228738: train                   accuracy |  0.63265306\n",
      "Step  228738: train                       loss |  1.76499438\n",
      "Step  228738: train         neg_log_perplexity |  1.76499438\n",
      "Step  228738: train          sequence_accuracy |  0.00000000\n",
      "Step  228738: train weights_per_batch_per_core |  98.00000000\n",
      "Step  228738: eval                    accuracy |  0.61111110\n",
      "Step  228738: eval                        loss |  2.07283592\n",
      "Step  228738: eval          neg_log_perplexity |  2.07283592\n",
      "Step  228738: eval           sequence_accuracy |  0.00000000\n",
      "Step  228738: eval  weights_per_batch_per_core |  90.00000000\n",
      "Step  228738: Finished evaluation\n",
      "\n",
      "Step  229738: Ran 1000 train steps in 2705.43 secs\n",
      "Step  229738: Evaluation\n",
      "Step  229738: train                   accuracy |  0.54022986\n",
      "Step  229738: train                       loss |  2.19249463\n",
      "Step  229738: train         neg_log_perplexity |  2.19249463\n",
      "Step  229738: train          sequence_accuracy |  0.00000000\n",
      "Step  229738: train weights_per_batch_per_core |  87.00000000\n",
      "Step  229738: eval                    accuracy |  0.60439563\n",
      "Step  229738: eval                        loss |  2.08317018\n",
      "Step  229738: eval          neg_log_perplexity |  2.08317018\n",
      "Step  229738: eval           sequence_accuracy |  0.00000000\n",
      "Step  229738: eval  weights_per_batch_per_core |  91.00000000\n",
      "Step  229738: Finished evaluation\n",
      "\n",
      "Step  230738: Ran 1000 train steps in 2703.46 secs\n",
      "Step  230738: Evaluation\n",
      "Step  230738: train                   accuracy |  0.57575756\n",
      "Step  230738: train                       loss |  1.91170955\n",
      "Step  230738: train         neg_log_perplexity |  1.91170955\n",
      "Step  230738: train          sequence_accuracy |  0.00000000\n",
      "Step  230738: train weights_per_batch_per_core |  99.00000000\n",
      "Step  230738: eval                    accuracy |  0.50442475\n",
      "Step  230738: eval                        loss |  2.27024293\n",
      "Step  230738: eval          neg_log_perplexity |  2.27024293\n",
      "Step  230738: eval           sequence_accuracy |  0.00000000\n",
      "Step  230738: eval  weights_per_batch_per_core |  113.00000000\n",
      "Step  230738: Finished evaluation\n",
      "\n",
      "Step  231738: Ran 1000 train steps in 2702.35 secs\n",
      "Step  231738: Evaluation\n",
      "Step  231738: train                   accuracy |  0.50000000\n",
      "Step  231738: train                       loss |  2.83829904\n",
      "Step  231738: train         neg_log_perplexity |  2.83829904\n",
      "Step  231738: train          sequence_accuracy |  0.00000000\n",
      "Step  231738: train weights_per_batch_per_core |  86.00000000\n",
      "Step  231738: eval                    accuracy |  0.56470591\n",
      "Step  231738: eval                        loss |  2.41524911\n",
      "Step  231738: eval          neg_log_perplexity |  2.41524911\n",
      "Step  231738: eval           sequence_accuracy |  0.00000000\n",
      "Step  231738: eval  weights_per_batch_per_core |  85.00000000\n",
      "Step  231738: Finished evaluation\n",
      "\n",
      "Step  232738: Ran 1000 train steps in 2703.81 secs\n",
      "Step  232738: Evaluation\n",
      "Step  232738: train                   accuracy |  0.56818181\n",
      "Step  232738: train                       loss |  2.26666021\n",
      "Step  232738: train         neg_log_perplexity |  2.26666021\n",
      "Step  232738: train          sequence_accuracy |  0.00000000\n",
      "Step  232738: train weights_per_batch_per_core |  88.00000000\n",
      "Step  232738: eval                    accuracy |  0.55555558\n",
      "Step  232738: eval                        loss |  2.03856540\n",
      "Step  232738: eval          neg_log_perplexity |  2.03856540\n",
      "Step  232738: eval           sequence_accuracy |  0.00000000\n",
      "Step  232738: eval  weights_per_batch_per_core |  90.00000000\n",
      "Step  232738: Finished evaluation\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    trainer.train_epoch(n_steps=1000, n_eval_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "trying openNMT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
