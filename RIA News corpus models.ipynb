{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yX74fF7DtbFx"
   },
   "outputs": [],
   "source": [
    "# ! pip install OpenNMT-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4rkgF8oakzS"
   },
   "outputs": [],
   "source": [
    "# ! pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade https://storage.googleapis.com/jax-releases/cuda102/jaxlib-0.1.44-cp37-none-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aWHXIGkGuirc"
   },
   "outputs": [],
   "source": [
    "import onmt\n",
    "import pyonmttok\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import rouge\n",
    "import gin\n",
    "import trax\n",
    "import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26UBuSCvdxOw"
   },
   "source": [
    "### 0. Загрузка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ggnp8RmHqUum"
   },
   "outputs": [],
   "source": [
    "#! wget https://github.com/RossiyaSegodnya/ria_news_dataset/raw/master/ria.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-VmbB8SVqm9O"
   },
   "outputs": [],
   "source": [
    "#! gunzip ria.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iGJi355ZmWjW"
   },
   "outputs": [],
   "source": [
    "#TAG_RE = re.compile(r'<[^>]+>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A6fT2ZxY5f95"
   },
   "outputs": [],
   "source": [
    "def remove_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    droped_html_tags = re.sub(clean, ' ', text)\n",
    "    return re.sub(r'\\n|\\r', ' ', droped_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1cnnjs9O0lgv",
    "outputId": "e220014e-c0d8-4843-c6a8-6b648841e135"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1003869it [02:55, 5733.59it/s]\n"
     ]
    }
   ],
   "source": [
    "test_val_indexes = np.random.choice(1003869, 40000, replace=False) # 40000 индексов для теста и валидации\n",
    "\n",
    "with open('ria.json') as f:\n",
    "    for i, line in tqdm(enumerate(f)):\n",
    "        l = json.loads(line)\n",
    "        text = remove_tags(l['text'])\n",
    "        title = remove_tags(l['title'])\n",
    "        if i in test_val_indexes:\n",
    "            with open('texts_val.txt', 'a') as text_file:\n",
    "                text_file.write(text)\n",
    "                text_file.write('\\n')\n",
    "            with open('labels_val.txt', 'a') as label_file:\n",
    "                label_file.write(title)\n",
    "                label_file.write('\\n')\n",
    "        else:\n",
    "            with open('texts.txt', 'a') as text_file:\n",
    "                text_file.write(text)\n",
    "                text_file.write('\\n')\n",
    "            with open('labels.txt', 'a') as label_file:\n",
    "                label_file.write(title)\n",
    "                label_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X1y1JbVxQzFZ"
   },
   "outputs": [],
   "source": [
    "with open(\"texts_val.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    with open(\"val_texts.txt\", \"w\") as f1:\n",
    "        f1.writelines(lines[:20000])\n",
    "    with open(\"test_texts.txt\", \"w\") as f2:\n",
    "        f2.writelines(lines[20000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vM7gmTwFRtCM"
   },
   "outputs": [],
   "source": [
    "with open(\"labels_val.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    with open(\"val_labels.txt\", \"w\") as f1:\n",
    "        f1.writelines(lines[:20000])\n",
    "    with open(\"test_labels.txt\", \"w\") as f2:\n",
    "        f2.writelines(lines[20000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K4Mg7n9Ldq8y"
   },
   "source": [
    "### 1. BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TF86zazGT4ps"
   },
   "outputs": [],
   "source": [
    "# ~10 минут\n",
    "tokenizer = pyonmttok.Tokenizer(\"conservative\", joiner_annotate=True)\n",
    "learner = pyonmttok.BPELearner(tokenizer=tokenizer, symbols=50000)\n",
    "learner.ingest_file(\"texts.txt\")\n",
    "BPE = learner.learn(\"model-50k_with_joiner\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('lstm_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ekToGL7VCQU"
   },
   "outputs": [],
   "source": [
    "# ~ 20 минут\n",
    "BPE.tokenize_file(input_path='texts.txt', output_path='lstm_v2/texts_tokenized_train')\n",
    "BPE.tokenize_file(input_path='labels.txt', output_path='lstm_v2/labels_tokenized_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3IQkGB2xaTqq"
   },
   "outputs": [],
   "source": [
    "BPE.tokenize_file(input_path='val_texts.txt', output_path='lstm_v2/texts_tokenized_val')\n",
    "BPE.tokenize_file(input_path='val_labels.txt', output_path='lstm_v2/labels_tokenized_val')\n",
    "BPE.tokenize_file(input_path='test_texts.txt', output_path='lstm_v2/texts_tokenized_test')\n",
    "BPE.tokenize_file(input_path='test_labels.txt', output_path='lstm_v2/labels_tokenized_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cahl6LKJd9lJ"
   },
   "outputs": [],
   "source": [
    "os.remove('texts.txt')\n",
    "os.remove('labels.txt')\n",
    "os.remove('val_texts.txt')\n",
    "os.remove('val_labels.txt')\n",
    "os.remove('test_texts.txt')\n",
    "os.remove('test_labels.txt')\n",
    "os.remove('texts_val.txt')\n",
    "os.remove('labels_val.txt')\n",
    "os.remove('lstm_v2/texts_tokenized_train')\n",
    "os.remove('lstm_v2/labels_tokenized_train')\n",
    "os.remove('lstm_v2/texts_tokenized_val')\n",
    "os.remove('lstm_v2/labels_tokenized_val')\n",
    "os.remove('lstm_v2/texts_tokenized_test')\n",
    "os.remove('lstm_v2/labels_tokenized_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### очистка от пустых строк после BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lstm_v2/labels_tokenized_test') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = []\n",
    "for i, line in enumerate(lines):\n",
    "    if line == '\\n':\n",
    "        nulls.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:00, 61913.21it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, line in tqdm(enumerate(lines)):\n",
    "    if i not in nulls:\n",
    "        with open('lstm_v2/clear_labels_tokenized_test', 'a') as f:\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ICDIzVBdk77"
   },
   "source": [
    "### 2. Препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MqtJ1L2ScvRg"
   },
   "outputs": [],
   "source": [
    "#os.mkdir('lstm_v2/preprocessed_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clear_texts_tokenized_val',\n",
       " 'clear_labels_tokenized_train',\n",
       " 'clear_labels_tokenized_val',\n",
       " 'clear_texts_tokenized_test',\n",
       " 'clear_texts_tokenized_train',\n",
       " 'preprocessed_data',\n",
       " 'clear_labels_tokenized_test']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('lstm_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "id": "qP64vnRvZpHR",
    "outputId": "c1542917-c41f-470f-bdca-0e13317f3995"
   },
   "outputs": [],
   "source": [
    "! onmt_preprocess -train_src lstm_v2/clear_texts_tokenized_train \\\n",
    "                  -train_tgt lstm_v2/clear_labels_tokenized_train \\\n",
    "                  -valid_src lstm_v2/clear_texts_tokenized_val \\\n",
    "                  -valid_tgt lstm_v2/clear_labels_tokenized_val \\\n",
    "                  -src_seq_length 3000 \\\n",
    "                  -tgt_seq_length 1000 \\\n",
    "                  -filter_valid \\\n",
    "                  -lower \\\n",
    "                  -shard_size 50000 \\\n",
    "                  -num_threads 6 \\\n",
    "                  -save_data lstm_v2/preprocessed_data/preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!onmt_train -data lstm_v2/preprocessed_data/preprocessed_data  \\\n",
    "            -save_model lstm_v2/lstm_model_v2 \\\n",
    "            -train_steps 60000 \\\n",
    "            -valid_steps 10000 \\\n",
    "            -optim adam \\\n",
    "            -learning_rate 0.001 \\\n",
    "            -report_every 2000 \\\n",
    "            -gpu_ranks 0 \\\n",
    "            -tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-01-31 14:55:57,690 INFO] Translating shard 0.\n",
      "PRED AVG SCORE: -0.2222, PRED PPL: 1.2488\n",
      "Total translation time (s): 381.589313\n",
      "Average translation time (s): 0.038159\n",
      "Tokens per second: 262.900445\n",
      "[2020-01-31 15:02:20,377 INFO] Translating shard 1.\n",
      "PRED AVG SCORE: -0.2205, PRED PPL: 1.2467\n",
      "Total translation time (s): 379.849602\n",
      "Average translation time (s): 0.037985\n",
      "Tokens per second: 267.390039\n"
     ]
    }
   ],
   "source": [
    "! onmt_translate -gpu 0 \\\n",
    "               -batch_size 8 \\\n",
    "               -beam_size 10 \\\n",
    "               -model lstm_v2/lstm_model_v2_step_55000.pt \\\n",
    "               -src lstm_v2/clear_texts_tokenized_test \\\n",
    "               -output lstm_v2/pred_test.txt \\\n",
    "               -report_time \\\n",
    "               -min_length 1 \\\n",
    "               -stepwise_penalty \\\n",
    "               -coverage_penalty summary \\\n",
    "               -beta 5 \\\n",
    "               -length_penalty wu \\\n",
    "               -alpha 0.9 \\\n",
    "               -block_ngram_repeat 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPE.detokenize_file(input_path='lstm_v2/pred_test.txt', output_path='lstm_v2/pred_test_detokenized.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPE.detokenize_file(input_path='lstm_v2/clear_labels_tokenized_test', output_path='lstm_v2/true_test_detokenized.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Очистка от пустых настоящих заголовков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lstm_v2/pred_test_detokenized.txt') as f1:\n",
    "    predictions = f1.readlines()\n",
    "with open('lstm_v2/true_test_detokenized.txt') as f2:\n",
    "    true = f2.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2612 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, element in enumerate(true):\n",
    "    if len(element) < 5:\n",
    "        print(i, element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, element in enumerate(true):\n",
    "    if i != 2612:        \n",
    "        with open('lstm_v2/true.txt', 'a') as f1:\n",
    "            f1.write(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, element in enumerate(predictions):\n",
    "    if i != 2612:        \n",
    "        with open('lstm_v2/predictions.txt', 'a') as f2:\n",
    "            f2.write(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.40704861192204495,\n",
       "  'p': 0.4254433360751212,\n",
       "  'r': 0.39838380926281153},\n",
       " 'rouge-2': {'f': 0.22824682595138046,\n",
       "  'p': 0.23940092178879546,\n",
       "  'r': 0.22377283102694753},\n",
       " 'rouge-l': {'f': 0.3785419590169131,\n",
       "  'p': 0.40359357600470086,\n",
       "  'r': 0.3778888807812343}}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge import FilesRouge\n",
    "files_rouge_detoc = FilesRouge('lstm_v2/predictions.txt', 'lstm_v2/true.txt')\n",
    "scores_detoc = files_rouge_detoc.get_scores(avg=True)\n",
    "scores_detoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравнение предсказанных заголовков и настоящих"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['греция попросила помощи ес в борьбе с лесными пожарами\\n',\n",
       " 'российские инспекторы совершат наблюдательные полеты над сша и канадой\\n',\n",
       " 'глушаков выйдет в стартовом составе сборной рф на матч против греции\\n',\n",
       " 'чуров ради ордена александра невского отказался от другой награды\\n',\n",
       " 'митрохин снова стал председателем \"яблока\"\\n',\n",
       " 'второй за день взрыв прогремел в багдаде, более 10 человек погибли\\n',\n",
       " 'заработал взломанный накануне сайт поддержки егэ в петербурге\\n',\n",
       " 'лавров обсудил с аннаном перспективы проведения конференции по сирии\\n',\n",
       " 'беспорядки вспыхнули в колонии на ставрополье\\n',\n",
       " 'власти испании позитивно восприняли доклад мвф\\n',\n",
       " 'биография игоря зубова\\n',\n",
       " 'замглавы мвд алешин снят с должности\\n',\n",
       " '\"новая голландия\" 16 июня вновь открывается для посетителей\\n',\n",
       " 'выборы губернатора в новгородской области обойдутся в 59 млн руб\\n',\n",
       " 'дело заведено на возможного виновника дтп, где погиб полицейский\\n',\n",
       " 'константин зырянов пропустил тренировку сборной россии из-за простуды\\n',\n",
       " 'биржи европы закрылись ростом в ожидании мер по поддержке экономики\\n',\n",
       " 'состояние жкх лужского района шокировало главу ленобласти\\n',\n",
       " 'глава мид рф и замгенсека оон обсудили гуманитарную помощь сирии\\n',\n",
       " 'музей \"огни москвы\", или как столица офонарела\\n',\n",
       " 'поклонный крест святой евдокии московской установят на рождественском бульваре в москве\\n',\n",
       " 'солнце выжигает поля подсолнухов в калмыкии\\n',\n",
       " '\"северная верфь\" отказалась от иска к \"коммерсанту\" о защите репутации\\n',\n",
       " 'маркин: ряд свидетелей по делу о болотной могут стать подозреваемыми\\n',\n",
       " 'питание соответствует нормам в 10% проверенных школ и детсадов москвы\\n',\n",
       " 'режиссер питер гринуэй сравнил российское кино со старым вином\\n',\n",
       " 'стресс заставляет кузнечиков есть много углеводов, выяснили ученые\\n',\n",
       " 'сасовское летное училище получило новую впп\\n',\n",
       " 'в подмосковном красногорске прошел фестиваль народного искусства\\n',\n",
       " 'население новокузнецка сократилось за восемь лет на 17 тысяч человек\\n']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['греция попросила ес помочь в борьбе с лесными пожарами\\n',\n",
       " 'российские военные инспекторы совершат наблюдательные полеты над сша\\n',\n",
       " 'футболист глушаков выйдет в стартовом составе сборной россии по футболу\\n',\n",
       " 'чуров попросил наградить александра невского орденом александра невского\\n',\n",
       " '\"яблоко\" переизбрал митрохина на пост председателя партии\\n',\n",
       " 'число погибших при взрыве в багдаде возросло до 11 человек\\n',\n",
       " 'сайт поддержки егэ, взломанный накануне, заработал в петербурге\\n',\n",
       " 'лавров и аннан обсудили перспективы конференции по сирии\\n',\n",
       " 'беспорядки произошли на ставрополье, данных о пострадавших нет\\n',\n",
       " 'испания позитивно восприняла доклад мвф\\n',\n",
       " 'биография зубов зубов\\n',\n",
       " 'замглавы мвд рф, курировавший криминалом, снят с должности\\n',\n",
       " 'памятник архитектуры \"новая голландия\" открывается в петербурге\\n',\n",
       " 'новгородские власти направят 59 млн рублей на выборы губернатора\\n',\n",
       " 'житель тамбовской области насмерть сбил сотрудника дпс\\n',\n",
       " 'футболист зырянов пропустил тренировку перед матчем с греками из-за простуды\\n',\n",
       " 'биржи европы закрылись ростом на внутреннем оптимизме\\n',\n",
       " 'губернатор ленобласти раскритиковал власти за реформу жкх\\n',\n",
       " 'лавров и замгенсека оон обсудили ситуацию в сирии\\n',\n",
       " '\"светить всегда, светить везде\"\\n',\n",
       " 'поклонный крест установят на рождественском бульваре в москве\\n',\n",
       " 'в калмыкии из-за жары введен режим чрезвычайной ситуации\\n',\n",
       " '\"северная верфь\" отказалась от иска к \"коммерсанту\"\\n',\n",
       " 'ск может стать подозреваемыми по делу о беспорядках на болотной\\n',\n",
       " 'лишь каждая десятый школа в москве соответствует санитарным требованиям\\n',\n",
       " 'режиссер питер гринуэй считает, что в россии можно создавать кино\\n',\n",
       " 'кузнечики в состоянии стресса, считают ученые\\n',\n",
       " 'сасовское училище получило новую взлетную полосу\\n',\n",
       " 'фестиваль \"хранители наследия россии\" прошел в красногорске\\n',\n",
       " 'в новокузнецке в 2010 году живут 548 тыс человек\\n']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Transformer trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"PAD\": PAD_token, \"SOS\": SOS_token, \"EOS\": EOS_token}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count PAD, SOS, EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "\n",
    "vocab = Voc('News')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 963781/963781 [01:51<00:00, 8633.57it/s] \n",
      "100%|██████████| 963781/963781 [00:03<00:00, 260758.44it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('lstm_v2/clear_texts_tokenized_train') as train_texts_file:\n",
    "    lines = train_texts_file.readlines()\n",
    "\n",
    "for i in tqdm(range(len(lines))):\n",
    "    vocab.addSentence(lines[i])\n",
    "\n",
    "with open('lstm_v2/clear_labels_tokenized_train') as train_labels_file:\n",
    "    lines = train_labels_file.readlines()\n",
    "\n",
    "for i in tqdm(range(len(lines))):\n",
    "    vocab.addSentence(lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir('trax_transformer/samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "963781it [38:00, 422.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# записываем каждый сэмпл в отдельный файлик + обрезка и паддинг\n",
    "TEXT_MAX_LENGTH = 3000\n",
    "LABEL_MAX_LENGTH = 90\n",
    "\n",
    "with open('lstm_v2/clear_texts_tokenized_train') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open('lstm_v2/clear_labels_tokenized_train') as f:\n",
    "    label_lines = f.readlines()\n",
    "\n",
    "name = 1\n",
    "for text, label in tqdm(zip(lines, label_lines)):\n",
    "    text = text.split(' ')\n",
    "    t_len = len(text)\n",
    "    if t_len > TEXT_MAX_LENGTH:\n",
    "        text = text[:TEXT_MAX_LENGTH]\n",
    "    elif t_len < TEXT_MAX_LENGTH:\n",
    "        text.extend([\"PAD\"] * (TEXT_MAX_LENGTH - t_len))\n",
    "    \n",
    "    label = label.split(' ')\n",
    "    l_len = len(label)\n",
    "    mask = [1] * l_len\n",
    "    if l_len > LABEL_MAX_LENGTH:\n",
    "        label = label[:LABEL_MAX_LENGTH]\n",
    "        mask = mask[:LABEL_MAX_LENGT]\n",
    "    elif l_len < LABEL_MAX_LENGTH:\n",
    "        label.extend([\"PAD\"] * (LABEL_MAX_LENGTH - l_len))\n",
    "        mask.extend([0] * (LABEL_MAX_LENGTH - l_len))\n",
    "    \n",
    "    tokenized_text = []\n",
    "    for token in text:\n",
    "        tokenized_text.append(vocab.word2index[token])\n",
    "\n",
    "    tokenized_label = []\n",
    "    for token in label:\n",
    "        tokenized_label.append(vocab.word2index[token])\n",
    "\n",
    "    output = {'text': tokenized_text, 'mask': mask, 'label': tokenized_label}\n",
    "    \n",
    "    with open('trax_transformer/samples/{}'.format(name), 'w') as text_labels_tokens_file:\n",
    "        json.dump(output, text_labels_tokens_file)\n",
    "    \n",
    "    name +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# даталоадер\n",
    "def news_inputs(vocab, batch_size, files_dir):\n",
    "    files_names = os.listdir(files_dir)\n",
    "    \n",
    "    while True:\n",
    "        batch_indexes = np.random.choice(range(1, len(files_names)+1), size=batch_size, replace=False)\n",
    "        \n",
    "        texts = []\n",
    "        labels = []\n",
    "        mask = []\n",
    "        for index in batch_indexes:\n",
    "            with open(os.path.join(files_dir, str(index))) as file:\n",
    "                sample = json.loads(file.readline())\n",
    "                texts.append(sample['text'])\n",
    "                labels.append(sample['label'])\n",
    "                mask.append(sample['mask'])\n",
    "\n",
    "        texts = np.stack(texts)\n",
    "        labels = np.stack(labels)\n",
    "        mask = np.stack(mask)\n",
    "\n",
    "        yield texts, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_inputs = trax.supervised.Inputs(lambda _: news_inputs(vocab, 8, 'trax_transformer/samples'))\n",
    "data_stream = copy_inputs.train_stream(1)\n",
    "inputs, target, mask = next(data_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 3000)\n",
      "(8, 90)\n",
      "(8, 90)\n",
      "74009\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(target.shape)\n",
    "print(mask.shape)\n",
    "print(len(vocab.word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure hyperparameters.\n",
    "gin.parse_config(\"\"\"\n",
    "import trax.models\n",
    "import trax.optimizers\n",
    "import trax.supervised.inputs\n",
    "import trax.supervised.trainer_lib\n",
    "\n",
    "# Parameters for MultifactorSchedule:\n",
    "# ==============================================================================\n",
    "MultifactorSchedule.constant = 1.0\n",
    "MultifactorSchedule.factors = 'constant * linear_warmup * rsqrt_decay'\n",
    "MultifactorSchedule.warmup_steps = 20000\n",
    "\n",
    "# Parameters for Adafactor:\n",
    "# ==============================================================================\n",
    "Adafactor.beta1 = 0.0\n",
    "Adafactor.decay_rate = 0.8\n",
    "Adafactor.clipping_threshold = 1.0\n",
    "Adafactor.epsilon1 = 1e-30\n",
    "Adafactor.epsilon2 = 0.001\n",
    "Adafactor.factored = True\n",
    "Adafactor.multiply_by_parameter_scale = True\n",
    "\n",
    "\n",
    "# Parameters for train:\n",
    "# ==============================================================================\n",
    "train.optimizer = @trax.optimizers.Adafactor\n",
    "train.id_to_mask = 0\n",
    "\n",
    "# Parameters for Transformer:\n",
    "# ==============================================================================\n",
    "Transformer.d_model = 512\n",
    "Transformer.d_ff = 1024\n",
    "Transformer.dropout = 0.1\n",
    "Transformer.max_len = 3000\n",
    "Transformer.mode = 'train'\n",
    "Transformer.n_heads = 4\n",
    "Transformer.n_encoder_layers = 2\n",
    "Transformer.n_decoder_layers = 2\n",
    "Transformer.input_vocab_size = 74009\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir('trax_transformer/model_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shutil.rmtree('trax_transformer/model_1/eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from trax_transformer/model_1/model.pkl at step 159000\n"
     ]
    }
   ],
   "source": [
    "output_dir = ('trax_transformer/model_1')\n",
    "trainer = trax.supervised.Trainer(\n",
    "    model=trax.models.Transformer,\n",
    "    loss_fn=trax.layers.CrossEntropyLoss,\n",
    "    optimizer=trax.optimizers.Adafactor,\n",
    "    inputs=copy_inputs,\n",
    "    lr_schedule=trax.lr.MultifactorSchedule,\n",
    "    output_dir=output_dir,\n",
    "    has_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step  160000: Ran 1000 train steps in 888.60 secs\n",
      "Step  160000: Evaluation\n",
      "Step  160000: train                   accuracy |  0.61363637\n",
      "Step  160000: train                       loss |  2.13586664\n",
      "Step  160000: train         neg_log_perplexity |  2.13586664\n",
      "Step  160000: train weights_per_batch_per_core |  88.00000000\n",
      "Step  160000: eval                    accuracy |  0.48571429\n",
      "Step  160000: eval                        loss |  2.51269341\n",
      "Step  160000: eval          neg_log_perplexity |  2.51269341\n",
      "Step  160000: eval  weights_per_batch_per_core |  70.00000000\n",
      "Step  160000: Finished evaluation\n",
      "\n",
      "Step  161000: Ran 1000 train steps in 795.50 secs\n",
      "Step  161000: Evaluation\n",
      "Step  161000: train                   accuracy |  0.70652175\n",
      "Step  161000: train                       loss |  1.43594956\n",
      "Step  161000: train         neg_log_perplexity |  1.43594956\n",
      "Step  161000: train weights_per_batch_per_core |  92.00000000\n",
      "Step  161000: eval                    accuracy |  0.50000000\n",
      "Step  161000: eval                        loss |  2.81419897\n",
      "Step  161000: eval          neg_log_perplexity |  2.81419897\n",
      "Step  161000: eval  weights_per_batch_per_core |  86.00000000\n",
      "Step  161000: Finished evaluation\n",
      "\n",
      "Step  162000: Ran 1000 train steps in 796.03 secs\n",
      "Step  162000: Evaluation\n",
      "Step  162000: train                   accuracy |  0.67326730\n",
      "Step  162000: train                       loss |  1.37369299\n",
      "Step  162000: train         neg_log_perplexity |  1.37369299\n",
      "Step  162000: train weights_per_batch_per_core |  101.00000000\n",
      "Step  162000: eval                    accuracy |  0.57142860\n",
      "Step  162000: eval                        loss |  2.43268967\n",
      "Step  162000: eval          neg_log_perplexity |  2.43268967\n",
      "Step  162000: eval  weights_per_batch_per_core |  91.00000000\n",
      "Step  162000: Finished evaluation\n",
      "\n",
      "Step  163000: Ran 1000 train steps in 795.43 secs\n",
      "Step  163000: Evaluation\n",
      "Step  163000: train                   accuracy |  0.50574714\n",
      "Step  163000: train                       loss |  2.21669793\n",
      "Step  163000: train         neg_log_perplexity |  2.21669793\n",
      "Step  163000: train weights_per_batch_per_core |  87.00000000\n",
      "Step  163000: eval                    accuracy |  0.60824746\n",
      "Step  163000: eval                        loss |  1.68598258\n",
      "Step  163000: eval          neg_log_perplexity |  1.68598258\n",
      "Step  163000: eval  weights_per_batch_per_core |  97.00000000\n",
      "Step  163000: Finished evaluation\n",
      "\n",
      "Step  164000: Ran 1000 train steps in 795.67 secs\n",
      "Step  164000: Evaluation\n",
      "Step  164000: train                   accuracy |  0.56521738\n",
      "Step  164000: train                       loss |  2.05917025\n",
      "Step  164000: train         neg_log_perplexity |  2.05917025\n",
      "Step  164000: train weights_per_batch_per_core |  92.00000000\n",
      "Step  164000: eval                    accuracy |  0.70652175\n",
      "Step  164000: eval                        loss |  1.47698116\n",
      "Step  164000: eval          neg_log_perplexity |  1.47698116\n",
      "Step  164000: eval  weights_per_batch_per_core |  92.00000000\n",
      "Step  164000: Finished evaluation\n",
      "\n",
      "Step  165000: Ran 1000 train steps in 795.62 secs\n",
      "Step  165000: Evaluation\n",
      "Step  165000: train                   accuracy |  0.65000004\n",
      "Step  165000: train                       loss |  2.01189470\n",
      "Step  165000: train         neg_log_perplexity |  2.01189470\n",
      "Step  165000: train weights_per_batch_per_core |  80.00000000\n",
      "Step  165000: eval                    accuracy |  0.53012049\n",
      "Step  165000: eval                        loss |  2.96968794\n",
      "Step  165000: eval          neg_log_perplexity |  2.96968794\n",
      "Step  165000: eval  weights_per_batch_per_core |  83.00000000\n",
      "Step  165000: Finished evaluation\n",
      "\n",
      "Step  166000: Ran 1000 train steps in 795.88 secs\n",
      "Step  166000: Evaluation\n",
      "Step  166000: train                   accuracy |  0.51162791\n",
      "Step  166000: train                       loss |  2.72051597\n",
      "Step  166000: train         neg_log_perplexity |  2.72051597\n",
      "Step  166000: train weights_per_batch_per_core |  86.00000000\n",
      "Step  166000: eval                    accuracy |  0.48780486\n",
      "Step  166000: eval                        loss |  2.61871600\n",
      "Step  166000: eval          neg_log_perplexity |  2.61871600\n",
      "Step  166000: eval  weights_per_batch_per_core |  82.00000000\n",
      "Step  166000: Finished evaluation\n",
      "\n",
      "Step  167000: Ran 1000 train steps in 795.66 secs\n",
      "Step  167000: Evaluation\n",
      "Step  167000: train                   accuracy |  0.62650603\n",
      "Step  167000: train                       loss |  2.05436087\n",
      "Step  167000: train         neg_log_perplexity |  2.05436087\n",
      "Step  167000: train weights_per_batch_per_core |  83.00000000\n",
      "Step  167000: eval                    accuracy |  0.49425286\n",
      "Step  167000: eval                        loss |  2.59633160\n",
      "Step  167000: eval          neg_log_perplexity |  2.59633160\n",
      "Step  167000: eval  weights_per_batch_per_core |  87.00000000\n",
      "Step  167000: Finished evaluation\n",
      "\n",
      "Step  168000: Ran 1000 train steps in 795.95 secs\n",
      "Step  168000: Evaluation\n",
      "Step  168000: train                   accuracy |  0.56818181\n",
      "Step  168000: train                       loss |  2.26797080\n",
      "Step  168000: train         neg_log_perplexity |  2.26797080\n",
      "Step  168000: train weights_per_batch_per_core |  88.00000000\n",
      "Step  168000: eval                    accuracy |  0.57999998\n",
      "Step  168000: eval                        loss |  2.19914174\n",
      "Step  168000: eval          neg_log_perplexity |  2.19914174\n",
      "Step  168000: eval  weights_per_batch_per_core |  100.00000000\n",
      "Step  168000: Finished evaluation\n",
      "\n",
      "Step  169000: Ran 1000 train steps in 795.61 secs\n",
      "Step  169000: Evaluation\n",
      "Step  169000: train                   accuracy |  0.67032969\n",
      "Step  169000: train                       loss |  2.07134914\n",
      "Step  169000: train         neg_log_perplexity |  2.07134914\n",
      "Step  169000: train weights_per_batch_per_core |  91.00000000\n",
      "Step  169000: eval                    accuracy |  0.51282054\n",
      "Step  169000: eval                        loss |  2.71154237\n",
      "Step  169000: eval          neg_log_perplexity |  2.71154237\n",
      "Step  169000: eval  weights_per_batch_per_core |  78.00000000\n",
      "Step  169000: Finished evaluation\n",
      "\n",
      "Step  170000: Ran 1000 train steps in 795.64 secs\n",
      "Step  170000: Evaluation\n",
      "Step  170000: train                   accuracy |  0.48684210\n",
      "Step  170000: train                       loss |  2.96068025\n",
      "Step  170000: train         neg_log_perplexity |  2.96068025\n",
      "Step  170000: train weights_per_batch_per_core |  76.00000000\n",
      "Step  170000: eval                    accuracy |  0.55421686\n",
      "Step  170000: eval                        loss |  2.19569635\n",
      "Step  170000: eval          neg_log_perplexity |  2.19569635\n",
      "Step  170000: eval  weights_per_batch_per_core |  83.00000000\n",
      "Step  170000: Finished evaluation\n",
      "\n",
      "Step  171000: Ran 1000 train steps in 795.27 secs\n",
      "Step  171000: Evaluation\n",
      "Step  171000: train                   accuracy |  0.57142860\n",
      "Step  171000: train                       loss |  2.17882776\n",
      "Step  171000: train         neg_log_perplexity |  2.17882776\n",
      "Step  171000: train weights_per_batch_per_core |  91.00000000\n",
      "Step  171000: eval                    accuracy |  0.72631580\n",
      "Step  171000: eval                        loss |  1.06942475\n",
      "Step  171000: eval          neg_log_perplexity |  1.06942475\n",
      "Step  171000: eval  weights_per_batch_per_core |  95.00000000\n",
      "Step  171000: Finished evaluation\n",
      "\n",
      "Step  172000: Ran 1000 train steps in 796.35 secs\n",
      "Step  172000: Evaluation\n",
      "Step  172000: train                   accuracy |  0.57142860\n",
      "Step  172000: train                       loss |  1.98286593\n",
      "Step  172000: train         neg_log_perplexity |  1.98286593\n",
      "Step  172000: train weights_per_batch_per_core |  84.00000000\n",
      "Step  172000: eval                    accuracy |  0.67391306\n",
      "Step  172000: eval                        loss |  1.63042390\n",
      "Step  172000: eval          neg_log_perplexity |  1.63042390\n",
      "Step  172000: eval  weights_per_batch_per_core |  92.00000000\n",
      "Step  172000: Finished evaluation\n",
      "\n",
      "Step  173000: Ran 1000 train steps in 796.40 secs\n",
      "Step  173000: Evaluation\n",
      "Step  173000: train                   accuracy |  0.61363637\n",
      "Step  173000: train                       loss |  1.92400813\n",
      "Step  173000: train         neg_log_perplexity |  1.92400813\n",
      "Step  173000: train weights_per_batch_per_core |  88.00000000\n",
      "Step  173000: eval                    accuracy |  0.56944448\n",
      "Step  173000: eval                        loss |  1.77133203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  173000: eval          neg_log_perplexity |  1.77133203\n",
      "Step  173000: eval  weights_per_batch_per_core |  72.00000000\n",
      "Step  173000: Finished evaluation\n",
      "\n",
      "Step  174000: Ran 1000 train steps in 796.52 secs\n",
      "Step  174000: Evaluation\n",
      "Step  174000: train                   accuracy |  0.73404253\n",
      "Step  174000: train                       loss |  1.23178387\n",
      "Step  174000: train         neg_log_perplexity |  1.23178387\n",
      "Step  174000: train weights_per_batch_per_core |  94.00000000\n",
      "Step  174000: eval                    accuracy |  0.48837209\n",
      "Step  174000: eval                        loss |  2.49915838\n",
      "Step  174000: eval          neg_log_perplexity |  2.49915838\n",
      "Step  174000: eval  weights_per_batch_per_core |  86.00000000\n",
      "Step  174000: Finished evaluation\n",
      "\n",
      "Step  175000: Ran 1000 train steps in 796.55 secs\n",
      "Step  175000: Evaluation\n",
      "Step  175000: train                   accuracy |  0.65979385\n",
      "Step  175000: train                       loss |  1.73388553\n",
      "Step  175000: train         neg_log_perplexity |  1.73388553\n",
      "Step  175000: train weights_per_batch_per_core |  97.00000000\n",
      "Step  175000: eval                    accuracy |  0.59139782\n",
      "Step  175000: eval                        loss |  1.78689229\n",
      "Step  175000: eval          neg_log_perplexity |  1.78689229\n",
      "Step  175000: eval  weights_per_batch_per_core |  93.00000000\n",
      "Step  175000: Finished evaluation\n",
      "\n",
      "Step  176000: Ran 1000 train steps in 795.87 secs\n",
      "Step  176000: Evaluation\n",
      "Step  176000: train                   accuracy |  0.73493981\n",
      "Step  176000: train                       loss |  1.14218616\n",
      "Step  176000: train         neg_log_perplexity |  1.14218616\n",
      "Step  176000: train weights_per_batch_per_core |  83.00000000\n",
      "Step  176000: eval                    accuracy |  0.64285713\n",
      "Step  176000: eval                        loss |  1.60656965\n",
      "Step  176000: eval          neg_log_perplexity |  1.60656965\n",
      "Step  176000: eval  weights_per_batch_per_core |  84.00000000\n",
      "Step  176000: Finished evaluation\n",
      "\n",
      "Step  177000: Ran 1000 train steps in 796.00 secs\n",
      "Step  177000: Evaluation\n",
      "Step  177000: train                   accuracy |  0.65306121\n",
      "Step  177000: train                       loss |  1.40929782\n",
      "Step  177000: train         neg_log_perplexity |  1.40929782\n",
      "Step  177000: train weights_per_batch_per_core |  98.00000000\n",
      "Step  177000: eval                    accuracy |  0.72619051\n",
      "Step  177000: eval                        loss |  1.29016328\n",
      "Step  177000: eval          neg_log_perplexity |  1.29016328\n",
      "Step  177000: eval  weights_per_batch_per_core |  84.00000000\n",
      "Step  177000: Finished evaluation\n",
      "\n",
      "Step  178000: Ran 1000 train steps in 796.14 secs\n",
      "Step  178000: Evaluation\n",
      "Step  178000: train                   accuracy |  0.59574467\n",
      "Step  178000: train                       loss |  1.78459489\n",
      "Step  178000: train         neg_log_perplexity |  1.78459489\n",
      "Step  178000: train weights_per_batch_per_core |  94.00000000\n",
      "Step  178000: eval                    accuracy |  0.50000000\n",
      "Step  178000: eval                        loss |  2.58975744\n",
      "Step  178000: eval          neg_log_perplexity |  2.58975744\n",
      "Step  178000: eval  weights_per_batch_per_core |  96.00000000\n",
      "Step  178000: Finished evaluation\n",
      "\n",
      "Step  179000: Ran 1000 train steps in 796.23 secs\n",
      "Step  179000: Evaluation\n",
      "Step  179000: train                   accuracy |  0.54347825\n",
      "Step  179000: train                       loss |  2.29735208\n",
      "Step  179000: train         neg_log_perplexity |  2.29735208\n",
      "Step  179000: train weights_per_batch_per_core |  92.00000000\n",
      "Step  179000: eval                    accuracy |  0.51315790\n",
      "Step  179000: eval                        loss |  2.77229071\n",
      "Step  179000: eval          neg_log_perplexity |  2.77229071\n",
      "Step  179000: eval  weights_per_batch_per_core |  76.00000000\n",
      "Step  179000: Finished evaluation\n",
      "\n",
      "Step  180000: Ran 1000 train steps in 796.39 secs\n",
      "Step  180000: Evaluation\n",
      "Step  180000: train                   accuracy |  0.48809525\n",
      "Step  180000: train                       loss |  2.48364472\n",
      "Step  180000: train         neg_log_perplexity |  2.48364472\n",
      "Step  180000: train weights_per_batch_per_core |  84.00000000\n",
      "Step  180000: eval                    accuracy |  0.55555558\n",
      "Step  180000: eval                        loss |  2.53603387\n",
      "Step  180000: eval          neg_log_perplexity |  2.53603387\n",
      "Step  180000: eval  weights_per_batch_per_core |  90.00000000\n",
      "Step  180000: Finished evaluation\n",
      "\n",
      "Step  181000: Ran 1000 train steps in 796.33 secs\n",
      "Step  181000: Evaluation\n",
      "Step  181000: train                   accuracy |  0.64516127\n",
      "Step  181000: train                       loss |  2.70612168\n",
      "Step  181000: train         neg_log_perplexity |  2.70612168\n",
      "Step  181000: train weights_per_batch_per_core |  93.00000000\n",
      "Step  181000: eval                    accuracy |  0.54430383\n",
      "Step  181000: eval                        loss |  2.35064244\n",
      "Step  181000: eval          neg_log_perplexity |  2.35064244\n",
      "Step  181000: eval  weights_per_batch_per_core |  79.00000000\n",
      "Step  181000: Finished evaluation\n",
      "\n",
      "Step  182000: Ran 1000 train steps in 796.84 secs\n",
      "Step  182000: Evaluation\n",
      "Step  182000: train                   accuracy |  0.54945058\n",
      "Step  182000: train                       loss |  2.71797776\n",
      "Step  182000: train         neg_log_perplexity |  2.71797776\n",
      "Step  182000: train weights_per_batch_per_core |  91.00000000\n",
      "Step  182000: eval                    accuracy |  0.55421686\n",
      "Step  182000: eval                        loss |  2.27290821\n",
      "Step  182000: eval          neg_log_perplexity |  2.27290821\n",
      "Step  182000: eval  weights_per_batch_per_core |  83.00000000\n",
      "Step  182000: Finished evaluation\n",
      "\n",
      "Step  183000: Ran 1000 train steps in 796.92 secs\n",
      "Step  183000: Evaluation\n",
      "Step  183000: train                   accuracy |  0.63218391\n",
      "Step  183000: train                       loss |  2.25177360\n",
      "Step  183000: train         neg_log_perplexity |  2.25177360\n",
      "Step  183000: train weights_per_batch_per_core |  87.00000000\n",
      "Step  183000: eval                    accuracy |  0.67777777\n",
      "Step  183000: eval                        loss |  1.61078238\n",
      "Step  183000: eval          neg_log_perplexity |  1.61078238\n",
      "Step  183000: eval  weights_per_batch_per_core |  90.00000000\n",
      "Step  183000: Finished evaluation\n",
      "\n",
      "Step  184000: Ran 1000 train steps in 796.17 secs\n",
      "Step  184000: Evaluation\n",
      "Step  184000: train                   accuracy |  0.55681819\n",
      "Step  184000: train                       loss |  2.02380943\n",
      "Step  184000: train         neg_log_perplexity |  2.02380943\n",
      "Step  184000: train weights_per_batch_per_core |  88.00000000\n",
      "Step  184000: eval                    accuracy |  0.60240966\n",
      "Step  184000: eval                        loss |  1.98313856\n",
      "Step  184000: eval          neg_log_perplexity |  1.98313856\n",
      "Step  184000: eval  weights_per_batch_per_core |  83.00000000\n",
      "Step  184000: Finished evaluation\n",
      "\n",
      "Step  185000: Ran 1000 train steps in 796.53 secs\n",
      "Step  185000: Evaluation\n",
      "Step  185000: train                   accuracy |  0.60465115\n",
      "Step  185000: train                       loss |  1.85800350\n",
      "Step  185000: train         neg_log_perplexity |  1.85800350\n",
      "Step  185000: train weights_per_batch_per_core |  86.00000000\n",
      "Step  185000: eval                    accuracy |  0.60714287\n",
      "Step  185000: eval                        loss |  1.79082370\n",
      "Step  185000: eval          neg_log_perplexity |  1.79082370\n",
      "Step  185000: eval  weights_per_batch_per_core |  84.00000000\n",
      "Step  185000: Finished evaluation\n",
      "\n",
      "Step  186000: Ran 1000 train steps in 796.55 secs\n",
      "Step  186000: Evaluation\n",
      "Step  186000: train                   accuracy |  0.64044946\n",
      "Step  186000: train                       loss |  1.73354089\n",
      "Step  186000: train         neg_log_perplexity |  1.73354089\n",
      "Step  186000: train weights_per_batch_per_core |  89.00000000\n",
      "Step  186000: eval                    accuracy |  0.51724136\n",
      "Step  186000: eval                        loss |  2.86760187\n",
      "Step  186000: eval          neg_log_perplexity |  2.86760187\n",
      "Step  186000: eval  weights_per_batch_per_core |  87.00000000\n",
      "Step  186000: Finished evaluation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(30):\n",
    "    trainer.train_epoch(n_steps=1000, n_eval_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir('trax_transformer/samples_test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# записываем каждый сэмпл в отдельный файлик + обрезка и паддинг\n",
    "TEXT_MAX_LENGTH = 3000\n",
    "LABEL_MAX_LENGTH = 90\n",
    "cancels = 0\n",
    "\n",
    "with open('lstm_v2/clear_texts_tokenized_test') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open('lstm_v2/clear_labels_tokenized_test') as f:\n",
    "    label_lines = f.readlines()\n",
    "\n",
    "name = 1\n",
    "for text, label in tqdm(zip(lines, label_lines)):\n",
    "    try:\n",
    "        text = text.split(' ')\n",
    "        t_len = len(text)\n",
    "        if t_len > TEXT_MAX_LENGTH:\n",
    "            text = text[:TEXT_MAX_LENGTH]\n",
    "        elif t_len < TEXT_MAX_LENGTH:\n",
    "            text.extend([\"PAD\"] * (TEXT_MAX_LENGTH - t_len))\n",
    "\n",
    "        label = label.split(' ')\n",
    "        l_len = len(label)\n",
    "        mask = [1] * l_len\n",
    "        if l_len > LABEL_MAX_LENGTH:\n",
    "            label = label[:LABEL_MAX_LENGTH]\n",
    "            mask = mask[:LABEL_MAX_LENGT]\n",
    "        elif l_len < LABEL_MAX_LENGTH:\n",
    "            label.extend([\"PAD\"] * (LABEL_MAX_LENGTH - l_len))\n",
    "            mask.extend([0] * (LABEL_MAX_LENGTH - l_len))\n",
    "\n",
    "        tokenized_text = []\n",
    "        for token in text:\n",
    "            tokenized_text.append(vocab.word2index[token])\n",
    "\n",
    "        tokenized_label = []\n",
    "        for token in label:\n",
    "            tokenized_label.append(vocab.word2index[token])\n",
    "\n",
    "        output = {'text': tokenized_text, 'mask': mask, 'label': tokenized_label}\n",
    "\n",
    "        with open('trax_transformer/samples_test/{}'.format(name), 'w') as text_labels_tokens_file:\n",
    "            json.dump(output, text_labels_tokens_file)\n",
    "\n",
    "        name +=1\n",
    "    \n",
    "    except:\n",
    "        cancels +=1\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция для получения предсказаний модели\n",
    "def index_to_token(x):\n",
    "    return vocab.index2word[x]\n",
    "\n",
    "def validate_inputs(vocab, batch_size, files_dir, model, predict_dir, true_dir):\n",
    "    files_names = os.listdir(files_dir)\n",
    "    files_names = np.arange(1, len(files_names) + 1)\n",
    "    back_to_tokens = np.vectorize(index_to_token)\n",
    "    \n",
    "    batch_indexes = files_names[::batch_size]\n",
    "    for i in tqdm(range(len(batch_indexes) - 1)):\n",
    "        batch_range = range(batch_indexes[i], batch_indexes[i+1])\n",
    "        texts = []\n",
    "        labels = []\n",
    "        \n",
    "        for index in batch_range:\n",
    "            with open(os.path.join(files_dir, str(index))) as file:\n",
    "                sample = json.loads(file.readline())\n",
    "                texts.append(sample['text'])\n",
    "                labels.append(sample['label'])\n",
    "     \n",
    "        texts = np.stack(texts)\n",
    "        labels = np.stack(labels)\n",
    "\n",
    "        predict = model((texts, labels))[0].argmax(axis=-1)\n",
    "        mask = predict[:, 1:] - predict[:, :-1]\n",
    "\n",
    "        for j, sentence in enumerate(predict):\n",
    "            stop_index = np.argmax(mask[j] == 0) + 1\n",
    "            to_write = ' '.join(back_to_tokens(sentence[:stop_index]))\n",
    "            to_write = re.sub(r'\\n|\\r', '', to_write) + '\\n'\n",
    "            \n",
    "            stop_index_labels = np.argmax(labels[j] == 0)\n",
    "            true = ' '.join(back_to_tokens(labels[j][:stop_index_labels]))\n",
    "\n",
    "            with open(predict_dir, 'a') as file_prediction:\n",
    "                file_prediction.write(to_write)\n",
    "            \n",
    "            with open(true_dir, 'a') as file_true:\n",
    "                file_true.write(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 тестирование модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trax.models.Transformer(mode='eval')\n",
    "model.init_from_file('trax_transformer/model_1/model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2492/2492 [26:26<00:00,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "validate_inputs(vocab, 8, 'trax_transformer/samples_test', model,\n",
    "                'trax_transformer/model_1/prediction',\n",
    "                'trax_transformer/model_1/true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPE = pyonmttok.Tokenizer(\"conservative\", bpe_model_path=\"model-50k_with_joiner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.remove('trax_transformer/model_1/true_detokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPE.detokenize_file(input_path='trax_transformer/model_1/prediction',\n",
    "                    output_path='trax_transformer/model_1/prediction_detokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPE.detokenize_file(input_path='trax_transformer/model_1/true',\n",
    "                    output_path='trax_transformer/model_1/true_detokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Очистка от пустых предсказанных заголовков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trax_transformer/model_1/true_detokenized') as f:\n",
    "    true = f.readlines()\n",
    "with open('trax_transformer/model_1/prediction_detokenized') as f:\n",
    "    prediction = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancels = []\n",
    "for i, sentence in enumerate(prediction):\n",
    "    if len(sentence) < 5:\n",
    "        cancels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cancels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.remove('trax_transformer/model_1/true_for_metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, line in enumerate(prediction):\n",
    "    if i not in cancels:\n",
    "        with open('trax_transformer/model_1/prediction_for_metrics', 'a') as f1:\n",
    "            f1.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, line in enumerate(true):\n",
    "    if i not in cancels:\n",
    "        with open('trax_transformer/model_1/true_for_metrics', 'a') as f1:\n",
    "            f1.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import FilesRouge\n",
    "files_rouge_detoc = FilesRouge('trax_transformer/model_1/prediction_for_metrics',\n",
    "                              'trax_transformer/model_1/true_for_metrics')\n",
    "scores_detoc = files_rouge_detoc.get_scores(avg=True, ignore_empty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.4290884118256301,\n",
       "  'p': 0.4632726660197333,\n",
       "  'r': 0.4321201598256151},\n",
       " 'rouge-2': {'f': 0.2135105618761511,\n",
       "  'p': 0.20826609730102505,\n",
       "  'r': 0.2292366333617198},\n",
       " 'rouge-l': {'f': 0.4029890042208497,\n",
       "  'p': 0.45219391395157066,\n",
       "  'r': 0.42164615234501285}}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_detoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trax_transformer/model_1/prediction_for_metrics') as f:\n",
    "    prediction_samples = f.readlines()\n",
    "    \n",
    "with open('trax_transformer/model_1/true_for_metrics') as f:\n",
    "    true_samples = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'два здания обрушились в италии из-за взрыва газа, 10 человек ранены\\n'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_samples[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'два здания обрушились в горах, взрыва газа, есть человек пострадали ранены\\n'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_samples[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Reformer (reversible only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "#SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 1  # End-of-sentence token\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"PAD\": PAD_token, \"EOS\": EOS_token}\n",
    "        self.index2word = {PAD_token: \"PAD\", EOS_token:\"EOS\"}\n",
    "        self.num_words = 2  # Count PAD, EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "\n",
    "vocab = Voc('News')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 963781/963781 [01:41<00:00, 9507.58it/s] \n",
      "100%|██████████| 963781/963781 [00:03<00:00, 286434.17it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('lstm_v2/clear_texts_tokenized_train') as train_texts_file:\n",
    "    lines = train_texts_file.readlines()\n",
    "\n",
    "for i in tqdm(range(len(lines))):\n",
    "    vocab.addSentence(lines[i])\n",
    "\n",
    "with open('lstm_v2/clear_labels_tokenized_train') as train_labels_file:\n",
    "    lines = train_labels_file.readlines()\n",
    "\n",
    "for i in tqdm(range(len(lines))):\n",
    "    vocab.addSentence(lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir('Reformer_reversible/samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "963781it [38:26, 417.90it/s]\n"
     ]
    }
   ],
   "source": [
    "TEXT_MAX_LENGTH = 3000\n",
    "LABEL_MAX_LENGTH = 90\n",
    "\n",
    "with open('lstm_v2/clear_texts_tokenized_train') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open('lstm_v2/clear_labels_tokenized_train') as f:\n",
    "    label_lines = f.readlines()\n",
    "\n",
    "name = 1\n",
    "for text, label in tqdm(zip(lines, label_lines)):\n",
    "    text = text.split(' ')\n",
    "    t_len = len(text)\n",
    "    if t_len > TEXT_MAX_LENGTH:\n",
    "        text = text[:TEXT_MAX_LENGTH]\n",
    "    elif t_len < TEXT_MAX_LENGTH:\n",
    "        text.extend([\"PAD\"] * (TEXT_MAX_LENGTH - t_len))\n",
    "    \n",
    "    label = label.split(' ')\n",
    "    l_len = len(label)\n",
    "    mask = [1] * l_len\n",
    "    if l_len > LABEL_MAX_LENGTH:\n",
    "        label = label[:LABEL_MAX_LENGTH]\n",
    "        label[-1] = \"EOS\"\n",
    "        mask = mask[:LABEL_MAX_LENGTH]\n",
    "    elif l_len < LABEL_MAX_LENGTH:\n",
    "        label.append(\"EOS\")\n",
    "        mask.append(1)\n",
    "        label.extend([\"PAD\"] * (LABEL_MAX_LENGTH - l_len - 1))\n",
    "        mask.extend([0] * (LABEL_MAX_LENGTH - l_len - 1))\n",
    "    else:\n",
    "        label[-1] = \"EOS\"\n",
    "    \n",
    "    tokenized_text = []\n",
    "    for token in text:\n",
    "        tokenized_text.append(vocab.word2index[token])\n",
    "\n",
    "    tokenized_label = []\n",
    "    for token in label:\n",
    "        tokenized_label.append(vocab.word2index[token])\n",
    "\n",
    "    output = {'text': tokenized_text, 'mask': mask, 'label': tokenized_label}\n",
    "    \n",
    "    with open('Reformer_reversible/samples/{}'.format(name), 'w') as text_labels_tokens_file:\n",
    "        json.dump(output, text_labels_tokens_file)\n",
    "    \n",
    "    name +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_inputs(vocab, batch_size, files_dir):\n",
    "    files_names = os.listdir(files_dir)\n",
    "    \n",
    "    while True:\n",
    "        batch_indexes = np.random.choice(range(1, len(files_names)+1), size=batch_size, replace=False)\n",
    "        \n",
    "        texts = []\n",
    "        labels = []\n",
    "        mask = []\n",
    "        for index in batch_indexes:\n",
    "            with open(os.path.join(files_dir, str(index))) as file:\n",
    "                sample = json.loads(file.readline())\n",
    "                texts.append(sample['text'])\n",
    "                labels.append(sample['label'])\n",
    "                mask.append(sample['mask'])\n",
    "\n",
    "        texts = np.stack(texts)\n",
    "        labels = np.stack(labels)\n",
    "        mask = np.stack(mask)\n",
    "\n",
    "        yield texts, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_inputs = trax.supervised.Inputs(lambda _: news_inputs(vocab, 8, 'Reformer_reversible/samples'))\n",
    "data_stream = copy_inputs.train_stream(1)\n",
    "inputs, target, mask = next(data_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 3000)\n",
      "(8, 90)\n",
      "(8, 90)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(target.shape)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74008"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config(\"\"\"\n",
    "import trax.models\n",
    "import trax.optimizers\n",
    "import trax.supervised.inputs\n",
    "import trax.supervised.trainer_lib\n",
    "# Parameters for _is_jit_init:\n",
    "# ==============================================================================\n",
    "_is_jit_init.value = True\n",
    "# Parameters for _jit_predict_fn:\n",
    "# ==============================================================================\n",
    "_jit_predict_fn.jit = True\n",
    "# Parameters for _jit_update_fn\n",
    "# ==============================================================================\n",
    "_jit_update_fn.jit = True\n",
    "# Parameters for Adafactor:\n",
    "# ==============================================================================\n",
    "Adafactor.beta1 = 0.0\n",
    "Adafactor.decay_rate = 0.8\n",
    "Adafactor.clipping_threshold = 1.0\n",
    "Adafactor.epsilon1 = 1e-30\n",
    "Adafactor.epsilon2 = 0.001\n",
    "Adafactor.factored = True\n",
    "Adafactor.multiply_by_parameter_scale = True\n",
    "# Parameters for backend:\n",
    "# ==============================================================================\n",
    "backend.name = 'jax'\n",
    "# Parameters for EncDecAttention:\n",
    "# ==============================================================================\n",
    "EncDecAttention.masked = True\n",
    "EncDecAttention.n_parallel_heads = None\n",
    "EncDecAttention.use_python_loop = False\n",
    "EncDecAttention.use_reference_code = False\n",
    "\n",
    "# Parameters for MultifactorSchedule:\n",
    "# ==============================================================================\n",
    "MultifactorSchedule.constant = 1.0\n",
    "MultifactorSchedule.factors = 'constant * linear_warmup * rsqrt_decay'\n",
    "MultifactorSchedule.warmup_steps = 3000\n",
    "# Parameters for Reformer:\n",
    "# ==============================================================================\n",
    "Reformer.d_ff = 1024\n",
    "Reformer.d_model = 512\n",
    "Reformer.dropout = 0.1\n",
    "Reformer.ff_activation = @trax.layers.Relu\n",
    "Reformer.ff_dropout = 0.1\n",
    "Reformer.input_vocab_size = 74008\n",
    "Reformer.max_len = 3000\n",
    "Reformer.n_decoder_layers = 6\n",
    "Reformer.n_encoder_layers = 6\n",
    "Reformer.n_heads = 4\n",
    "#Reformer.output_vocab_size\n",
    "\n",
    "# Parameters for SelfAttention\n",
    "# ==============================================================================\n",
    "# SelfAttention.causal = False\n",
    "# SelfAttention.chunk_len = None\n",
    "# SelfAttention.masked = False\n",
    "# SelfAttention.n_chunks_after = 0\n",
    "# SelfAttention.n_chunks_before = 0\n",
    "# SelfAttention.n_parallel_heads = None\n",
    "# SelfAttention.predict_drop_len = 64\n",
    "# SelfAttention.predict_mem_len = 192\n",
    "# SelfAttention.share_qk = False\n",
    "# SelfAttention.use_python_loop = False\n",
    "# SelfAttention.use_reference_code = False\n",
    "\n",
    "# Parameters for train:\n",
    "# ==============================================================================\n",
    "#train.checkpoint_highest = None\n",
    "#train.checkpoint_lowest = None\n",
    "#train.checkpoints_at = None\n",
    "#train.eval_frequency = 1000\n",
    "#train.eval_steps = 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'Reformer_reversible/model_2'\n",
    "\n",
    "trainer = trax.supervised.Trainer(\n",
    "    model=trax.models.Reformer,\n",
    "    loss_fn=trax.layers.CrossEntropyLoss,\n",
    "    optimizer=trax.optimizers.Adafactor,\n",
    "    inputs=copy_inputs,\n",
    "    lr_schedule=trax.lr.MultifactorSchedule,\n",
    "    output_dir=output_dir,\n",
    "    id_to_mask=0,\n",
    "    has_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step  111000: Ran 1000 train steps in 3105.33 secs\n",
      "Step  111000: Evaluation\n",
      "Step  111000: train                   accuracy |  0.40217391\n",
      "Step  111000: train                       loss |  3.81247330\n",
      "Step  111000: train         neg_log_perplexity |  3.81247330\n",
      "Step  111000: train          sequence_accuracy |  0.00000000\n",
      "Step  111000: train weights_per_batch_per_core |  92.00000000\n",
      "Step  111000: eval                    accuracy |  0.40000001\n",
      "Step  111000: eval                        loss |  3.64806557\n",
      "Step  111000: eval          neg_log_perplexity |  3.64806557\n",
      "Step  111000: eval           sequence_accuracy |  0.00000000\n",
      "Step  111000: eval  weights_per_batch_per_core |  90.00000000\n",
      "Step  111000: Finished evaluation\n",
      "\n",
      "Step  112000: Ran 1000 train steps in 2883.80 secs\n",
      "Step  112000: Evaluation\n",
      "Step  112000: train                   accuracy |  0.46296296\n",
      "Step  112000: train                       loss |  3.51094055\n",
      "Step  112000: train         neg_log_perplexity |  3.51094055\n",
      "Step  112000: train          sequence_accuracy |  0.00000000\n",
      "Step  112000: train weights_per_batch_per_core |  108.00000000\n",
      "Step  112000: eval                    accuracy |  0.49999997\n",
      "Step  112000: eval                        loss |  2.49042082\n",
      "Step  112000: eval          neg_log_perplexity |  2.49042082\n",
      "Step  112000: eval           sequence_accuracy |  0.00000000\n",
      "Step  112000: eval  weights_per_batch_per_core |  82.00000000\n",
      "Step  112000: Finished evaluation\n",
      "\n",
      "Step  113000: Ran 1000 train steps in 2885.57 secs\n",
      "Step  113000: Evaluation\n",
      "Step  113000: train                   accuracy |  0.40860215\n",
      "Step  113000: train                       loss |  3.23141122\n",
      "Step  113000: train         neg_log_perplexity |  3.23141122\n",
      "Step  113000: train          sequence_accuracy |  0.00000000\n",
      "Step  113000: train weights_per_batch_per_core |  93.00000000\n",
      "Step  113000: eval                    accuracy |  0.38202247\n",
      "Step  113000: eval                        loss |  3.21137071\n",
      "Step  113000: eval          neg_log_perplexity |  3.21137071\n",
      "Step  113000: eval           sequence_accuracy |  0.00000000\n",
      "Step  113000: eval  weights_per_batch_per_core |  89.00000000\n",
      "Step  113000: Finished evaluation\n",
      "\n",
      "Step  114000: Ran 1000 train steps in 2866.05 secs\n",
      "Step  114000: Evaluation\n",
      "Step  114000: train                   accuracy |  0.47826087\n",
      "Step  114000: train                       loss |  2.52207899\n",
      "Step  114000: train         neg_log_perplexity |  2.52207899\n",
      "Step  114000: train          sequence_accuracy |  0.00000000\n",
      "Step  114000: train weights_per_batch_per_core |  92.00000000\n",
      "Step  114000: eval                    accuracy |  0.50505048\n",
      "Step  114000: eval                        loss |  2.75413775\n",
      "Step  114000: eval          neg_log_perplexity |  2.75413775\n",
      "Step  114000: eval           sequence_accuracy |  0.00000000\n",
      "Step  114000: eval  weights_per_batch_per_core |  99.00000000\n",
      "Step  114000: Finished evaluation\n",
      "\n",
      "Step  115000: Ran 1000 train steps in 2824.06 secs\n",
      "Step  115000: Evaluation\n",
      "Step  115000: train                   accuracy |  0.44554454\n",
      "Step  115000: train                       loss |  3.31455517\n",
      "Step  115000: train         neg_log_perplexity |  3.31455517\n",
      "Step  115000: train          sequence_accuracy |  0.00000000\n",
      "Step  115000: train weights_per_batch_per_core |  101.00000000\n",
      "Step  115000: eval                    accuracy |  0.49038464\n",
      "Step  115000: eval                        loss |  2.68826270\n",
      "Step  115000: eval          neg_log_perplexity |  2.68826270\n",
      "Step  115000: eval           sequence_accuracy |  0.00000000\n",
      "Step  115000: eval  weights_per_batch_per_core |  104.00000000\n",
      "Step  115000: Finished evaluation\n",
      "\n",
      "Step  116000: Ran 1000 train steps in 2816.83 secs\n",
      "Step  116000: Evaluation\n",
      "Step  116000: train                   accuracy |  0.44999999\n",
      "Step  116000: train                       loss |  3.11629629\n",
      "Step  116000: train         neg_log_perplexity |  3.11629629\n",
      "Step  116000: train          sequence_accuracy |  0.00000000\n",
      "Step  116000: train weights_per_batch_per_core |  100.00000000\n",
      "Step  116000: eval                    accuracy |  0.54117650\n",
      "Step  116000: eval                        loss |  2.94160175\n",
      "Step  116000: eval          neg_log_perplexity |  2.94160175\n",
      "Step  116000: eval           sequence_accuracy |  0.00000000\n",
      "Step  116000: eval  weights_per_batch_per_core |  85.00000000\n",
      "Step  116000: Finished evaluation\n",
      "\n",
      "Step  117000: Ran 1000 train steps in 2848.87 secs\n",
      "Step  117000: Evaluation\n",
      "Step  117000: train                   accuracy |  0.44791669\n",
      "Step  117000: train                       loss |  3.08398247\n",
      "Step  117000: train         neg_log_perplexity |  3.08398247\n",
      "Step  117000: train          sequence_accuracy |  0.00000000\n",
      "Step  117000: train weights_per_batch_per_core |  96.00000000\n",
      "Step  117000: eval                    accuracy |  0.49019611\n",
      "Step  117000: eval                        loss |  2.61189437\n",
      "Step  117000: eval          neg_log_perplexity |  2.61189437\n",
      "Step  117000: eval           sequence_accuracy |  0.00000000\n",
      "Step  117000: eval  weights_per_batch_per_core |  102.00000000\n",
      "Step  117000: Finished evaluation\n",
      "\n",
      "Step  118000: Ran 1000 train steps in 2821.54 secs\n",
      "Step  118000: Evaluation\n",
      "Step  118000: train                   accuracy |  0.53535354\n",
      "Step  118000: train                       loss |  2.50759482\n",
      "Step  118000: train         neg_log_perplexity |  2.50759482\n",
      "Step  118000: train          sequence_accuracy |  0.00000000\n",
      "Step  118000: train weights_per_batch_per_core |  99.00000000\n",
      "Step  118000: eval                    accuracy |  0.51401865\n",
      "Step  118000: eval                        loss |  2.55365729\n",
      "Step  118000: eval          neg_log_perplexity |  2.55365729\n",
      "Step  118000: eval           sequence_accuracy |  0.00000000\n",
      "Step  118000: eval  weights_per_batch_per_core |  107.00000000\n",
      "Step  118000: Finished evaluation\n",
      "\n",
      "Step  119000: Ran 1000 train steps in 2817.34 secs\n",
      "Step  119000: Evaluation\n",
      "Step  119000: train                   accuracy |  0.47311828\n",
      "Step  119000: train                       loss |  2.97789407\n",
      "Step  119000: train         neg_log_perplexity |  2.97789407\n",
      "Step  119000: train          sequence_accuracy |  0.00000000\n",
      "Step  119000: train weights_per_batch_per_core |  93.00000000\n",
      "Step  119000: eval                    accuracy |  0.35051548\n",
      "Step  119000: eval                        loss |  3.59041405\n",
      "Step  119000: eval          neg_log_perplexity |  3.59041405\n",
      "Step  119000: eval           sequence_accuracy |  0.00000000\n",
      "Step  119000: eval  weights_per_batch_per_core |  97.00000000\n",
      "Step  119000: Finished evaluation\n",
      "\n",
      "Step  120000: Ran 1000 train steps in 2821.57 secs\n",
      "Step  120000: Evaluation\n",
      "Step  120000: train                   accuracy |  0.44680849\n",
      "Step  120000: train                       loss |  3.12868905\n",
      "Step  120000: train         neg_log_perplexity |  3.12868905\n",
      "Step  120000: train          sequence_accuracy |  0.00000000\n",
      "Step  120000: train weights_per_batch_per_core |  94.00000000\n",
      "Step  120000: eval                    accuracy |  0.45714289\n",
      "Step  120000: eval                        loss |  2.74544144\n",
      "Step  120000: eval          neg_log_perplexity |  2.74544144\n",
      "Step  120000: eval           sequence_accuracy |  0.00000000\n",
      "Step  120000: eval  weights_per_batch_per_core |  105.00000000\n",
      "Step  120000: Finished evaluation\n",
      "\n",
      "Step  121000: Ran 1000 train steps in 2876.35 secs\n",
      "Step  121000: Evaluation\n",
      "Step  121000: train                   accuracy |  0.47747749\n",
      "Step  121000: train                       loss |  3.02447987\n",
      "Step  121000: train         neg_log_perplexity |  3.02447987\n",
      "Step  121000: train          sequence_accuracy |  0.00000000\n",
      "Step  121000: train weights_per_batch_per_core |  111.00000000\n",
      "Step  121000: eval                    accuracy |  0.41000000\n",
      "Step  121000: eval                        loss |  3.26112485\n",
      "Step  121000: eval          neg_log_perplexity |  3.26112485\n",
      "Step  121000: eval           sequence_accuracy |  0.00000000\n",
      "Step  121000: eval  weights_per_batch_per_core |  100.00000000\n",
      "Step  121000: Finished evaluation\n",
      "\n",
      "Step  122000: Ran 1000 train steps in 2884.71 secs\n",
      "Step  122000: Evaluation\n",
      "Step  122000: train                   accuracy |  0.41964287\n",
      "Step  122000: train                       loss |  3.05906844\n",
      "Step  122000: train         neg_log_perplexity |  3.05906844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  122000: train          sequence_accuracy |  0.00000000\n",
      "Step  122000: train weights_per_batch_per_core |  112.00000000\n",
      "Step  122000: eval                    accuracy |  0.45833334\n",
      "Step  122000: eval                        loss |  2.80004311\n",
      "Step  122000: eval          neg_log_perplexity |  2.80004311\n",
      "Step  122000: eval           sequence_accuracy |  0.00000000\n",
      "Step  122000: eval  weights_per_batch_per_core |  96.00000000\n",
      "Step  122000: Finished evaluation\n",
      "\n",
      "Step  123000: Ran 1000 train steps in 2885.01 secs\n",
      "Step  123000: Evaluation\n",
      "Step  123000: train                   accuracy |  0.53846157\n",
      "Step  123000: train                       loss |  2.62778187\n",
      "Step  123000: train         neg_log_perplexity |  2.62778187\n",
      "Step  123000: train          sequence_accuracy |  0.00000000\n",
      "Step  123000: train weights_per_batch_per_core |  104.00000000\n",
      "Step  123000: eval                    accuracy |  0.49367091\n",
      "Step  123000: eval                        loss |  3.09498739\n",
      "Step  123000: eval          neg_log_perplexity |  3.09498739\n",
      "Step  123000: eval           sequence_accuracy |  0.00000000\n",
      "Step  123000: eval  weights_per_batch_per_core |  79.00000000\n",
      "Step  123000: Finished evaluation\n",
      "\n",
      "Step  124000: Ran 1000 train steps in 2887.57 secs\n",
      "Step  124000: Evaluation\n",
      "Step  124000: train                   accuracy |  0.36538464\n",
      "Step  124000: train                       loss |  3.45366788\n",
      "Step  124000: train         neg_log_perplexity |  3.45366788\n",
      "Step  124000: train          sequence_accuracy |  0.00000000\n",
      "Step  124000: train weights_per_batch_per_core |  104.00000000\n",
      "Step  124000: eval                    accuracy |  0.46078432\n",
      "Step  124000: eval                        loss |  2.96603155\n",
      "Step  124000: eval          neg_log_perplexity |  2.96603155\n",
      "Step  124000: eval           sequence_accuracy |  0.00000000\n",
      "Step  124000: eval  weights_per_batch_per_core |  102.00000000\n",
      "Step  124000: Finished evaluation\n",
      "\n",
      "Step  125000: Ran 1000 train steps in 2884.48 secs\n",
      "Step  125000: Evaluation\n",
      "Step  125000: train                   accuracy |  0.42201832\n",
      "Step  125000: train                       loss |  3.33760023\n",
      "Step  125000: train         neg_log_perplexity |  3.33760023\n",
      "Step  125000: train          sequence_accuracy |  0.00000000\n",
      "Step  125000: train weights_per_batch_per_core |  109.00000000\n",
      "Step  125000: eval                    accuracy |  0.43478262\n",
      "Step  125000: eval                        loss |  3.16479826\n",
      "Step  125000: eval          neg_log_perplexity |  3.16479826\n",
      "Step  125000: eval           sequence_accuracy |  0.00000000\n",
      "Step  125000: eval  weights_per_batch_per_core |  92.00000000\n",
      "Step  125000: Finished evaluation\n"
     ]
    }
   ],
   "source": [
    "for _ in range(15):\n",
    "    trainer.train_epoch(n_steps=1000, n_eval_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step  228738: Ran 1000 train steps in 2939.15 secs\n",
      "Step  228738: Evaluation\n",
      "Step  228738: train                   accuracy |  0.63265306\n",
      "Step  228738: train                       loss |  1.76499438\n",
      "Step  228738: train         neg_log_perplexity |  1.76499438\n",
      "Step  228738: train          sequence_accuracy |  0.00000000\n",
      "Step  228738: train weights_per_batch_per_core |  98.00000000\n",
      "Step  228738: eval                    accuracy |  0.61111110\n",
      "Step  228738: eval                        loss |  2.07283592\n",
      "Step  228738: eval          neg_log_perplexity |  2.07283592\n",
      "Step  228738: eval           sequence_accuracy |  0.00000000\n",
      "Step  228738: eval  weights_per_batch_per_core |  90.00000000\n",
      "Step  228738: Finished evaluation\n",
      "\n",
      "Step  229738: Ran 1000 train steps in 2705.43 secs\n",
      "Step  229738: Evaluation\n",
      "Step  229738: train                   accuracy |  0.54022986\n",
      "Step  229738: train                       loss |  2.19249463\n",
      "Step  229738: train         neg_log_perplexity |  2.19249463\n",
      "Step  229738: train          sequence_accuracy |  0.00000000\n",
      "Step  229738: train weights_per_batch_per_core |  87.00000000\n",
      "Step  229738: eval                    accuracy |  0.60439563\n",
      "Step  229738: eval                        loss |  2.08317018\n",
      "Step  229738: eval          neg_log_perplexity |  2.08317018\n",
      "Step  229738: eval           sequence_accuracy |  0.00000000\n",
      "Step  229738: eval  weights_per_batch_per_core |  91.00000000\n",
      "Step  229738: Finished evaluation\n",
      "\n",
      "Step  230738: Ran 1000 train steps in 2703.46 secs\n",
      "Step  230738: Evaluation\n",
      "Step  230738: train                   accuracy |  0.57575756\n",
      "Step  230738: train                       loss |  1.91170955\n",
      "Step  230738: train         neg_log_perplexity |  1.91170955\n",
      "Step  230738: train          sequence_accuracy |  0.00000000\n",
      "Step  230738: train weights_per_batch_per_core |  99.00000000\n",
      "Step  230738: eval                    accuracy |  0.50442475\n",
      "Step  230738: eval                        loss |  2.27024293\n",
      "Step  230738: eval          neg_log_perplexity |  2.27024293\n",
      "Step  230738: eval           sequence_accuracy |  0.00000000\n",
      "Step  230738: eval  weights_per_batch_per_core |  113.00000000\n",
      "Step  230738: Finished evaluation\n",
      "\n",
      "Step  231738: Ran 1000 train steps in 2702.35 secs\n",
      "Step  231738: Evaluation\n",
      "Step  231738: train                   accuracy |  0.50000000\n",
      "Step  231738: train                       loss |  2.83829904\n",
      "Step  231738: train         neg_log_perplexity |  2.83829904\n",
      "Step  231738: train          sequence_accuracy |  0.00000000\n",
      "Step  231738: train weights_per_batch_per_core |  86.00000000\n",
      "Step  231738: eval                    accuracy |  0.56470591\n",
      "Step  231738: eval                        loss |  2.41524911\n",
      "Step  231738: eval          neg_log_perplexity |  2.41524911\n",
      "Step  231738: eval           sequence_accuracy |  0.00000000\n",
      "Step  231738: eval  weights_per_batch_per_core |  85.00000000\n",
      "Step  231738: Finished evaluation\n",
      "\n",
      "Step  232738: Ran 1000 train steps in 2703.81 secs\n",
      "Step  232738: Evaluation\n",
      "Step  232738: train                   accuracy |  0.56818181\n",
      "Step  232738: train                       loss |  2.26666021\n",
      "Step  232738: train         neg_log_perplexity |  2.26666021\n",
      "Step  232738: train          sequence_accuracy |  0.00000000\n",
      "Step  232738: train weights_per_batch_per_core |  88.00000000\n",
      "Step  232738: eval                    accuracy |  0.55555558\n",
      "Step  232738: eval                        loss |  2.03856540\n",
      "Step  232738: eval          neg_log_perplexity |  2.03856540\n",
      "Step  232738: eval           sequence_accuracy |  0.00000000\n",
      "Step  232738: eval  weights_per_batch_per_core |  90.00000000\n",
      "Step  232738: Finished evaluation\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    trainer.train_epoch(n_steps=1000, n_eval_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "trying openNMT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
