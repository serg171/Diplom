{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Копия Experiments_3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ykb-vUY5q2Y9",
        "Jo2lE2B7hJgT",
        "rHuctw7zjM3I",
        "ruIxiPk_qtnW",
        "NuOU6jqwo8aJ",
        "rvCdz0EbeKx6",
        "AP7HSGiukfKZ",
        "ZG15kakQknvS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b8mPXjZLsJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install OpenNMT-py\n",
        "! pip install rouge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN7M6oYclgdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import copy\n",
        "import math\n",
        "import spacy\n",
        "import torch\n",
        "import pyonmttok\n",
        "import json\n",
        "from shutil import copyfile\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6QCsVYLy52E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "73351afa-e3fd-46d9-d847-b945deb0de7a"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jun 16 12:13:54 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykb-vUY5q2Y9",
        "colab_type": "text"
      },
      "source": [
        "#### Data preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2po_H-Murf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH5SyhhAOfrv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41c84e42-4277-445a-b7e3-3dbb5d5df7d0"
      },
      "source": [
        "copyfile('/content/drive/My Drive/TransformerLSH/model-50k_with_joiner', '/content/model-50k_with_joiner')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/model-50k_with_joiner'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tut9ilPwnMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Default word tokens\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {\"PAD\": PAD_token, \"SOS\": SOS_token, \"EOS\": EOS_token}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token:\"EOS\"}\n",
        "        self.num_words = 3  # Count PAD, SOS, EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "\n",
        "vocab = Voc('News')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvtgeDYOx4X5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "127dbea1-5bb5-4341-b599-b818ba4f32ba"
      },
      "source": [
        "TRAIN_TEST_DATASET_LEN = 21000\n",
        "\n",
        "with open('/content/drive/My Drive/texts_tokenized_val') as train_texts_file:\n",
        "    lines = train_texts_file.readlines()[:TRAIN_TEST_DATASET_LEN]\n",
        "\n",
        "for i in tqdm(range(len(lines))):\n",
        "    vocab.addSentence(lines[i])\n",
        "\n",
        "with open('/content/drive/My Drive/labels_tokenized_val') as train_labels_file:\n",
        "    lines = train_labels_file.readlines()[:TRAIN_TEST_DATASET_LEN]\n",
        "\n",
        "for i in tqdm(range(len(lines))):\n",
        "    vocab.addSentence(lines[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 21000/21000 [00:03<00:00, 6882.98it/s]\n",
            "100%|██████████| 21000/21000 [00:00<00:00, 164306.08it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMwBrNgIguQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLPDataset(Dataset):\n",
        "    def __init__(self, samples_dir):       \n",
        "        self.samples_dir = samples_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(os.listdir(self.samples_dir))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        file_dir = os.path.join(self.samples_dir, str(idx+1))\n",
        "        with open(file_dir) as file:\n",
        "            sample = json.loads(file.readline())\n",
        "            text = np.array(sample['text'])\n",
        "            label = np.array(sample['label'])\n",
        "\n",
        "        sample = {'text': text, 'label': label}\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az7izJ2eiwQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# max label length = 36\n",
        "# 99.7% of texts' length < 4750"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3fLGAm3gYgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import shutil\n",
        "# shutil.rmtree('samples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGLOBapZx4VK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc83404b-be20-4057-d321-578e7d2c1bd0"
      },
      "source": [
        "os.mkdir('samples')\n",
        "\n",
        "DATASET_LEN = 20000\n",
        "TEXT_MAX_LENGTH = 2048\n",
        "LABEL_MAX_LENGTH = 32\n",
        "\n",
        "with open('/content/drive/My Drive/texts_tokenized_val') as f:\n",
        "    lines = f.readlines()[:DATASET_LEN]\n",
        "\n",
        "with open('/content/drive/My Drive/labels_tokenized_val') as f:\n",
        "    label_lines = f.readlines()[:DATASET_LEN]\n",
        "\n",
        "name = 1\n",
        "for text, label in tqdm(zip(lines, label_lines)):\n",
        "    text = text.split(' ')\n",
        "    t_len = len(text)\n",
        "    if t_len > TEXT_MAX_LENGTH:\n",
        "        text = text[:TEXT_MAX_LENGTH]\n",
        "    elif t_len < TEXT_MAX_LENGTH:\n",
        "        text.extend([\"PAD\"] * (TEXT_MAX_LENGTH - t_len))\n",
        "    \n",
        "    label = label.split(' ')\n",
        "    label = [\"SOS\"] + label\n",
        "    l_len = len(label)\n",
        "    mask = [1] * l_len\n",
        "    if l_len > LABEL_MAX_LENGTH:\n",
        "        label = label[:LABEL_MAX_LENGTH]\n",
        "        label[-1] = \"EOS\"\n",
        "        mask = mask[:LABEL_MAX_LENGTH]\n",
        "    elif l_len < LABEL_MAX_LENGTH:\n",
        "        label.append(\"EOS\")\n",
        "        mask.append(1)\n",
        "        label.extend([\"PAD\"] * (LABEL_MAX_LENGTH - l_len - 1))\n",
        "        mask.extend([0] * (LABEL_MAX_LENGTH - l_len - 1))\n",
        "    else:\n",
        "        label[-1] = \"EOS\"\n",
        "    \n",
        "    tokenized_text = []\n",
        "    for token in text:\n",
        "        tokenized_text.append(vocab.word2index[token])\n",
        "\n",
        "    tokenized_label = []\n",
        "    for token in label:\n",
        "        tokenized_label.append(vocab.word2index[token])\n",
        "\n",
        "    output = {'text': tokenized_text, 'mask': mask, 'label': tokenized_label}\n",
        "    \n",
        "    with open('samples/{}'.format(name), 'w') as text_labels_tokens_file:\n",
        "        json.dump(output, text_labels_tokens_file)\n",
        "    \n",
        "    name +=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000it [00:43, 455.48it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWF2G6ReGogn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9a01b2b-7213-425a-bb0b-3988990c75c4"
      },
      "source": [
        "os.mkdir('test_samples')\n",
        "\n",
        "TEST_DATASET_LEN = 1000\n",
        "TEXT_MAX_LENGTH = 2048\n",
        "LABEL_MAX_LENGTH = 32\n",
        "\n",
        "with open('/content/drive/My Drive/texts_tokenized_val') as f:\n",
        "    lines = f.readlines()[DATASET_LEN:DATASET_LEN + TEST_DATASET_LEN]\n",
        "\n",
        "with open('/content/drive/My Drive/labels_tokenized_val') as f:\n",
        "    label_lines = f.readlines()[DATASET_LEN:DATASET_LEN + TEST_DATASET_LEN]\n",
        "\n",
        "name = 1\n",
        "for text, label in tqdm(zip(lines, label_lines)):\n",
        "    text = text.split(' ')\n",
        "    t_len = len(text)\n",
        "    if t_len > TEXT_MAX_LENGTH:\n",
        "        text = text[:TEXT_MAX_LENGTH]\n",
        "    elif t_len < TEXT_MAX_LENGTH:\n",
        "        text.extend([\"PAD\"] * (TEXT_MAX_LENGTH - t_len))\n",
        "    \n",
        "    label = label.split(' ')\n",
        "    label = [\"SOS\"] + label\n",
        "    l_len = len(label)\n",
        "    mask = [1] * l_len\n",
        "    if l_len > LABEL_MAX_LENGTH:\n",
        "        label = label[:LABEL_MAX_LENGTH]\n",
        "        label[-1] = \"EOS\"\n",
        "        mask = mask[:LABEL_MAX_LENGTH]\n",
        "    elif l_len < LABEL_MAX_LENGTH:\n",
        "        label.append(\"EOS\")\n",
        "        mask.append(1)\n",
        "        label.extend([\"PAD\"] * (LABEL_MAX_LENGTH - l_len - 1))\n",
        "        mask.extend([0] * (LABEL_MAX_LENGTH - l_len - 1))\n",
        "    else:\n",
        "        label[-1] = \"EOS\"\n",
        "    \n",
        "    tokenized_text = []\n",
        "    for token in text:\n",
        "        tokenized_text.append(vocab.word2index[token])\n",
        "\n",
        "    tokenized_label = []\n",
        "    for token in label:\n",
        "        tokenized_label.append(vocab.word2index[token])\n",
        "\n",
        "    output = {'text': tokenized_text, 'mask': mask, 'label': tokenized_label}\n",
        "    \n",
        "    with open('test_samples/{}'.format(name), 'w') as text_labels_tokens_file:\n",
        "        json.dump(output, text_labels_tokens_file)\n",
        "    \n",
        "    name +=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000it [00:02, 445.20it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XhHJ6u01Tmo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f8f3ea2-7529-4424-ddc6-cb6886705847"
      },
      "source": [
        "dataset = NLPDataset('samples')\n",
        "test_dataset = NLPDataset('test_samples')\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=4)\n",
        "len(vocab.index2word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10573"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHuctw7zjM3I",
        "colab_type": "text"
      },
      "source": [
        "#### Adafactor optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTuRJhdsjLLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "#\n",
        "# This source code is licensed under the MIT license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "# https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.optim\n",
        "\n",
        "\n",
        "class Adafactor(torch.optim.Optimizer):\n",
        "    \"\"\"Implements Adafactor algorithm.\n",
        "    This implementation is based on:\n",
        "    `Adafactor: Adaptive Learning Rates with Sublinear Memory Cost`\n",
        "    (see https://arxiv.org/abs/1804.04235)\n",
        "    Note that this optimizer internally adjusts the learning rate\n",
        "    depending on the *scale_parameter*, *relative_step* and\n",
        "    *warmup_init* options. To use a manual (external) learning rate\n",
        "    schedule you should set `scale_parameter=False` and\n",
        "    `relative_step=False`.\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): external learning rate (default: None)\n",
        "        eps (tuple[float, float]): regularization constans for square gradient\n",
        "            and parameter scale respectively (default: (1e-30, 1e-3))\n",
        "        clip_threshold (float): threshold of root mean square of\n",
        "            final gradient update (default: 1.0)\n",
        "        decay_rate (float): coefficient used to compute running averages of square\n",
        "            gradient (default: -0.8)\n",
        "        beta1 (float): coefficient used for computing running averages of gradient\n",
        "            (default: None)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        scale_parameter (bool): if True, learning rate is scaled by root mean square of\n",
        "            parameter (default: True)\n",
        "        relative_step (bool): if True, time-dependent learning rate is computed\n",
        "            instead of external learning rate (default: True)\n",
        "        warmup_init (bool): time-dependent learning rate computation depends on\n",
        "            whether warm-up initialization is being used (default: False)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=None, eps=(1e-30, 1e-3), clip_threshold=1.0,\n",
        "                 decay_rate=-0.8, beta1=None, weight_decay=0.0, scale_parameter=True,\n",
        "                 relative_step=True, warmup_init=False):\n",
        "        if lr is not None and relative_step:\n",
        "            raise ValueError('Cannot combine manual lr and relative_step options')\n",
        "        if warmup_init and not relative_step:\n",
        "            raise ValueError('warmup_init requires relative_step=True')\n",
        "\n",
        "        defaults = dict(lr=lr, eps=eps, clip_threshold=clip_threshold, decay_rate=decay_rate,\n",
        "                        beta1=beta1, weight_decay=weight_decay, scale_parameter=scale_parameter,\n",
        "                        relative_step=relative_step, warmup_init=warmup_init)\n",
        "        super(Adafactor, self).__init__(params, defaults)\n",
        "\n",
        "    @property\n",
        "    def supports_memory_efficient_fp16(self):\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def supports_flat_params(self):\n",
        "        return False\n",
        "\n",
        "    def _get_lr(self, param_group, param_state):\n",
        "        rel_step_sz = param_group['lr']\n",
        "        if param_group['relative_step']:\n",
        "            min_step = 1e-6 * param_state['step'] if param_group['warmup_init'] else 1e-2\n",
        "            rel_step_sz = min(min_step, 1.0/math.sqrt(param_state['step']))\n",
        "        param_scale = 1.0\n",
        "        if param_group['scale_parameter']:\n",
        "            param_scale = max(param_group['eps'][1], param_state['RMS'])\n",
        "        return param_scale * rel_step_sz\n",
        "\n",
        "    def _get_options(self, param_group, param_shape):\n",
        "        factored = len(param_shape) >= 2\n",
        "        use_first_moment = param_group['beta1'] is not None\n",
        "        return factored, use_first_moment\n",
        "\n",
        "    def _rms(self, tensor):\n",
        "        return tensor.norm(2) / (tensor.numel() ** 0.5)\n",
        "\n",
        "    def _approx_sq_grad(self, exp_avg_sq_row, exp_avg_sq_col, output):\n",
        "        r_factor = (exp_avg_sq_row / exp_avg_sq_row.mean(dim=-1).unsqueeze(-1)).rsqrt_().unsqueeze(-1)\n",
        "        c_factor = exp_avg_sq_col.unsqueeze(-2).rsqrt()\n",
        "        torch.mul(r_factor, c_factor, out=output)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adafactor does not support sparse gradients.')\n",
        "\n",
        "                state = self.state[p]\n",
        "                grad_shape = grad.shape\n",
        "\n",
        "                factored, use_first_moment = self._get_options(group, grad_shape)\n",
        "                # State Initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "\n",
        "                    if use_first_moment:\n",
        "                        # Exponential moving average of gradient values\n",
        "                        state['exp_avg'] = torch.zeros_like(grad)\n",
        "                    if factored:\n",
        "                        state['exp_avg_sq_row'] = torch.zeros(grad_shape[:-1]).type_as(grad)\n",
        "                        state['exp_avg_sq_col'] = torch.zeros(grad_shape[:-2] + grad_shape[-1:]).type_as(grad)\n",
        "                    else:\n",
        "                        state['exp_avg_sq'] = torch.zeros_like(grad)\n",
        "\n",
        "                    state['RMS'] = 0\n",
        "                else:\n",
        "                    if use_first_moment:\n",
        "                        state['exp_avg'] = state['exp_avg'].type_as(grad)\n",
        "                    if factored:\n",
        "                        state['exp_avg_sq_row'] = state['exp_avg_sq_row'].type_as(grad)\n",
        "                        state['exp_avg_sq_col'] = state['exp_avg_sq_col'].type_as(grad)\n",
        "                    else:\n",
        "                        state['exp_avg_sq'] = state['exp_avg_sq'].type_as(grad)\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state['step'] += 1\n",
        "                state['RMS'] = self._rms(p_data_fp32)\n",
        "                group['lr'] = self._get_lr(group, state)\n",
        "\n",
        "                beta2t = 1.0 - math.pow(state['step'], group['decay_rate'])\n",
        "                update = (grad**2) + group['eps'][0]\n",
        "                if factored:\n",
        "                    exp_avg_sq_row = state['exp_avg_sq_row']\n",
        "                    exp_avg_sq_col = state['exp_avg_sq_col']\n",
        "\n",
        "                    # exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n",
        "                    # exp_avg_sq_col.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-2))\n",
        "                    exp_avg_sq_row.mul_(beta2t).add_(update.mean(dim=-1), alpha=1.0 - beta2t)\n",
        "                    exp_avg_sq_col.mul_(beta2t).add_(update.mean(dim=-2), alpha=1.0 - beta2t)\n",
        "\n",
        "                    # Approximation of exponential moving average of square of gradient\n",
        "                    self._approx_sq_grad(exp_avg_sq_row, exp_avg_sq_col, update)\n",
        "                    update.mul_(grad)\n",
        "                else:\n",
        "                    exp_avg_sq = state['exp_avg_sq']\n",
        "\n",
        "                    # exp_avg_sq.mul_(beta2t).add_(1.0 - beta2t, update)\n",
        "                    exp_avg_sq.mul_(beta2t).add_(update, alpha=1.0 - beta2t)\n",
        "                    torch.rsqrt(exp_avg_sq, out=update).mul_(grad)\n",
        "\n",
        "                update.div_(max(1.0, self._rms(update) / group['clip_threshold']))\n",
        "                update.mul_(group['lr'])\n",
        "\n",
        "                if use_first_moment:\n",
        "                    exp_avg = state['exp_avg']\n",
        "                    # exp_avg.mul_(group['beta1']).add_(1 - group['beta1'], update)\n",
        "                    exp_avg.mul_(group['beta1']).add_(update, alpha=1 - group['beta1'])\n",
        "                    update = exp_avg\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    # p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    p_data_fp32.add_(p_data_fp32, alpha=-group['weight_decay'] * group['lr'])\n",
        "\n",
        "                p_data_fp32.add_(-update)\n",
        "\n",
        "                # TODO: remove check once pyTorch avoids a copy for this case\n",
        "                if p.data_ptr() != p_data_fp32.data_ptr():\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruIxiPk_qtnW",
        "colab_type": "text"
      },
      "source": [
        "#### Pytorch Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLJVPlJJlzI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2vmZHdClzC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, src_vocab, trg_vocab, d_model=512, heads=8, nlayers=6, d_ff=1024, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.enc_embeder = nn.Embedding(src_vocab, d_model)\n",
        "        self.dec_embeder = nn.Embedding(trg_vocab, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        self.transformer = nn.Transformer(d_model, heads, nlayers, nlayers, d_ff, dropout)\n",
        "        self.output = nn.Linear(d_model, trg_vocab)\n",
        "\n",
        "    def forward(self, src, tgt, src_pad_mask=None, tgt_pad_mask=None, tgt_mask=None):\n",
        "\n",
        "        src = self.pos_encoder(self.enc_embeder(src))\n",
        "        tgt = self.pos_encoder(self.dec_embeder(tgt))\n",
        "        output = self.transformer(src, tgt, tgt_mask = tgt_mask,\n",
        "                        src_key_padding_mask = src_pad_mask,\n",
        "                        tgt_key_padding_mask = tgt_pad_mask,\n",
        "                        memory_key_padding_mask = src_pad_mask)\n",
        "        \n",
        "        output = self.output(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPxkIO5_ly87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_target_mask(targets):\n",
        "    size = targets.size(0)\n",
        "    nopeak_mask = np.triu(np.ones((size, size)), k=1)\n",
        "    nopeak_mask = torch.from_numpy(nopeak_mask)\n",
        "    nopeak_mask = Variable(nopeak_mask.masked_fill(nopeak_mask == 1, -np.inf))\n",
        "    return nopeak_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfvBGKzRly18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "PAD_INDEX = 0\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip=5):\n",
        "    \n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for _, batch in tqdm(enumerate(iterator)):\n",
        "\n",
        "        src = batch['text'].cuda()\n",
        "        trg = batch['label'].cuda()\n",
        "        src_pad_mask = src == PAD_INDEX\n",
        "        trg_pad_mask = trg == PAD_INDEX\n",
        "        src = src.transpose(0, 1)\n",
        "        trg = trg.transpose(0, 1)\n",
        "        TRG_MASK = make_target_mask(trg).cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg, src_pad_mask, trg_pad_mask, TRG_MASK)[:-1]\n",
        "        output = output.transpose(0, 1).contiguous().view(-1, output.size(-1))\n",
        "        trg = trg.transpose(0, 1)[:, 1:].flatten()\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0J6ipijbQEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL_MAX_LEN = 32\n",
        "PAD_INDEX = 0\n",
        "\n",
        "def index_2_token(x):\n",
        "    return vocab.index2word[x]\n",
        "\n",
        "back_to_tokens = np.vectorize(index_2_token)\n",
        "\n",
        "def validate_model(model, iterator, stop_token):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    trues = []\n",
        "    for _, batch in tqdm(enumerate(iterator)):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            src = batch['text'].cuda()\n",
        "            trg = batch['label'].cuda()\n",
        "            src_pad_mask = src == PAD_INDEX\n",
        "\n",
        "            src = src.transpose(0, 1)\n",
        "            trg = trg.transpose(0, 1)\n",
        "\n",
        "            trg_input = trg[0, :].view(1, -1)\n",
        "            for i in range(trg.shape[0] - 1):\n",
        "                trg_pad_mask = trg_input.transpose(0,1) == PAD_INDEX\n",
        "                out = model(src, trg_input, src_pad_mask, trg_pad_mask).argmax(dim=-1)[-1].view(1, trg.shape[1])\n",
        "                trg_input = torch.cat((trg_input, out), dim = 0)\n",
        "            \n",
        "            trg_input = trg_input.transpose(0, 1).cpu().detach().numpy()[:, 1:]\n",
        "            trg = trg.transpose(0, 1).cpu().numpy()[:, 1:]\n",
        "            \n",
        "            for j, sentence in enumerate(trg_input):\n",
        "                try:\n",
        "                    stop_index = np.min(np.where(sentence == stop_token)[0])\n",
        "                except:\n",
        "                    stop_index = LABEL_MAX_LEN\n",
        "\n",
        "                predict = ' '.join(back_to_tokens(sentence[:stop_index]))\n",
        "                predict = re.sub(r'\\n|\\r', '', predict) + '\\n'\n",
        "                predict = predict.split(' ')\n",
        "                predictions.append(predict)\n",
        "\n",
        "                stop_index = np.min(np.where(trg[j] == stop_token)[0])\n",
        "                true = back_to_tokens(trg[j][:stop_index])\n",
        "                true = true.tolist()\n",
        "                trues.append(true)\n",
        "    \n",
        "    return predictions, trues"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuOU6jqwo8aJ",
        "colab_type": "text"
      },
      "source": [
        "#### Pytorch transformer training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkB76V-imHT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = TransformerModel(10573, 10573, nlayers=2).cuda()\n",
        "optimizer = Adafactor(model.parameters(), beta1=0, warmup_init=False)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oZjuE63Bo95m",
        "colab": {}
      },
      "source": [
        "copyfile('/content/drive/My Drive/TransformerLSH/model_torch4.pt', '/content/model.pt')\n",
        "checkpoint = torch.load('/content/model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PfY3LIzDo95p",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uTOacH6VCYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#25 эпох прошло"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoZ3inPfmHOt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "48885c51-8944-47fc-866e-3389ac40003f"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "CLIP = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, dataloader, optimizer, criterion, CLIP)\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000it [24:08,  3.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 24m 9s\n",
            "\tTrain Loss: 3.473 | Train PPL:  32.221\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [24:09,  3.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Time: 24m 9s\n",
            "\tTrain Loss: 3.228 | Train PPL:  25.220\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [24:11,  3.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Time: 24m 11s\n",
            "\tTrain Loss: 3.006 | Train PPL:  20.206\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [24:18,  3.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Time: 24m 18s\n",
            "\tTrain Loss: 2.811 | Train PPL:  16.626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [24:15,  3.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Time: 24m 15s\n",
            "\tTrain Loss: 2.645 | Train PPL:  14.089\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wOXXE9XI2nQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "04e9ceef-67cc-4a87-e763-bc4cfc15eca1"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "CLIP = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, dataloader, optimizer, criterion, CLIP)\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000it [24:11,  3.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 24m 11s\n",
            "\tTrain Loss: 2.487 | Train PPL:  12.022\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [24:02,  3.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Time: 24m 3s\n",
            "\tTrain Loss: 2.358 | Train PPL:  10.567\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [24:01,  3.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Time: 24m 2s\n",
            "\tTrain Loss: 2.243 | Train PPL:   9.424\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [24:04,  3.46it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Time: 24m 4s\n",
            "\tTrain Loss: 2.139 | Train PPL:   8.489\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [24:09,  3.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Time: 24m 9s\n",
            "\tTrain Loss: 2.050 | Train PPL:   7.765\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zixeWmCL7tqU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "0a6e918f-5111-4fc6-d930-4e34c4efdb33"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "CLIP = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, dataloader, optimizer, criterion, CLIP)\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000it [24:07,  3.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 24m 7s\n",
            "\tTrain Loss: 1.966 | Train PPL:   7.143\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [24:06,  3.46it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Time: 24m 6s\n",
            "\tTrain Loss: 1.888 | Train PPL:   6.604\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [24:16,  3.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Time: 24m 16s\n",
            "\tTrain Loss: 1.815 | Train PPL:   6.139\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [24:18,  3.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Time: 24m 18s\n",
            "\tTrain Loss: 1.755 | Train PPL:   5.782\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [24:19,  3.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Time: 24m 19s\n",
            "\tTrain Loss: 1.701 | Train PPL:   5.481\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7RQlaQBrR-o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "6faa7fcc-316d-4391-d776-ecfd74ca7d4a"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "CLIP = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, dataloader, optimizer, criterion, CLIP)\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000it [24:36,  3.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 24m 36s\n",
            "\tTrain Loss: 1.648 | Train PPL:   5.196\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [24:40,  3.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Time: 24m 40s\n",
            "\tTrain Loss: 1.598 | Train PPL:   4.945\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [24:43,  3.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Time: 24m 43s\n",
            "\tTrain Loss: 1.556 | Train PPL:   4.738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [24:40,  3.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Time: 24m 40s\n",
            "\tTrain Loss: 1.514 | Train PPL:   4.544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [24:40,  3.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Time: 24m 40s\n",
            "\tTrain Loss: 1.479 | Train PPL:   4.388\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o81eW2iUAKBb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "f92a2b32-4a78-4b07-f24c-c8a31148327e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed May 20 16:33:16 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P0    30W /  70W |   6949MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tbAqDdR1NoXP",
        "colab": {}
      },
      "source": [
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': train_loss,\n",
        "            }, 'model_torch5.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gGjpmSNcNoXS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb460f43-897e-47a5-d34b-bcc0173d3be4"
      },
      "source": [
        "copyfile('/content/model_torch5.pt', '/content/drive/My Drive/TransformerLSH/model_torch5.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/TransformerLSH/model_torch5.pt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HcsdoKkNe4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WFmrDGQF9RF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd807f78-5910-4ee5-e7b5-547e86aef538"
      },
      "source": [
        "predictions, trues = validate_model(model, test_dataloader, stop_token=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "250it [12:58,  3.11s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgNFrR14BDV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BPE = pyonmttok.Tokenizer(\"conservative\", bpe_model_path=\"model-50k_with_joiner\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IDknSMsLmiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_detok = []\n",
        "for sentence in predictions:\n",
        "    predictions_detok.append(BPE.detokenize(sentence))\n",
        "\n",
        "trues_detok = []\n",
        "for sentence in trues:\n",
        "    trues_detok.append(BPE.detokenize(sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NPxKhzMLmf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import rouge\n",
        "rouge = rouge.Rouge()\n",
        "scores = rouge.get_scores(predictions_detok, trues_detok, avg=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psE_6s8v-0tZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "5656f932-88ce-4ebe-b1f1-ab97e290bd6e"
      },
      "source": [
        "scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge-1': {'f': 0.06887463710319201,\n",
              "  'p': 0.08638927363486204,\n",
              "  'r': 0.06282519425019459},\n",
              " 'rouge-2': {'f': 0.0033033848732950595,\n",
              "  'p': 0.004736183261183262,\n",
              "  'r': 0.0030555916305916304},\n",
              " 'rouge-l': {'f': 0.06847248789840453,\n",
              "  'p': 0.08663711843711856,\n",
              "  'r': 0.06160128760128791}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6NNQ3Czlprq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvCdz0EbeKx6",
        "colab_type": "text"
      },
      "source": [
        "#### FullLSHTransformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPrIoLZUgnB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n",
        "\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len = 80):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # create constant 'pe' matrix with values dependant on \n",
        "        # pos and i\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = np.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "                \n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        " \n",
        "    def forward(self, x):\n",
        "        # make embeddings relatively larger\n",
        "        x = x * np.sqrt(self.d_model)\n",
        "        #add constant to embedding\n",
        "        seq_len = x.size(1)\n",
        "        x = x + Variable(self.pe[:,:seq_len], requires_grad=False)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout = 0.1, activation = nn.ReLU()):\n",
        "        super().__init__() \n",
        "        # We set d_ff as a default to 2048\n",
        "        self.activation = activation\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(self.activation(self.linear_1(x)))\n",
        "        x = self.linear_2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps = 1e-6):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.size = d_model\n",
        "        # create two learnable parameters to calibrate normalisation\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
        "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm\n",
        "\n",
        "\n",
        "# build an encoder layer with one multi-head attention layer and one ff layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, heads, d_model, bucket_size,\n",
        "                 n_hashes, chunk_len, random_rotations_per_head = False,\n",
        "                 dropout = 0.1, d_ff = 2048, act_ff = nn.ReLU()):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.attn = SelfLSHAttention(heads, d_model, bucket_size,\n",
        "                                     n_hashes, chunk_len, False,\n",
        "                                     random_rotations_per_head, dropout)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout, act_ff)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask=None, exclude_self=True):\n",
        "        x_attn = self.dropout_1(self.attn(x, pad_mask=mask,\n",
        "                                          exclude_self=exclude_self)) + x\n",
        "        x_attn_norm = self.norm_1(x_attn)\n",
        "        x_ff = self.dropout_2(self.ff(x_attn_norm)) + x_attn_norm\n",
        "        x_ff_norm = self.norm_2(x_ff)\n",
        "        return x_ff_norm\n",
        "    \n",
        "# build a decoder layer with two multi-head attention layers and one ff layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, heads, d_model, trg_bucket_size, enc_dec_n_buckets,\n",
        "                 n_hashes, trg_chunk_len, src_chunk_len, random_rotations_per_head,\n",
        "                 dropout=0.1, d_ff=2048, act_ff = nn.ReLU()):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.norm_3 = Norm(d_model)\n",
        "        \n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "        \n",
        "        self.attn_1 = SelfLSHAttention(heads, d_model, trg_bucket_size,\n",
        "                                n_hashes, trg_chunk_len, True,\n",
        "                                random_rotations_per_head, dropout)\n",
        "        \n",
        "        self.attn_2 = EncDecLSHAttention(heads, d_model, enc_dec_n_buckets,\n",
        "                                         n_hashes, trg_chunk_len, src_chunk_len,\n",
        "                                         random_rotations_per_head, dropout)\n",
        "        \n",
        "        self.ff = FeedForward(d_model, d_ff, dropout, act_ff)\n",
        "\n",
        "    def forward(self, x, e_outputs, src_mask=None, trg_mask=None, exclude_self=True):\n",
        "        x_attn = self.dropout_1(self.attn_1(x, pad_mask=trg_mask,\n",
        "                                            exclude_self=exclude_self)) + x\n",
        "        x_attn_norm = self.norm_1(x_attn)\n",
        "        x_attn_enc_dec = x_attn_norm + self.dropout_2(self.attn_2(x_attn_norm,\n",
        "                                    e_outputs, trg_mask, src_mask))\n",
        "        \n",
        "        x_attn_enc_dec_norm = self.norm_2(x_attn_enc_dec)\n",
        "        x_ff = self.dropout_3(self.ff(x_attn_enc_dec_norm)) + x_attn_enc_dec_norm\n",
        "        x_ff_norm = self.norm_3(x_ff)        \n",
        "        return x_ff_norm\n",
        "    \n",
        "# We can then build a convenient cloning function that can generate multiple layers:\n",
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, max_src_len, d_model, N_layers, heads,\n",
        "                 bucket_size, n_hashes, chunk_len, random_rotations_per_head,\n",
        "                 dropout=0.1, d_ff=2048, act_ff=nn.ReLU()):\n",
        "        super().__init__()\n",
        "        self.N = N_layers\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model, max_src_len)\n",
        "        self.layers = get_clones(EncoderLayer(heads, d_model, bucket_size,\n",
        "                                    n_hashes, chunk_len, random_rotations_per_head,\n",
        "                                    dropout, d_ff, act_ff), N_layers)\n",
        "\n",
        "    def forward(self, src, mask=None, exclude_self=True):\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, mask, exclude_self)\n",
        "        return x\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, max_trg_len, d_model, N_layers, heads,\n",
        "                 trg_bucket_size, enc_dec_n_buckets, n_hashes, trg_chunk_len, src_chunk_len, random_rotations_per_head,\n",
        "                dropout=0.1, d_ff=2048, act_ff=nn.ReLU()):\n",
        "        super().__init__()\n",
        "        self.N = N_layers\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model, max_trg_len)\n",
        "        self.layers = get_clones(DecoderLayer(heads, d_model, trg_bucket_size,\n",
        "                                    enc_dec_n_buckets,\n",
        "                                    n_hashes, trg_chunk_len, src_chunk_len, random_rotations_per_head,\n",
        "                                    dropout, d_ff, act_ff), N_layers)\n",
        "\n",
        "    def forward(self, trg, e_outputs, src_mask=None, trg_mask=None, exclude_self=True):\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, e_outputs, src_mask, trg_mask, exclude_self)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerFullLSH(nn.Module):\n",
        "    def __init__(self, src_vocab, trg_vocab, src_bucket_size, trg_bucket_size,\n",
        "                 enc_dec_n_buckets, n_hashes, src_chunk_len, trg_chunk_len,\n",
        "                 max_src_len=3000, max_trg_len=100, d_model=512, N_layers=2,\n",
        "                 heads=8, random_rotations_per_head=False, dropout=0.1, \n",
        "                 d_ff=2048, act_ff=nn.ReLU()):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, max_src_len, d_model, N_layers, heads,\n",
        "                            src_bucket_size, n_hashes, src_chunk_len, random_rotations_per_head,\n",
        "                            dropout, d_ff, act_ff)\n",
        "        self.decoder = Decoder(trg_vocab, max_trg_len, d_model, N_layers, heads,\n",
        "                            trg_bucket_size, enc_dec_n_buckets, n_hashes, trg_chunk_len, src_chunk_len, random_rotations_per_head,\n",
        "                            dropout, d_ff, act_ff)\n",
        "        self.out = nn.Linear(d_model, trg_vocab)\n",
        "        \n",
        "    def forward(self, src, trg, src_mask=None, trg_mask=None, exclude_self=True):\n",
        "        e_outputs = self.encoder(src, src_mask, exclude_self)\n",
        "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask, exclude_self)\n",
        "        output = self.out(d_output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIT7yMwYeWPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mask_enc_dec_attention(dots, q_pad_mask_chanked=None, kv_pad_mask_chunked=None):\n",
        "    mask = (q_pad_mask_chanked[..., :, None] * kv_pad_mask_chunked[..., None, :]).bool()\n",
        "    # in the model notation 0 in pad_mask means token that must be masked to -1e9\n",
        "    dots.masked_fill_(~mask, -1e9)\n",
        "    \n",
        "    return dots\n",
        "\n",
        "\n",
        "class EncDecLSHAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 heads,\n",
        "                 d_model,\n",
        "                 n_buckets,\n",
        "                 n_hashes,\n",
        "                 q_chunk_len,\n",
        "                 k_chunk_len,\n",
        "                 random_rotations_per_head = False,\n",
        "                 dropout = 0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // heads\n",
        "        self.h = heads\n",
        "        self.n_buckets = n_buckets\n",
        "        self.n_hashes = n_hashes\n",
        "        self.q_chunk_len = q_chunk_len\n",
        "        self.k_chunk_len = k_chunk_len\n",
        "        self.random_rotations_per_head = random_rotations_per_head\n",
        "        \n",
        "        self.q_linear = nn.Linear(d_model, d_model)    \n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        if dropout is not None:\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "        else:\n",
        "            self.dropout = None\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        \n",
        "    def forward(self, x, e_outputs, q_pad_mask=None, kv_pad_mask=None):\n",
        "        bs = x.size(0)\n",
        "        \n",
        "        # perform linear operation and split into h heads\n",
        "        # bs, q_seq, heads, dk\n",
        "        q = self.q_linear(x).view(bs, -1, self.h, self.d_k)\n",
        "        # bs, kv_seq, heads, dk\n",
        "        k = self.q_linear(e_outputs).view(bs, -1, self.h, self.d_k)\n",
        "        v = self.v_linear(e_outputs).view(bs, -1, self.h, self.d_k)\n",
        "        # transpose to get dimensions bs, heads, seq, dk\n",
        "        q = q.transpose(1,2)\n",
        "        k = k.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "        # expand mask on number of hands\n",
        "        # bs, heads, seq\n",
        "        if (q_pad_mask is not None) and (kv_pad_mask is not None):\n",
        "            q_pad_mask = q_pad_mask.unsqueeze(1).expand(-1, self.h, -1)\n",
        "            kv_pad_mask = kv_pad_mask.unsqueeze(1).expand(-1, self.h, -1)\n",
        "        # calculate LSHattention\n",
        "        scores = self.enc_dec_lsh_attention(q, k, v, self.n_buckets, self.n_hashes,\n",
        "                                            self.q_chunk_len, self.k_chunk_len,\n",
        "                                            q_pad_mask, kv_pad_mask,\n",
        "                                            self.random_rotations_per_head)\n",
        "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model)        \n",
        "        output = self.out(concat)\n",
        "    \n",
        "        return output\n",
        "    \n",
        "     \n",
        "    def enc_dec_lsh_attention(self, q, k, v,\n",
        "                           n_buckets,\n",
        "                           n_hashes,\n",
        "                           q_chunk_len,\n",
        "                           k_chunk_len,\n",
        "                           q_pad_mask=None,\n",
        "                           kv_pad_mask=None,\n",
        "                           random_rotations_per_head=False):\n",
        "        # q shape: bs, heads, q_seqlen, dk\n",
        "        # k shape: bs, heads, kv_seqlen, dk\n",
        "        # v shape: bs, heads, kv_seqlen, dk\n",
        "        # n_buckets - number of buckets for q(trg) and kv(src)\n",
        "        batch_size, heads, q_seqlen, dk = q.shape\n",
        "        k_seqlen = k.size(-2)\n",
        "        device = q.device\n",
        "\n",
        "        assert q_seqlen % (n_buckets * 2) == 0\n",
        "        assert k_seqlen % (n_buckets * 2) == 0\n",
        "\n",
        "        # get buckets of shape bs, heads, seqlen*n_hashes\n",
        "        q_buckets, k_buckets = self.hash_vectors(n_buckets, n_hashes, q, k, random_rotations_per_head)\n",
        "        \n",
        "        q_ticker = torch.arange(n_hashes * q_seqlen, device=device).expand_as(q_buckets)\n",
        "        q_buckets_and_t = q_seqlen * q_buckets + (q_ticker % q_seqlen)\n",
        "        q_buckets_and_t = q_buckets_and_t.detach()\n",
        "              \n",
        "        k_ticker = torch.arange(n_hashes * k_seqlen, device=device).expand_as(k_buckets)\n",
        "        k_buckets_and_t = k_seqlen * k_buckets + (k_ticker % k_seqlen)\n",
        "        k_buckets_and_t = k_buckets_and_t.detach()\n",
        "\n",
        "        # hash-based sort (\"s\" at the start of variable names means \"sorted\")\n",
        "        sq_buckets_and_t, sq_ticker = q_buckets_and_t.sort(dim=-1)\n",
        "        _, q_undo_sort = sq_ticker.sort(dim=-1)\n",
        "\n",
        "        sq_buckets_and_t = sq_buckets_and_t.detach()\n",
        "        sq_ticker = sq_ticker.detach()\n",
        "        q_undo_sort = q_undo_sort.detach()\n",
        "          \n",
        "        sk_buckets_and_t, sk_ticker = k_buckets_and_t.sort(dim=-1)\n",
        "\n",
        "        sk_buckets_and_t = sk_buckets_and_t.detach()\n",
        "        sk_ticker = sk_ticker.detach()\n",
        "                        \n",
        "        # get vectors in the order of hash-based sort\n",
        "        sqt = sq_ticker % q_seqlen\n",
        "        skt = sk_ticker % k_seqlen\n",
        "        # sq shape: bs, heads, q_seqlen*n_hashes, dk\n",
        "        # sk and sv shape: bs, heads, k_seqlen*n_hashes, dk\n",
        "        sq = batched_index_select(q, sqt)\n",
        "        sk = batched_index_select(k, skt)\n",
        "        sv = batched_index_select(v, skt)\n",
        "        \n",
        "        if (q_pad_mask is not None) and (kv_pad_mask is not None):\n",
        "            q_pad_mask_chunked = q_pad_mask.gather(-1, sqt).view(batch_size, heads, -1, q_chunk_len)\n",
        "            kv_pad_mask_chunked = kv_pad_mask.gather(-1, skt).view(batch_size, heads, -1, k_chunk_len)\n",
        "        else:\n",
        "            q_pad_mask_chunked=None\n",
        "            kv_pad_mask_chunked=None\n",
        "\n",
        "        soutputs, sdots_logsumexp = self.attend(sq, sk, sv, q_chunk_len, k_chunk_len,\n",
        "                                                q_pad_mask_chunked, kv_pad_mask_chunked)\n",
        "        \n",
        "        # use undo_sort to get true order of sequence\n",
        "        outputs = batched_index_select(soutputs, q_undo_sort)\n",
        "        _, dots_logsumexp = sort_key_val(sq_ticker, sdots_logsumexp, dim=-1)\n",
        "\n",
        "        if n_hashes > 1:\n",
        "            outputs = torch.reshape(outputs, (batch_size, heads, n_hashes, q_seqlen, dk))\n",
        "            dots_logsumexp = torch.reshape(dots_logsumexp, (batch_size, heads, n_hashes, q_seqlen, 1))\n",
        "            probs = F.softmax(dots_logsumexp, dim=2)\n",
        "            outputs = torch.sum(outputs * probs, dim=2)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    \n",
        "    def hash_vectors(self, n_buckets, n_hashes, q, k, random_rotations_per_head=False):\n",
        "        # q vecs of shape bs, heads, q_seqlen, dk\n",
        "        # k vecs of shape bs, heads, kv_seqlen, dk\n",
        "        # n_buckets of q == n_buckets of k\n",
        "        assert n_buckets % 2 == 0\n",
        "\n",
        "        batch_size = q.size(0)\n",
        "        heads = q.size(1)\n",
        "        device = q.device\n",
        "\n",
        "        rotations_shape = (\n",
        "            batch_size,\n",
        "            heads if random_rotations_per_head else 1,\n",
        "            q.size(-1),\n",
        "            n_hashes,\n",
        "            n_buckets // 2)\n",
        "\n",
        "        random_rotations = torch.randn(rotations_shape, dtype=q.dtype,\n",
        "                                      device=device).expand(batch_size, heads, -1, -1, -1)\n",
        "        # rotated_q size: bs(b), heads(h), n_hashes(r), q_seqlen(s), buckets//2 (i)\n",
        "        rotated_q = torch.einsum('bhsd,bhdri->bhrsi', q, random_rotations)\n",
        "        rotated_q = torch.cat([rotated_q, -rotated_q], dim=-1)\n",
        "        q_buckets = torch.argmax(rotated_q, dim=-1)\n",
        "        \n",
        "        # rotated_k size: bs(b), heads(h), n_hashes(r), k_seqlen(s), buckets//2 (i)\n",
        "        rotated_k = torch.einsum('bhsd,bhdri->bhrsi', k, random_rotations)\n",
        "        rotated_k = torch.cat([rotated_k, -rotated_k], dim=-1)\n",
        "        k_buckets = torch.argmax(rotated_k, dim=-1)\n",
        "        \n",
        "        # buckets is now (bs, heads, n_hashes, seqlen). Next we add offsets so that\n",
        "        # bucket numbers from different hashing rounds don't overlap.        \n",
        "        offsets = torch.arange(n_hashes, device=device)\n",
        "        offsets = torch.reshape(offsets * n_buckets, (1, 1, -1, 1))\n",
        "        q_buckets = torch.reshape(q_buckets + offsets, (batch_size, heads, -1,))\n",
        "        k_buckets = torch.reshape(k_buckets + offsets, (batch_size, heads, -1,))\n",
        "        # out shape of each tensor: bs, heads, seqlen*n_hashes\n",
        "        return q_buckets, k_buckets\n",
        "     \n",
        "        \n",
        "    def attend(self, q, k, v, q_chunk_len, k_chunk_len,\n",
        "               q_pad_mask_chunked=None,\n",
        "               kv_pad_mask_chunked=None):\n",
        "        \n",
        "        batch_size, heads, _, dk = q.size()\n",
        "            \n",
        "        # q, k, v shapes: bs, heads, n_hashes*seqlen/chunk_len, chunk_len, dk\n",
        "        q = torch.reshape(q, (batch_size, heads, -1, q_chunk_len, dk))\n",
        "        k = torch.reshape(k, (batch_size, heads, -1, k_chunk_len, dk))\n",
        "        v = torch.reshape(v, (batch_size, heads, -1, k_chunk_len, dk))\n",
        "        \n",
        "        k = length_normalized(k)\n",
        "        k = k / (dk**0.5)\n",
        "        \n",
        "        # dots shape: bs, heads, n_hashes*seqlen/chunk_len, chunk_len, chunk_len * 2\n",
        "        dots = torch.einsum('bhcsd,bhcfd->bhcsf', q, k)\n",
        "        if (q_pad_mask_chunked is not None) and (kv_pad_mask_chunked is not None):\n",
        "            dots = mask_enc_dec_attention(dots, q_pad_mask_chunked, kv_pad_mask_chunked)\n",
        "\n",
        "        # softmax\n",
        "        dots_logsumexp = torch.logsumexp(dots, dim=-1, keepdim=True)\n",
        "        dots = torch.exp(dots - dots_logsumexp)\n",
        "        \n",
        "        if self.dropout is not None:\n",
        "            dots = self.dropout(dots)\n",
        "        \n",
        "        out = torch.matmul(dots, v)\n",
        "        out = torch.reshape(out, (batch_size, heads, -1, dk))\n",
        "        dots_logsumexp = torch.reshape(dots_logsumexp, (batch_size, heads, -1))\n",
        "        return out, dots_logsumexp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb2XGjexeWHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def length_normalized(x, epsilon=1e-6):\n",
        "    variance = torch.mean(x**2, dim=-1, keepdim=True)\n",
        "    norm_inputs = x / torch.sqrt(variance + epsilon)\n",
        "    return norm_inputs\n",
        "\n",
        "\n",
        "def look_one_back(x):\n",
        "    # Allow each chunk to attend within itself, and also one chunk back. Chunk\n",
        "    # boundaries might occur in the middle of a sequence of items from the\n",
        "    # same bucket, so this increases the chances of attending to relevant items.\n",
        "    x_extra = torch.cat([x[:, :, -1:, ...], x[:, :, :-1, ...]], dim=2)\n",
        "    return torch.cat([x, x_extra], dim=3)\n",
        "\n",
        "\n",
        "def batched_index_select(values, indices):\n",
        "    last_dim = values.size(-1)\n",
        "    return values.gather(2, indices[:, :, :, None].expand(-1, -1, -1, last_dim))\n",
        "\n",
        "\n",
        "def sort_key_val(t1, t2, dim=-1):\n",
        "    values, indices = t1.sort(dim=dim)\n",
        "    #t2 = t2.expand_as(t1)\n",
        "    return values, t2.gather(dim, indices)\n",
        "\n",
        "\n",
        "def mask_self_attention(dots, q_info, kv_info, pad_mask_chanked=None, is_decoder=False, exclude_self=False):\n",
        "    # mask: 1 - token must be masked to -1e9, 0 - otherwise\n",
        "    if is_decoder:\n",
        "        mask = torch.lt(q_info[..., :, None], kv_info[..., None, :])\n",
        "        dots.masked_fill_(mask, -1e9)\n",
        "    if exclude_self:\n",
        "        # we don't mask the token with the least index in the chunk\n",
        "        le = torch.le(q_info[...,:, None], kv_info[..., None, :])\n",
        "        not_min_mask = ~((kv_info.size(-1) - le.sum(dim=-1, keepdim=True)) == 0).expand_as(le)\n",
        "        # equality_mask for to mask self token if there some other tokens in sequence, that are unmasked\n",
        "        equality_mask = torch.eq(q_info[..., :, None], kv_info[..., None, :])\n",
        "        # combine two masks\n",
        "        mask = not_min_mask & equality_mask\n",
        "        dots.masked_fill_(mask, -1e9)\n",
        "    \n",
        "    if pad_mask_chanked is not None:\n",
        "        kv_pad_mask = look_one_back(pad_mask_chanked)\n",
        "        mask = (pad_mask_chanked[..., :, None] * kv_pad_mask[..., None, :]).bool()\n",
        "        # in the model notation 0 in pad_mask means token that must be masked to -1e9\n",
        "        dots.masked_fill_(~mask, -1e9)\n",
        "    \n",
        "    return dots\n",
        "\n",
        "\n",
        "class SelfLSHAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 heads,\n",
        "                 d_model,\n",
        "                 bucket_size,\n",
        "                 n_hashes,\n",
        "                 chunk_len,\n",
        "                 is_decoder = False,\n",
        "                 random_rotations_per_head = False,\n",
        "                 dropout = 0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // heads\n",
        "        self.h = heads\n",
        "        self.bucket_size = bucket_size\n",
        "        self.n_hashes = n_hashes\n",
        "        self.chunk_len = chunk_len\n",
        "        self.is_decoder = is_decoder\n",
        "        self.random_rotations_per_head = random_rotations_per_head\n",
        "        \n",
        "        self.q_linear = nn.Linear(d_model, d_model)    \n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        if dropout is not None:\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "        else:\n",
        "            self.dropout = None\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        \n",
        "    def forward(self, x, pad_mask=None, exclude_self=False):\n",
        "        bs = x.size(0)\n",
        "        \n",
        "        #qk_mask = make_shared_qk_mask(q.size(1)) # 1, 1, seq_len, seq_len\n",
        "        # perform linear operation and split into h heads\n",
        "        # bs, seq, heads, dk\n",
        "        q = self.q_linear(x).view(bs, -1, self.h, self.d_k)\n",
        "        v = self.v_linear(x).view(bs, -1, self.h, self.d_k)\n",
        "        # transpose to get dimensions bs, heads, seq, dk\n",
        "        q = q.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "        # expand mask on number of hands\n",
        "        # bs, heads, seq\n",
        "        if pad_mask is not None:\n",
        "            pad_mask = pad_mask.unsqueeze(1).expand(-1, self.h, -1)\n",
        "        # calculate LSHattention\n",
        "        scores = self.self_lsh_attention(q, v, self.bucket_size, self.n_hashes,\n",
        "                                         self.chunk_len, pad_mask,\n",
        "                                         self.random_rotations_per_head,\n",
        "                                         self.is_decoder, exclude_self)\n",
        "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model)        \n",
        "        output = self.out(concat)\n",
        "    \n",
        "        return output\n",
        "    \n",
        "     \n",
        "    def self_lsh_attention(self, qk, v,\n",
        "                           bucket_size,\n",
        "                           n_hashes,\n",
        "                           chunk_len,\n",
        "                           pad_mask=None,\n",
        "                           random_rotations_per_head=False,\n",
        "                           is_decoder=False,\n",
        "                           exclude_self=False):\n",
        "        # qk shape: bs, heads, qk_seqlen, dk\n",
        "        # v shape: bs, heads, v_seqlen (same as qk_seqlen), dk\n",
        "        # bucket size - mean number of vectors in one bucket\n",
        "        batch_size, heads, seqlen, dk = qk.shape\n",
        "        device = qk.device\n",
        "\n",
        "        assert seqlen % (bucket_size * 2) == 0\n",
        "\n",
        "        n_buckets = seqlen // bucket_size\n",
        "        # get buckets of shape bs, heads, seqlen*n_hashes\n",
        "        buckets = self.hash_vectors(n_buckets, n_hashes, qk, random_rotations_per_head)\n",
        "\n",
        "        ticker = torch.arange(n_hashes * seqlen, device=qk.device).expand_as(buckets)\n",
        "        buckets_and_t = seqlen * buckets + (ticker % seqlen)\n",
        "        buckets_and_t = buckets_and_t.detach()\n",
        "\n",
        "        # hash-based sort (\"s\" at the start of variable names means \"sorted\")\n",
        "        # была сортировка через ф-цию sort_key_val\n",
        "        sbuckets_and_t, sticker = buckets_and_t.sort(dim=-1)\n",
        "        _, undo_sort = sticker.sort(dim=-1)\n",
        "\n",
        "        sbuckets_and_t = sbuckets_and_t.detach()\n",
        "        sticker = sticker.detach()\n",
        "        undo_sort = undo_sort.detach()\n",
        "        # get vectors in the order of hash-based sort\n",
        "        st = sticker % seqlen\n",
        "        # sqk shape: bs, heads, seqlen*n_hashes, dk\n",
        "        sqk = batched_index_select(qk, st)\n",
        "        sv = batched_index_select(v, st)\n",
        "\n",
        "        soutputs, sdots_logsumexp = self.attend(sqk, sv, chunk_len, st, pad_mask, is_decoder, exclude_self)\n",
        "        # use undo_sort to get true order of sequence\n",
        "        outputs = batched_index_select(soutputs, undo_sort)\n",
        "        _, dots_logsumexp = sort_key_val(sticker, sdots_logsumexp, dim=-1)\n",
        "\n",
        "        if n_hashes > 1:\n",
        "            outputs = torch.reshape(outputs, (batch_size, heads, n_hashes, seqlen, dk))\n",
        "            dots_logsumexp = torch.reshape(dots_logsumexp, (batch_size, heads, n_hashes, seqlen, 1))\n",
        "            probs = F.softmax(dots_logsumexp, dim=2)\n",
        "            outputs = torch.sum(outputs * probs, dim=2)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    \n",
        "    def hash_vectors(self, n_buckets, n_hashes, vecs, random_rotations_per_head=False):\n",
        "        # input vecs of shape bs, heads, seqlen, dk\n",
        "        # n_buckets = seqlen // bucketsize\n",
        "        assert n_buckets % 2 == 0\n",
        "\n",
        "        batch_size = vecs.size(0)\n",
        "        heads = vecs.size(1)\n",
        "        device = vecs.device\n",
        "\n",
        "        rotations_shape = (\n",
        "            batch_size,\n",
        "            heads if random_rotations_per_head else 1,\n",
        "            vecs.size(-1),\n",
        "            n_hashes,\n",
        "            n_buckets // 2)\n",
        "\n",
        "        random_rotations = torch.randn(rotations_shape, dtype=vecs.dtype, device=device).expand(batch_size, heads, -1, -1, -1)\n",
        "        # rotated_vecs size: bs(b), heads(h), n_hashes(r), seqlen(s), buckets//2 (i)\n",
        "        rotated_vecs = torch.einsum('bhsd,bhdri->bhrsi', vecs, random_rotations)\n",
        "\n",
        "        rotated_vecs = torch.cat([rotated_vecs, -rotated_vecs], dim=-1)\n",
        "        buckets = torch.argmax(rotated_vecs, dim=-1)\n",
        "\n",
        "        # buckets is now (bs, heads, n_hashes, seqlen). Next we add offsets so that\n",
        "        # bucket numbers from different hashing rounds don't overlap.        \n",
        "        offsets = torch.arange(n_hashes, device=device)\n",
        "        offsets = torch.reshape(offsets * n_buckets, (1, 1, -1, 1))\n",
        "        buckets = torch.reshape(buckets + offsets, (batch_size, heads, -1,))\n",
        "        # out shape bs, heads, seqlen*n_hashes\n",
        "        return buckets\n",
        "     \n",
        "\n",
        "    def attend(self, q, v, chunk_len, info, pad_mask=None, is_decoder=False, exclude_self=False):\n",
        "        # attend for qk sharing\n",
        "        batch_size, heads, _, dk = q.size()\n",
        "        \n",
        "        if pad_mask is not None:\n",
        "            pad_mask_chunked = pad_mask.gather(-1, info).view(batch_size, heads, -1, chunk_len)\n",
        "        else:\n",
        "            pad_mask_chunked=None\n",
        "            \n",
        "        info = torch.reshape(info, (batch_size, heads, -1, chunk_len))\n",
        "        # q, k, v shapes: bs, heads, n_hashes*seqlen/chunk_len, chunk_len, dk\n",
        "        q = torch.reshape(q, (batch_size, heads, -1, chunk_len, dk))\n",
        "        v = torch.reshape(v, (batch_size, heads, -1, chunk_len, dk))\n",
        "        k = q.clone()    \n",
        "        k = length_normalized(k)\n",
        "        k = k / (dk**0.5)\n",
        "\n",
        "        # form chunks\n",
        "        # k and v shape: bs, heads, n_hashes*seqlen/chunk_len, chunk_len * 2, dk\n",
        "        k = look_one_back(k)\n",
        "        v = look_one_back(v)\n",
        "        kv_info = look_one_back(info)\n",
        "\n",
        "        # dots shape: bs, heads, n_hashes*seqlen/chunk_len, chunk_len, chunk_len * 2\n",
        "        dots = torch.einsum('bhcsd,bhcfd->bhcsf', q, k)\n",
        "        dots = mask_self_attention(dots, info, kv_info,\n",
        "                                   pad_mask_chunked,\n",
        "                                   is_decoder,\n",
        "                                   exclude_self)\n",
        "\n",
        "        # softmax\n",
        "        dots_logsumexp = torch.logsumexp(dots, dim=-1, keepdim=True)\n",
        "        dots = torch.exp(dots - dots_logsumexp)\n",
        "        \n",
        "        if self.dropout is not None:\n",
        "            dots = self.dropout(dots)\n",
        "        \n",
        "        out = torch.matmul(dots, v)\n",
        "        out = torch.reshape(out, (batch_size, heads, -1, dk))\n",
        "        dots_logsumexp = torch.reshape(dots_logsumexp, (batch_size, heads, -1))\n",
        "        return out, dots_logsumexp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhcDOckZZ7hw",
        "colab_type": "text"
      },
      "source": [
        "#### FullLSHTransformer training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1NR_vXZh1xx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import time\n",
        "PAD_INDEX = 0\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip=5):\n",
        "    \n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for _, batch in tqdm(enumerate(iterator)):\n",
        "\n",
        "        src = batch['text'].cuda()\n",
        "        trg = batch['label'].cuda()\n",
        "\n",
        "        src_pad_mask = src != PAD_INDEX\n",
        "        trg_pad_mask = trg != PAD_INDEX\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg, src_pad_mask, trg_pad_mask)[:, :-1]\n",
        "        output = output.contiguous().view(-1, output.size(-1))\n",
        "        trg = trg[:, 1:].flatten()\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-oIs9Qsh1po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL_MAX_LEN = 32\n",
        "PAD_INDEX = 0\n",
        "\n",
        "def index_2_token(x):\n",
        "    return vocab.index2word[x]\n",
        "\n",
        "back_to_tokens = np.vectorize(index_2_token)\n",
        "\n",
        "def validate_model(model, iterator, stop_token):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    trues = []\n",
        "    for _, batch in tqdm(enumerate(iterator)):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            src = batch['text'].cuda()\n",
        "            trg = batch['label'].cuda()\n",
        "            src_pad_mask = src != PAD_INDEX\n",
        "            device = src.device\n",
        "\n",
        "            trg_input = trg[:, 0].view(-1, 1)\n",
        "            for i in range(trg.shape[1] - 1):\n",
        "                pad_input = torch.zeros((trg.shape[0], trg.shape[1] - trg_input.shape[1])).long().to(device)\n",
        "                inputs = torch.cat((trg_input, pad_input), dim = 1)\n",
        "                trg_pad_mask = inputs != PAD_INDEX\n",
        "                out = model(src, inputs, src_pad_mask, trg_pad_mask).argmax(dim=-1)[:, i].view(trg.shape[0], 1)\n",
        "                trg_input = torch.cat((trg_input, out), dim = 1)\n",
        "            \n",
        "            trg_input = trg_input.cpu().detach().numpy()[:, 1:]\n",
        "            trg = trg.cpu().numpy()[:, 1:]\n",
        "            \n",
        "            for j, sentence in enumerate(trg_input):\n",
        "                try:\n",
        "                    stop_index = np.min(np.where(sentence == stop_token)[0])\n",
        "                except:\n",
        "                    stop_index = LABEL_MAX_LEN\n",
        "\n",
        "                predict = ' '.join(back_to_tokens(sentence[:stop_index]))\n",
        "                predict = re.sub(r'\\n|\\r', '', predict) + '\\n'\n",
        "                predict = predict.split(' ')\n",
        "                predictions.append(predict)\n",
        "\n",
        "                stop_index = np.min(np.where(trg[j] == stop_token)[0])\n",
        "                true = back_to_tokens(trg[j][:stop_index])\n",
        "                true = true.tolist()\n",
        "                trues.append(true)\n",
        "    \n",
        "    return predictions, trues"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP7HSGiukfKZ",
        "colab_type": "text"
      },
      "source": [
        "##### training for article bucket size = 512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bcp5uEN3aC42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FullLSHmodel = TransformerFullLSH(10573, 10573, src_bucket_size=512, trg_bucket_size=8, enc_dec_n_buckets=4, n_hashes=2,\n",
        "                              src_chunk_len=512, trg_chunk_len=8, max_src_len=2048, max_trg_len=32, d_model=512,\n",
        "                              N_layers=2, heads=8, d_ff=1024).cuda()\n",
        "optimizer = Adafactor(FullLSHmodel.parameters(), beta1=0, warmup_init=False)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x03zk8N7VltQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "copyfile('/content/drive/My Drive/TransformerLSH/model4.pt', '/content/model.pt')\n",
        "checkpoint = torch.load('/content/model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hky569aCVlmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FullLSHmodel.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUfjUf2TVljr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 25 эпох прошло"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfY6TgObaR8z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "88727868-a45a-4347-939a-890c9bb67c45"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "CLIP = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(FullLSHmodel, dataloader, optimizer, criterion, CLIP)\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000it [28:54,  2.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 28m 55s\n",
            "\tTrain Loss: 2.577 | Train PPL:  13.161\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [28:54,  2.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Time: 28m 54s\n",
            "\tTrain Loss: 2.399 | Train PPL:  11.011\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [28:56,  2.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Time: 28m 56s\n",
            "\tTrain Loss: 2.233 | Train PPL:   9.327\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [28:54,  2.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Time: 28m 54s\n",
            "\tTrain Loss: 2.089 | Train PPL:   8.081\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [29:04,  2.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Time: 29m 4s\n",
            "\tTrain Loss: 1.965 | Train PPL:   7.133\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM5SSQG6DjH1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "8f136a9a-d67c-4319-ba26-ea1da3a36bfe"
      },
      "source": [
        "N_EPOCHS = 3\n",
        "CLIP = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(FullLSHmodel, dataloader, optimizer, criterion, CLIP)\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000it [29:15,  2.85it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 29m 15s\n",
            "\tTrain Loss: 1.851 | Train PPL:   6.367\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [29:10,  2.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Time: 29m 11s\n",
            "\tTrain Loss: 1.754 | Train PPL:   5.780\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [29:12,  2.85it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Time: 29m 12s\n",
            "\tTrain Loss: 1.670 | Train PPL:   5.311\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxi_dDYwdZtW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "7804353e-d428-4bdf-a7ea-06b99edb6893"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "CLIP = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(FullLSHmodel, dataloader, optimizer, criterion, CLIP)\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000it [26:46,  3.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 26m 46s\n",
            "\tTrain Loss: 1.590 | Train PPL:   4.902\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [26:46,  3.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Time: 26m 46s\n",
            "\tTrain Loss: 1.522 | Train PPL:   4.581\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [26:46,  3.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Time: 26m 46s\n",
            "\tTrain Loss: 1.461 | Train PPL:   4.312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [26:48,  3.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Time: 26m 49s\n",
            "\tTrain Loss: 1.406 | Train PPL:   4.080\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [26:49,  3.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Time: 26m 49s\n",
            "\tTrain Loss: 1.354 | Train PPL:   3.874\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHYsx2xM8igE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "f27446c2-dc4b-459c-c893-d51629f3c131"
      },
      "source": [
        "N_EPOCHS = 4\n",
        "CLIP = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(FullLSHmodel, dataloader, optimizer, criterion, CLIP)\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000it [26:47,  3.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 26m 47s\n",
            "\tTrain Loss: 1.308 | Train PPL:   3.701\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [26:49,  3.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Time: 26m 49s\n",
            "\tTrain Loss: 1.264 | Train PPL:   3.538\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [26:50,  3.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Time: 26m 50s\n",
            "\tTrain Loss: 1.221 | Train PPL:   3.389\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [26:49,  3.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Time: 26m 50s\n",
            "\tTrain Loss: 1.188 | Train PPL:   3.280\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRyjmgjoaCwN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "bfbf4e44-d72b-4a88-a5fb-8669c51c13b4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue May 19 08:00:07 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P0    34W / 250W |   7547MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t01YB1YXYhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': FullLSHmodel.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': train_loss,\n",
        "            }, 'model6.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "314hGm0NXeVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6be1a9a9-aa39-4cbe-9d04-e14bb830b0a7"
      },
      "source": [
        "copyfile('/content/model6.pt', '/content/drive/My Drive/TransformerLSH/model6.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/TransformerLSH/model6.pt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z42LDa4bR_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0A2bPkogkgz9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "918f4958-f372-4563-8c7b-506209e849ef"
      },
      "source": [
        "predictions, trues = validate_model(FullLSHmodel, test_dataloader, stop_token=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "250it [13:13,  3.18s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vxdU7kRKkg0C",
        "colab": {}
      },
      "source": [
        "BPE = pyonmttok.Tokenizer(\"conservative\", bpe_model_path=\"model-50k_with_joiner\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BG0k9kmfkg0E",
        "colab": {}
      },
      "source": [
        "predictions_detok = []\n",
        "for sentence in predictions:\n",
        "    predictions_detok.append(BPE.detokenize(sentence))\n",
        "\n",
        "trues_detok = []\n",
        "for sentence in trues:\n",
        "    trues_detok.append(BPE.detokenize(sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O4JU66Ydkg0G",
        "colab": {}
      },
      "source": [
        "import rouge\n",
        "rouge = rouge.Rouge()\n",
        "scores = rouge.get_scores(predictions_detok, trues_detok, avg=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8KwEIOdskg0J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "24d63f44-8d2c-4206-f883-d7575eb9845c"
      },
      "source": [
        "scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge-1': {'f': 0.07717138136059545,\n",
              "  'p': 0.08127906203273878,\n",
              "  'r': 0.07639163059163077},\n",
              " 'rouge-2': {'f': 0.01357722028002533,\n",
              "  'p': 0.013940523365523356,\n",
              "  'r': 0.013551151626151618},\n",
              " 'rouge-l': {'f': 0.07469833506246332,\n",
              "  'p': 0.07879135586635615,\n",
              "  'r': 0.07368989066489091}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvku2D0EZ6aE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAMfPmhO34ZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIE21NRE35V-",
        "colab_type": "text"
      },
      "source": [
        "##### training for article bucket size = 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lpeOHu4TtYNL",
        "colab": {}
      },
      "source": [
        "copyfile('/content/drive/My Drive/TransformerLSH/LSH2.pt', '/content/model.pt')\n",
        "checkpoint = torch.load('/content/model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsBU53SS34XJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FullLSHmodel = TransformerFullLSH(10573, 10573, src_bucket_size=128, trg_bucket_size=2, enc_dec_n_buckets=16, n_hashes=2,\n",
        "                              src_chunk_len=128, trg_chunk_len=2, max_src_len=2048, max_trg_len=32, d_model=512,\n",
        "                              N_layers=2, heads=8, d_ff=1024).cuda()\n",
        "optimizer = Adafactor(FullLSHmodel.parameters(), beta1=0, warmup_init=False)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnlR3EvLtuU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FullLSHmodel.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25ebWSvO34Uk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "999df27c-3bf3-4a30-fe1b-5f475084bc30"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "CLIP = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(FullLSHmodel, dataloader, optimizer, criterion, CLIP)\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000it [44:15,  1.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 44m 15s\n",
            "\tTrain Loss: 7.125 | Train PPL: 1243.114\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [44:13,  1.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Time: 44m 13s\n",
            "\tTrain Loss: 5.302 | Train PPL: 200.735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [44:10,  1.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Time: 44m 10s\n",
            "\tTrain Loss: 4.637 | Train PPL: 103.195\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [43:43,  1.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Time: 43m 43s\n",
            "\tTrain Loss: 4.136 | Train PPL:  62.545\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [43:41,  1.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Time: 43m 41s\n",
            "\tTrain Loss: 3.755 | Train PPL:  42.734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMFROBEDugJk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "e6aaf355-86ef-42c5-bb8d-d74a9d891ff2"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(FullLSHmodel, dataloader, optimizer, criterion, CLIP)\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000it [15:03,  5.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 15m 3s\n",
            "\tTrain Loss: 3.447 | Train PPL:  31.403\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [15:03,  5.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Time: 15m 4s\n",
            "\tTrain Loss: 3.186 | Train PPL:  24.191\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [15:04,  5.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Time: 15m 4s\n",
            "\tTrain Loss: 2.972 | Train PPL:  19.534\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [15:07,  5.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Time: 15m 7s\n",
            "\tTrain Loss: 2.786 | Train PPL:  16.217\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [15:17,  5.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Time: 15m 17s\n",
            "\tTrain Loss: 2.622 | Train PPL:  13.768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [15:17,  5.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 06 | Time: 15m 17s\n",
            "\tTrain Loss: 2.478 | Train PPL:  11.915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [15:22,  5.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 07 | Time: 15m 22s\n",
            "\tTrain Loss: 2.338 | Train PPL:  10.358\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [15:18,  5.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 08 | Time: 15m 18s\n",
            "\tTrain Loss: 2.213 | Train PPL:   9.141\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [15:16,  5.46it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 09 | Time: 15m 16s\n",
            "\tTrain Loss: 2.120 | Train PPL:   8.330\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [15:16,  5.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10 | Time: 15m 17s\n",
            "\tTrain Loss: 2.040 | Train PPL:   7.687\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iirmWFZ7xWC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "5186c345-8585-460c-fc91-d7dd45d0938b"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(FullLSHmodel, dataloader, optimizer, criterion, CLIP)\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000it [15:13,  5.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 15m 13s\n",
            "\tTrain Loss: 1.952 | Train PPL:   7.041\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [15:13,  5.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Time: 15m 13s\n",
            "\tTrain Loss: 1.874 | Train PPL:   6.518\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [15:11,  5.49it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Time: 15m 11s\n",
            "\tTrain Loss: 1.799 | Train PPL:   6.044\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [15:10,  5.49it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Time: 15m 10s\n",
            "\tTrain Loss: 1.746 | Train PPL:   5.731\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [15:06,  5.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Time: 15m 6s\n",
            "\tTrain Loss: 1.698 | Train PPL:   5.463\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [15:05,  5.52it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 06 | Time: 15m 5s\n",
            "\tTrain Loss: 1.657 | Train PPL:   5.242\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [15:05,  5.52it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 07 | Time: 15m 5s\n",
            "\tTrain Loss: 1.606 | Train PPL:   4.983\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [15:08,  5.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 08 | Time: 15m 8s\n",
            "\tTrain Loss: 1.567 | Train PPL:   4.793\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [15:07,  5.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 09 | Time: 15m 7s\n",
            "\tTrain Loss: 1.530 | Train PPL:   4.620\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5000it [15:07,  5.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10 | Time: 15m 7s\n",
            "\tTrain Loss: 1.490 | Train PPL:   4.437\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEM88Uu-7xTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': FullLSHmodel.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': train_loss,\n",
        "            }, 'LSH2_3.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-z4hUYb7xPx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3a0410b-fc65-4534-e784-e39a0922d345"
      },
      "source": [
        "copyfile('/content/LSH2_3.pt', '/content/drive/My Drive/TransformerLSH/LSH2_3.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/TransformerLSH/LSH2_3.pt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR8EhdCwSJ1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xC2wJwW5U3OQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e42a5f57-c0a6-45ca-d8e3-f44118400b1d"
      },
      "source": [
        "predictions, trues = validate_model(FullLSHmodel, test_dataloader, stop_token=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "250it [06:34,  1.58s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_b2Wi5lmU3OW",
        "colab": {}
      },
      "source": [
        "BPE = pyonmttok.Tokenizer(\"conservative\", bpe_model_path=\"model-50k_with_joiner\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LtgEr0QEU3OY",
        "colab": {}
      },
      "source": [
        "predictions_detok = []\n",
        "for sentence in predictions:\n",
        "    predictions_detok.append(BPE.detokenize(sentence))\n",
        "\n",
        "trues_detok = []\n",
        "for sentence in trues:\n",
        "    trues_detok.append(BPE.detokenize(sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JiOvDYDKU3Oa",
        "colab": {}
      },
      "source": [
        "import rouge\n",
        "rouge = rouge.Rouge()\n",
        "scores = rouge.get_scores(predictions_detok, trues_detok, avg=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S135Ku6qU3Od",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "e186e0e8-3ff0-4d45-8d96-f6dfca3e261b"
      },
      "source": [
        "scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge-1': {'f': 0.061742156144260124,\n",
              "  'p': 0.06666360989801418,\n",
              "  'r': 0.0602998085248088},\n",
              " 'rouge-2': {'f': 0.006710968747283967,\n",
              "  'p': 0.007141341991341988,\n",
              "  'r': 0.006568398268398267},\n",
              " 'rouge-l': {'f': 0.05933669709711019,\n",
              "  'p': 0.06467209302135801,\n",
              "  'r': 0.05742897935397962}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOnnLQRS34Rm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}